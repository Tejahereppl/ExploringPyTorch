{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdLNNv31tefV",
        "outputId": "0c32b014-f84e-42b9-c1a1-5b214b5bb44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 18.8MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 302kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.54MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 18.0MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Downloading the Dataset from torchvision.datasets\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = None\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYJVZKiH6QrK",
        "outputId": "12d2c219-fb00-4e30-f83f-3a98d2d7908a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0118, 0.0039, 0.0000, 0.0000, 0.0275,\n",
              "           0.0000, 0.1451, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000,\n",
              "           0.1059, 0.3294, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.4667, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
              "           0.3451, 0.5608, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
              "           0.3647, 0.4157, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.2078,\n",
              "           0.5059, 0.4706, 0.5765, 0.6863, 0.6157, 0.6510, 0.5294, 0.6039,\n",
              "           0.6588, 0.5490, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0431, 0.5373,\n",
              "           0.5098, 0.5020, 0.6275, 0.6902, 0.6235, 0.6549, 0.6980, 0.5843,\n",
              "           0.5922, 0.5647, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
              "           0.0078, 0.0039, 0.0000, 0.0118, 0.0000, 0.0000, 0.4510, 0.4471,\n",
              "           0.4157, 0.5373, 0.6588, 0.6000, 0.6118, 0.6471, 0.6549, 0.5608,\n",
              "           0.6157, 0.6196, 0.0431, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.3490, 0.5451, 0.3529,\n",
              "           0.3686, 0.6000, 0.5843, 0.5137, 0.5922, 0.6627, 0.6745, 0.5608,\n",
              "           0.6235, 0.6627, 0.1882, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0157,\n",
              "           0.0039, 0.0000, 0.0000, 0.0000, 0.3843, 0.5333, 0.4314, 0.4275,\n",
              "           0.4314, 0.6353, 0.5294, 0.5647, 0.5843, 0.6235, 0.6549, 0.5647,\n",
              "           0.6196, 0.6627, 0.4667, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.1020, 0.4235, 0.4588, 0.3882, 0.4353, 0.4588,\n",
              "           0.5333, 0.6118, 0.5255, 0.6039, 0.6039, 0.6118, 0.6275, 0.5529,\n",
              "           0.5765, 0.6118, 0.6980, 0.0000],\n",
              "          [0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,\n",
              "           0.2078, 0.3608, 0.4588, 0.4353, 0.4039, 0.4510, 0.5059, 0.5255,\n",
              "           0.5608, 0.6039, 0.6471, 0.6667, 0.6039, 0.5922, 0.6039, 0.5608,\n",
              "           0.5412, 0.5882, 0.6471, 0.1686],\n",
              "          [0.0000, 0.0000, 0.0902, 0.2118, 0.2549, 0.2980, 0.3333, 0.4627,\n",
              "           0.5020, 0.4824, 0.4353, 0.4431, 0.4627, 0.4980, 0.4902, 0.5451,\n",
              "           0.5216, 0.5333, 0.6275, 0.5490, 0.6078, 0.6314, 0.5647, 0.6078,\n",
              "           0.6745, 0.6314, 0.7412, 0.2431],\n",
              "          [0.0000, 0.2667, 0.3686, 0.3529, 0.4353, 0.4471, 0.4353, 0.4471,\n",
              "           0.4510, 0.4980, 0.5294, 0.5333, 0.5608, 0.4941, 0.4980, 0.5922,\n",
              "           0.6039, 0.5608, 0.5804, 0.4902, 0.6353, 0.6353, 0.5647, 0.5412,\n",
              "           0.6000, 0.6353, 0.7686, 0.2275],\n",
              "          [0.2745, 0.6627, 0.5059, 0.4078, 0.3843, 0.3922, 0.3686, 0.3804,\n",
              "           0.3843, 0.4000, 0.4235, 0.4157, 0.4667, 0.4706, 0.5059, 0.5843,\n",
              "           0.6118, 0.6549, 0.7451, 0.7451, 0.7686, 0.7765, 0.7765, 0.7333,\n",
              "           0.7725, 0.7412, 0.7216, 0.1412],\n",
              "          [0.0627, 0.4941, 0.6706, 0.7373, 0.7373, 0.7216, 0.6706, 0.6000,\n",
              "           0.5294, 0.4706, 0.4941, 0.4980, 0.5725, 0.7255, 0.7647, 0.8196,\n",
              "           0.8157, 1.0000, 0.8196, 0.6941, 0.9608, 0.9882, 0.9843, 0.9843,\n",
              "           0.9686, 0.8627, 0.8078, 0.1922],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0471, 0.2627, 0.4157, 0.6431, 0.7255,\n",
              "           0.7804, 0.8235, 0.8275, 0.8235, 0.8157, 0.7451, 0.5882, 0.3216,\n",
              "           0.0314, 0.0000, 0.0000, 0.0000, 0.6980, 0.8157, 0.7373, 0.6863,\n",
              "           0.6353, 0.6196, 0.5922, 0.0431],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 9)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGsj9uGtz4tX",
        "outputId": "d0d0c629-f785-41ca-b18b-8bb690c48795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data.classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7StOuxkDysfj"
      },
      "outputs": [],
      "source": [
        "test_data = datasets.FashionMNIST(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8rYOUGqvKC7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size = 32,\n",
        "    shuffle= True\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWLetCMPyIac"
      },
      "outputs": [],
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVJ7Cx1AyRgO",
        "outputId": "565331b4-ef3c-46f8-f158-bd6a94cc4fa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len of train dataloader 1875\n",
            "len of test dataloader 313\n"
          ]
        }
      ],
      "source": [
        "print(f\"len of train dataloader {len(train_dataloader)}\")\n",
        "print(f\"len of test dataloader {len(test_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrS9DswNyx_N"
      },
      "outputs": [],
      "source": [
        "#Building the model\n",
        "class ImageClassifierV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = 784,out_features = 30),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features =30,out_features = 30 ),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features = 30,out_features = 10)\n",
        "    )\n",
        "  def forward(self,X):\n",
        "    return self.layer(X)\n",
        "\n",
        "model1 = ImageClassifierV1()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opYmCF4w1fxn"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params =model1.parameters(),lr = 0.1 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9rArUn73QHm"
      },
      "outputs": [],
      "source": [
        "def acc_fn(Y_pred,Y_test):\n",
        "  _,Y_pred = torch.max(Y_pred,1)\n",
        "  return ((Y_pred==Y_test).sum())/len(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwxvoUYK15LU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def trainer(dataloader,model,acc_fn,loss_fn,optimizer):\n",
        "  model.train()\n",
        "  for batch,(X,y) in enumerate(dataloader):\n",
        "\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred,y)\n",
        "\n",
        "    print(f\"Batch : {batch}|Training Loss: {loss}|Training Accuracy : {acc_fn(y_pred,y)}\")\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def tester(dataloader,model,acc_fn,loss_fn):\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X,y in (dataloader):\n",
        "\n",
        "      y_pred = model(X)\n",
        "\n",
        "      loss = loss_fn(y_pred,y)\n",
        "      print(f\"Test Loss: {loss}|Test Accuracy : {acc_fn(y_pred,y)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4LqlSIpW4Vu8",
        "outputId": "b114ef3a-1fb0-4fbe-a940-4c45aa973496"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch : 1566|Training Loss: 0.3400443494319916|Training Accuracy : 0.90625\n",
            "Batch : 1567|Training Loss: 0.3881898522377014|Training Accuracy : 0.8125\n",
            "Batch : 1568|Training Loss: 0.24825860559940338|Training Accuracy : 0.9375\n",
            "Batch : 1569|Training Loss: 0.16086483001708984|Training Accuracy : 0.90625\n",
            "Batch : 1570|Training Loss: 0.3666287660598755|Training Accuracy : 0.90625\n",
            "Batch : 1571|Training Loss: 0.13568194210529327|Training Accuracy : 0.96875\n",
            "Batch : 1572|Training Loss: 0.33039095997810364|Training Accuracy : 0.90625\n",
            "Batch : 1573|Training Loss: 0.15440228581428528|Training Accuracy : 0.90625\n",
            "Batch : 1574|Training Loss: 0.13587364554405212|Training Accuracy : 0.9375\n",
            "Batch : 1575|Training Loss: 0.11893057078123093|Training Accuracy : 0.9375\n",
            "Batch : 1576|Training Loss: 0.21072182059288025|Training Accuracy : 0.875\n",
            "Batch : 1577|Training Loss: 0.21786755323410034|Training Accuracy : 0.90625\n",
            "Batch : 1578|Training Loss: 0.2130308896303177|Training Accuracy : 0.90625\n",
            "Batch : 1579|Training Loss: 0.2467186450958252|Training Accuracy : 0.90625\n",
            "Batch : 1580|Training Loss: 0.1732160747051239|Training Accuracy : 0.90625\n",
            "Batch : 1581|Training Loss: 0.0884980782866478|Training Accuracy : 0.96875\n",
            "Batch : 1582|Training Loss: 0.11457425355911255|Training Accuracy : 0.96875\n",
            "Batch : 1583|Training Loss: 0.0736997053027153|Training Accuracy : 0.96875\n",
            "Batch : 1584|Training Loss: 0.13208061456680298|Training Accuracy : 0.96875\n",
            "Batch : 1585|Training Loss: 0.12490034103393555|Training Accuracy : 0.96875\n",
            "Batch : 1586|Training Loss: 0.18181417882442474|Training Accuracy : 0.96875\n",
            "Batch : 1587|Training Loss: 0.1307791769504547|Training Accuracy : 0.96875\n",
            "Batch : 1588|Training Loss: 0.40260064601898193|Training Accuracy : 0.875\n",
            "Batch : 1589|Training Loss: 0.35917797684669495|Training Accuracy : 0.9375\n",
            "Batch : 1590|Training Loss: 0.0977194681763649|Training Accuracy : 0.96875\n",
            "Batch : 1591|Training Loss: 0.31536537408828735|Training Accuracy : 0.875\n",
            "Batch : 1592|Training Loss: 0.22775129973888397|Training Accuracy : 0.9375\n",
            "Batch : 1593|Training Loss: 0.23160013556480408|Training Accuracy : 0.9375\n",
            "Batch : 1594|Training Loss: 0.2685178220272064|Training Accuracy : 0.875\n",
            "Batch : 1595|Training Loss: 0.04054020345211029|Training Accuracy : 1.0\n",
            "Batch : 1596|Training Loss: 0.14886333048343658|Training Accuracy : 0.90625\n",
            "Batch : 1597|Training Loss: 0.17149809002876282|Training Accuracy : 0.96875\n",
            "Batch : 1598|Training Loss: 0.1997719258069992|Training Accuracy : 0.90625\n",
            "Batch : 1599|Training Loss: 0.18964783847332|Training Accuracy : 0.96875\n",
            "Batch : 1600|Training Loss: 0.20014867186546326|Training Accuracy : 0.9375\n",
            "Batch : 1601|Training Loss: 0.0688217356801033|Training Accuracy : 0.9375\n",
            "Batch : 1602|Training Loss: 0.35035133361816406|Training Accuracy : 0.84375\n",
            "Batch : 1603|Training Loss: 0.25077909231185913|Training Accuracy : 0.875\n",
            "Batch : 1604|Training Loss: 0.08610758930444717|Training Accuracy : 0.96875\n",
            "Batch : 1605|Training Loss: 0.162923663854599|Training Accuracy : 0.9375\n",
            "Batch : 1606|Training Loss: 0.09551428258419037|Training Accuracy : 1.0\n",
            "Batch : 1607|Training Loss: 0.0901852399110794|Training Accuracy : 0.96875\n",
            "Batch : 1608|Training Loss: 0.20705290138721466|Training Accuracy : 0.9375\n",
            "Batch : 1609|Training Loss: 0.13006138801574707|Training Accuracy : 0.9375\n",
            "Batch : 1610|Training Loss: 0.08620978146791458|Training Accuracy : 0.96875\n",
            "Batch : 1611|Training Loss: 0.310371071100235|Training Accuracy : 0.875\n",
            "Batch : 1612|Training Loss: 0.19096262753009796|Training Accuracy : 0.9375\n",
            "Batch : 1613|Training Loss: 0.3256405293941498|Training Accuracy : 0.875\n",
            "Batch : 1614|Training Loss: 0.19106526672840118|Training Accuracy : 0.90625\n",
            "Batch : 1615|Training Loss: 0.18090490996837616|Training Accuracy : 0.9375\n",
            "Batch : 1616|Training Loss: 0.14714668691158295|Training Accuracy : 0.9375\n",
            "Batch : 1617|Training Loss: 0.2817751169204712|Training Accuracy : 0.90625\n",
            "Batch : 1618|Training Loss: 0.08441928029060364|Training Accuracy : 0.96875\n",
            "Batch : 1619|Training Loss: 0.3061169981956482|Training Accuracy : 0.875\n",
            "Batch : 1620|Training Loss: 0.055372290313243866|Training Accuracy : 1.0\n",
            "Batch : 1621|Training Loss: 0.3851168751716614|Training Accuracy : 0.875\n",
            "Batch : 1622|Training Loss: 0.13167797029018402|Training Accuracy : 0.96875\n",
            "Batch : 1623|Training Loss: 0.1782352179288864|Training Accuracy : 0.96875\n",
            "Batch : 1624|Training Loss: 0.08819679915904999|Training Accuracy : 1.0\n",
            "Batch : 1625|Training Loss: 0.022578805685043335|Training Accuracy : 1.0\n",
            "Batch : 1626|Training Loss: 0.1753237545490265|Training Accuracy : 0.9375\n",
            "Batch : 1627|Training Loss: 0.19456394016742706|Training Accuracy : 0.90625\n",
            "Batch : 1628|Training Loss: 0.06010125204920769|Training Accuracy : 0.96875\n",
            "Batch : 1629|Training Loss: 0.0705774798989296|Training Accuracy : 1.0\n",
            "Batch : 1630|Training Loss: 0.12849687039852142|Training Accuracy : 0.9375\n",
            "Batch : 1631|Training Loss: 0.06748764216899872|Training Accuracy : 1.0\n",
            "Batch : 1632|Training Loss: 0.19247835874557495|Training Accuracy : 0.90625\n",
            "Batch : 1633|Training Loss: 0.08598990738391876|Training Accuracy : 0.9375\n",
            "Batch : 1634|Training Loss: 0.14027507603168488|Training Accuracy : 0.9375\n",
            "Batch : 1635|Training Loss: 0.19426128268241882|Training Accuracy : 0.9375\n",
            "Batch : 1636|Training Loss: 0.3208250105381012|Training Accuracy : 0.90625\n",
            "Batch : 1637|Training Loss: 0.19124282896518707|Training Accuracy : 0.90625\n",
            "Batch : 1638|Training Loss: 0.0657769963145256|Training Accuracy : 1.0\n",
            "Batch : 1639|Training Loss: 0.2955615222454071|Training Accuracy : 0.9375\n",
            "Batch : 1640|Training Loss: 0.20497551560401917|Training Accuracy : 0.90625\n",
            "Batch : 1641|Training Loss: 0.3213937282562256|Training Accuracy : 0.9375\n",
            "Batch : 1642|Training Loss: 0.07467354834079742|Training Accuracy : 0.96875\n",
            "Batch : 1643|Training Loss: 0.1616402566432953|Training Accuracy : 0.96875\n",
            "Batch : 1644|Training Loss: 0.22062934935092926|Training Accuracy : 0.90625\n",
            "Batch : 1645|Training Loss: 0.08041193336248398|Training Accuracy : 0.96875\n",
            "Batch : 1646|Training Loss: 0.24626240134239197|Training Accuracy : 0.96875\n",
            "Batch : 1647|Training Loss: 0.12627601623535156|Training Accuracy : 0.96875\n",
            "Batch : 1648|Training Loss: 0.20324015617370605|Training Accuracy : 0.90625\n",
            "Batch : 1649|Training Loss: 0.10049895197153091|Training Accuracy : 0.9375\n",
            "Batch : 1650|Training Loss: 0.09094807505607605|Training Accuracy : 0.9375\n",
            "Batch : 1651|Training Loss: 0.20759260654449463|Training Accuracy : 0.90625\n",
            "Batch : 1652|Training Loss: 0.10843323171138763|Training Accuracy : 0.96875\n",
            "Batch : 1653|Training Loss: 0.15605004131793976|Training Accuracy : 0.9375\n",
            "Batch : 1654|Training Loss: 0.12490454316139221|Training Accuracy : 0.96875\n",
            "Batch : 1655|Training Loss: 0.14639362692832947|Training Accuracy : 0.9375\n",
            "Batch : 1656|Training Loss: 0.32745224237442017|Training Accuracy : 0.875\n",
            "Batch : 1657|Training Loss: 0.1455334722995758|Training Accuracy : 0.9375\n",
            "Batch : 1658|Training Loss: 0.11263252049684525|Training Accuracy : 0.9375\n",
            "Batch : 1659|Training Loss: 0.14133119583129883|Training Accuracy : 0.9375\n",
            "Batch : 1660|Training Loss: 0.19019176065921783|Training Accuracy : 0.90625\n",
            "Batch : 1661|Training Loss: 0.33794263005256653|Training Accuracy : 0.84375\n",
            "Batch : 1662|Training Loss: 0.2372671216726303|Training Accuracy : 0.90625\n",
            "Batch : 1663|Training Loss: 0.08190514147281647|Training Accuracy : 1.0\n",
            "Batch : 1664|Training Loss: 0.2068822830915451|Training Accuracy : 0.90625\n",
            "Batch : 1665|Training Loss: 0.06019365042448044|Training Accuracy : 1.0\n",
            "Batch : 1666|Training Loss: 0.12524251639842987|Training Accuracy : 0.9375\n",
            "Batch : 1667|Training Loss: 0.1258145570755005|Training Accuracy : 0.9375\n",
            "Batch : 1668|Training Loss: 0.17102360725402832|Training Accuracy : 0.9375\n",
            "Batch : 1669|Training Loss: 0.2268812507390976|Training Accuracy : 0.90625\n",
            "Batch : 1670|Training Loss: 0.15830378234386444|Training Accuracy : 0.9375\n",
            "Batch : 1671|Training Loss: 0.11952529102563858|Training Accuracy : 0.96875\n",
            "Batch : 1672|Training Loss: 0.07701687514781952|Training Accuracy : 0.96875\n",
            "Batch : 1673|Training Loss: 0.3100703954696655|Training Accuracy : 0.90625\n",
            "Batch : 1674|Training Loss: 0.13763447105884552|Training Accuracy : 0.96875\n",
            "Batch : 1675|Training Loss: 0.20786333084106445|Training Accuracy : 0.90625\n",
            "Batch : 1676|Training Loss: 0.25300052762031555|Training Accuracy : 0.90625\n",
            "Batch : 1677|Training Loss: 0.2343178242444992|Training Accuracy : 0.90625\n",
            "Batch : 1678|Training Loss: 0.04505552351474762|Training Accuracy : 0.96875\n",
            "Batch : 1679|Training Loss: 0.16621068120002747|Training Accuracy : 0.96875\n",
            "Batch : 1680|Training Loss: 0.0811007171869278|Training Accuracy : 0.96875\n",
            "Batch : 1681|Training Loss: 0.4030011296272278|Training Accuracy : 0.84375\n",
            "Batch : 1682|Training Loss: 0.45594388246536255|Training Accuracy : 0.84375\n",
            "Batch : 1683|Training Loss: 0.05273822322487831|Training Accuracy : 0.96875\n",
            "Batch : 1684|Training Loss: 0.2658297121524811|Training Accuracy : 0.875\n",
            "Batch : 1685|Training Loss: 0.269090473651886|Training Accuracy : 0.84375\n",
            "Batch : 1686|Training Loss: 0.12369904667139053|Training Accuracy : 0.96875\n",
            "Batch : 1687|Training Loss: 0.3571871817111969|Training Accuracy : 0.90625\n",
            "Batch : 1688|Training Loss: 0.04205206036567688|Training Accuracy : 1.0\n",
            "Batch : 1689|Training Loss: 0.09564217925071716|Training Accuracy : 0.96875\n",
            "Batch : 1690|Training Loss: 0.20765037834644318|Training Accuracy : 0.9375\n",
            "Batch : 1691|Training Loss: 0.13009727001190186|Training Accuracy : 0.9375\n",
            "Batch : 1692|Training Loss: 0.022090088576078415|Training Accuracy : 1.0\n",
            "Batch : 1693|Training Loss: 0.2712790071964264|Training Accuracy : 0.90625\n",
            "Batch : 1694|Training Loss: 0.2145148366689682|Training Accuracy : 0.90625\n",
            "Batch : 1695|Training Loss: 0.2067115753889084|Training Accuracy : 0.9375\n",
            "Batch : 1696|Training Loss: 0.2274593710899353|Training Accuracy : 0.90625\n",
            "Batch : 1697|Training Loss: 0.23530173301696777|Training Accuracy : 0.90625\n",
            "Batch : 1698|Training Loss: 0.07842769473791122|Training Accuracy : 1.0\n",
            "Batch : 1699|Training Loss: 0.1585054099559784|Training Accuracy : 0.9375\n",
            "Batch : 1700|Training Loss: 0.3816026449203491|Training Accuracy : 0.90625\n",
            "Batch : 1701|Training Loss: 0.08512049913406372|Training Accuracy : 0.96875\n",
            "Batch : 1702|Training Loss: 0.1902562975883484|Training Accuracy : 0.9375\n",
            "Batch : 1703|Training Loss: 0.16503679752349854|Training Accuracy : 0.90625\n",
            "Batch : 1704|Training Loss: 0.06840530037879944|Training Accuracy : 1.0\n",
            "Batch : 1705|Training Loss: 0.23671594262123108|Training Accuracy : 0.84375\n",
            "Batch : 1706|Training Loss: 0.24359311163425446|Training Accuracy : 0.9375\n",
            "Batch : 1707|Training Loss: 0.13354399800300598|Training Accuracy : 0.9375\n",
            "Batch : 1708|Training Loss: 0.22056275606155396|Training Accuracy : 0.9375\n",
            "Batch : 1709|Training Loss: 0.36012569069862366|Training Accuracy : 0.875\n",
            "Batch : 1710|Training Loss: 0.18849989771842957|Training Accuracy : 0.90625\n",
            "Batch : 1711|Training Loss: 0.06762317568063736|Training Accuracy : 0.96875\n",
            "Batch : 1712|Training Loss: 0.17510344088077545|Training Accuracy : 0.90625\n",
            "Batch : 1713|Training Loss: 0.33532169461250305|Training Accuracy : 0.9375\n",
            "Batch : 1714|Training Loss: 0.19014734029769897|Training Accuracy : 0.9375\n",
            "Batch : 1715|Training Loss: 0.12027189135551453|Training Accuracy : 0.9375\n",
            "Batch : 1716|Training Loss: 0.1265752762556076|Training Accuracy : 0.9375\n",
            "Batch : 1717|Training Loss: 0.37806758284568787|Training Accuracy : 0.9375\n",
            "Batch : 1718|Training Loss: 0.07614503055810928|Training Accuracy : 0.96875\n",
            "Batch : 1719|Training Loss: 0.02718510851264|Training Accuracy : 1.0\n",
            "Batch : 1720|Training Loss: 0.1377868801355362|Training Accuracy : 0.90625\n",
            "Batch : 1721|Training Loss: 0.10221778601408005|Training Accuracy : 1.0\n",
            "Batch : 1722|Training Loss: 0.18639948964118958|Training Accuracy : 0.9375\n",
            "Batch : 1723|Training Loss: 0.1798563450574875|Training Accuracy : 0.90625\n",
            "Batch : 1724|Training Loss: 0.09427858889102936|Training Accuracy : 0.96875\n",
            "Batch : 1725|Training Loss: 0.022820118814706802|Training Accuracy : 1.0\n",
            "Batch : 1726|Training Loss: 0.4601539969444275|Training Accuracy : 0.84375\n",
            "Batch : 1727|Training Loss: 0.22555676102638245|Training Accuracy : 0.84375\n",
            "Batch : 1728|Training Loss: 0.03473719209432602|Training Accuracy : 1.0\n",
            "Batch : 1729|Training Loss: 0.26501891016960144|Training Accuracy : 0.875\n",
            "Batch : 1730|Training Loss: 0.18956215679645538|Training Accuracy : 0.96875\n",
            "Batch : 1731|Training Loss: 0.11901280283927917|Training Accuracy : 0.96875\n",
            "Batch : 1732|Training Loss: 0.12317956984043121|Training Accuracy : 0.96875\n",
            "Batch : 1733|Training Loss: 0.21021318435668945|Training Accuracy : 0.90625\n",
            "Batch : 1734|Training Loss: 0.2222273349761963|Training Accuracy : 0.9375\n",
            "Batch : 1735|Training Loss: 0.0803319662809372|Training Accuracy : 1.0\n",
            "Batch : 1736|Training Loss: 0.18110093474388123|Training Accuracy : 0.96875\n",
            "Batch : 1737|Training Loss: 0.1449611335992813|Training Accuracy : 0.9375\n",
            "Batch : 1738|Training Loss: 0.04626309871673584|Training Accuracy : 1.0\n",
            "Batch : 1739|Training Loss: 0.20311573147773743|Training Accuracy : 0.90625\n",
            "Batch : 1740|Training Loss: 0.22132755815982819|Training Accuracy : 0.875\n",
            "Batch : 1741|Training Loss: 0.1333969682455063|Training Accuracy : 0.9375\n",
            "Batch : 1742|Training Loss: 0.23538437485694885|Training Accuracy : 0.875\n",
            "Batch : 1743|Training Loss: 0.1987217217683792|Training Accuracy : 0.9375\n",
            "Batch : 1744|Training Loss: 0.13126924633979797|Training Accuracy : 0.9375\n",
            "Batch : 1745|Training Loss: 0.16458208858966827|Training Accuracy : 0.90625\n",
            "Batch : 1746|Training Loss: 0.22489739954471588|Training Accuracy : 0.90625\n",
            "Batch : 1747|Training Loss: 0.2438165545463562|Training Accuracy : 0.875\n",
            "Batch : 1748|Training Loss: 0.3109779357910156|Training Accuracy : 0.90625\n",
            "Batch : 1749|Training Loss: 0.11418850719928741|Training Accuracy : 0.9375\n",
            "Batch : 1750|Training Loss: 0.13953398168087006|Training Accuracy : 0.96875\n",
            "Batch : 1751|Training Loss: 0.196218803524971|Training Accuracy : 0.90625\n",
            "Batch : 1752|Training Loss: 0.236037939786911|Training Accuracy : 0.9375\n",
            "Batch : 1753|Training Loss: 0.21540550887584686|Training Accuracy : 0.9375\n",
            "Batch : 1754|Training Loss: 0.3537522852420807|Training Accuracy : 0.875\n",
            "Batch : 1755|Training Loss: 0.0464496947824955|Training Accuracy : 1.0\n",
            "Batch : 1756|Training Loss: 0.22810406982898712|Training Accuracy : 0.875\n",
            "Batch : 1757|Training Loss: 0.4732637405395508|Training Accuracy : 0.875\n",
            "Batch : 1758|Training Loss: 0.18486288189888|Training Accuracy : 0.9375\n",
            "Batch : 1759|Training Loss: 0.4370487332344055|Training Accuracy : 0.90625\n",
            "Batch : 1760|Training Loss: 0.14664188027381897|Training Accuracy : 0.9375\n",
            "Batch : 1761|Training Loss: 0.08706284314393997|Training Accuracy : 0.96875\n",
            "Batch : 1762|Training Loss: 0.16768287122249603|Training Accuracy : 0.875\n",
            "Batch : 1763|Training Loss: 0.1374608874320984|Training Accuracy : 0.96875\n",
            "Batch : 1764|Training Loss: 0.08168697357177734|Training Accuracy : 0.96875\n",
            "Batch : 1765|Training Loss: 0.2952592372894287|Training Accuracy : 0.8125\n",
            "Batch : 1766|Training Loss: 0.1670083999633789|Training Accuracy : 0.90625\n",
            "Batch : 1767|Training Loss: 0.17956455051898956|Training Accuracy : 0.9375\n",
            "Batch : 1768|Training Loss: 0.16051028668880463|Training Accuracy : 0.96875\n",
            "Batch : 1769|Training Loss: 0.20203091204166412|Training Accuracy : 0.90625\n",
            "Batch : 1770|Training Loss: 0.11329391598701477|Training Accuracy : 0.90625\n",
            "Batch : 1771|Training Loss: 0.390666127204895|Training Accuracy : 0.875\n",
            "Batch : 1772|Training Loss: 0.28857946395874023|Training Accuracy : 0.90625\n",
            "Batch : 1773|Training Loss: 0.3074646592140198|Training Accuracy : 0.9375\n",
            "Batch : 1774|Training Loss: 0.2992168068885803|Training Accuracy : 0.84375\n",
            "Batch : 1775|Training Loss: 0.10869929194450378|Training Accuracy : 0.9375\n",
            "Batch : 1776|Training Loss: 0.06228906288743019|Training Accuracy : 0.96875\n",
            "Batch : 1777|Training Loss: 0.07872790843248367|Training Accuracy : 0.96875\n",
            "Batch : 1778|Training Loss: 0.5161288976669312|Training Accuracy : 0.78125\n",
            "Batch : 1779|Training Loss: 0.3699457049369812|Training Accuracy : 0.875\n",
            "Batch : 1780|Training Loss: 0.3542601764202118|Training Accuracy : 0.75\n",
            "Batch : 1781|Training Loss: 0.07759465277194977|Training Accuracy : 1.0\n",
            "Batch : 1782|Training Loss: 0.22404755651950836|Training Accuracy : 0.875\n",
            "Batch : 1783|Training Loss: 0.32769638299942017|Training Accuracy : 0.84375\n",
            "Batch : 1784|Training Loss: 0.29787248373031616|Training Accuracy : 0.90625\n",
            "Batch : 1785|Training Loss: 0.20617987215518951|Training Accuracy : 0.875\n",
            "Batch : 1786|Training Loss: 0.21349479258060455|Training Accuracy : 0.90625\n",
            "Batch : 1787|Training Loss: 0.0581248514354229|Training Accuracy : 0.96875\n",
            "Batch : 1788|Training Loss: 0.15910732746124268|Training Accuracy : 0.9375\n",
            "Batch : 1789|Training Loss: 0.22097744047641754|Training Accuracy : 0.90625\n",
            "Batch : 1790|Training Loss: 0.18675819039344788|Training Accuracy : 0.9375\n",
            "Batch : 1791|Training Loss: 0.033057816326618195|Training Accuracy : 1.0\n",
            "Batch : 1792|Training Loss: 0.10352924466133118|Training Accuracy : 0.9375\n",
            "Batch : 1793|Training Loss: 0.12332187592983246|Training Accuracy : 0.96875\n",
            "Batch : 1794|Training Loss: 0.2536696493625641|Training Accuracy : 0.9375\n",
            "Batch : 1795|Training Loss: 0.22342327237129211|Training Accuracy : 0.90625\n",
            "Batch : 1796|Training Loss: 0.08025528490543365|Training Accuracy : 0.96875\n",
            "Batch : 1797|Training Loss: 0.22867310047149658|Training Accuracy : 0.875\n",
            "Batch : 1798|Training Loss: 0.10897158831357956|Training Accuracy : 0.96875\n",
            "Batch : 1799|Training Loss: 0.1548486351966858|Training Accuracy : 0.90625\n",
            "Batch : 1800|Training Loss: 0.14141397178173065|Training Accuracy : 0.90625\n",
            "Batch : 1801|Training Loss: 0.04923832416534424|Training Accuracy : 1.0\n",
            "Batch : 1802|Training Loss: 0.16888095438480377|Training Accuracy : 0.9375\n",
            "Batch : 1803|Training Loss: 0.08200094848871231|Training Accuracy : 0.96875\n",
            "Batch : 1804|Training Loss: 0.06958052515983582|Training Accuracy : 0.96875\n",
            "Batch : 1805|Training Loss: 0.45893728733062744|Training Accuracy : 0.90625\n",
            "Batch : 1806|Training Loss: 0.11035799235105515|Training Accuracy : 0.9375\n",
            "Batch : 1807|Training Loss: 0.04827908053994179|Training Accuracy : 1.0\n",
            "Batch : 1808|Training Loss: 0.2049475908279419|Training Accuracy : 0.9375\n",
            "Batch : 1809|Training Loss: 0.04945296421647072|Training Accuracy : 1.0\n",
            "Batch : 1810|Training Loss: 0.12786176800727844|Training Accuracy : 0.9375\n",
            "Batch : 1811|Training Loss: 0.2961519658565521|Training Accuracy : 0.875\n",
            "Batch : 1812|Training Loss: 0.14443744719028473|Training Accuracy : 0.9375\n",
            "Batch : 1813|Training Loss: 0.11158908903598785|Training Accuracy : 0.9375\n",
            "Batch : 1814|Training Loss: 0.20211359858512878|Training Accuracy : 0.90625\n",
            "Batch : 1815|Training Loss: 0.1146172285079956|Training Accuracy : 0.9375\n",
            "Batch : 1816|Training Loss: 0.19334222376346588|Training Accuracy : 0.90625\n",
            "Batch : 1817|Training Loss: 0.13244301080703735|Training Accuracy : 0.9375\n",
            "Batch : 1818|Training Loss: 0.13024955987930298|Training Accuracy : 0.9375\n",
            "Batch : 1819|Training Loss: 0.10431182384490967|Training Accuracy : 0.96875\n",
            "Batch : 1820|Training Loss: 0.15429802238941193|Training Accuracy : 0.9375\n",
            "Batch : 1821|Training Loss: 0.1726132482290268|Training Accuracy : 0.9375\n",
            "Batch : 1822|Training Loss: 0.19019560515880585|Training Accuracy : 0.9375\n",
            "Batch : 1823|Training Loss: 0.08518196642398834|Training Accuracy : 0.96875\n",
            "Batch : 1824|Training Loss: 0.3332594633102417|Training Accuracy : 0.9375\n",
            "Batch : 1825|Training Loss: 0.1473783552646637|Training Accuracy : 0.9375\n",
            "Batch : 1826|Training Loss: 0.08992626518011093|Training Accuracy : 0.96875\n",
            "Batch : 1827|Training Loss: 0.1743122786283493|Training Accuracy : 0.9375\n",
            "Batch : 1828|Training Loss: 0.24868975579738617|Training Accuracy : 0.9375\n",
            "Batch : 1829|Training Loss: 0.15165944397449493|Training Accuracy : 0.96875\n",
            "Batch : 1830|Training Loss: 0.32704225182533264|Training Accuracy : 0.84375\n",
            "Batch : 1831|Training Loss: 0.07904556393623352|Training Accuracy : 1.0\n",
            "Batch : 1832|Training Loss: 0.1899976134300232|Training Accuracy : 0.9375\n",
            "Batch : 1833|Training Loss: 0.03545234352350235|Training Accuracy : 1.0\n",
            "Batch : 1834|Training Loss: 0.16869397461414337|Training Accuracy : 0.9375\n",
            "Batch : 1835|Training Loss: 0.1307295858860016|Training Accuracy : 0.9375\n",
            "Batch : 1836|Training Loss: 0.22361260652542114|Training Accuracy : 0.90625\n",
            "Batch : 1837|Training Loss: 0.2666761875152588|Training Accuracy : 0.875\n",
            "Batch : 1838|Training Loss: 0.35795456171035767|Training Accuracy : 0.8125\n",
            "Batch : 1839|Training Loss: 0.04787822440266609|Training Accuracy : 1.0\n",
            "Batch : 1840|Training Loss: 0.3044942021369934|Training Accuracy : 0.875\n",
            "Batch : 1841|Training Loss: 0.3636797070503235|Training Accuracy : 0.84375\n",
            "Batch : 1842|Training Loss: 0.37136465311050415|Training Accuracy : 0.90625\n",
            "Batch : 1843|Training Loss: 0.07415026426315308|Training Accuracy : 0.9375\n",
            "Batch : 1844|Training Loss: 0.1326223611831665|Training Accuracy : 0.9375\n",
            "Batch : 1845|Training Loss: 0.2458834946155548|Training Accuracy : 0.90625\n",
            "Batch : 1846|Training Loss: 0.34813061356544495|Training Accuracy : 0.90625\n",
            "Batch : 1847|Training Loss: 0.10858984291553497|Training Accuracy : 0.96875\n",
            "Batch : 1848|Training Loss: 0.13699504733085632|Training Accuracy : 0.9375\n",
            "Batch : 1849|Training Loss: 0.08834810554981232|Training Accuracy : 0.96875\n",
            "Batch : 1850|Training Loss: 0.19343598186969757|Training Accuracy : 0.9375\n",
            "Batch : 1851|Training Loss: 0.0764768049120903|Training Accuracy : 0.9375\n",
            "Batch : 1852|Training Loss: 0.14385569095611572|Training Accuracy : 0.90625\n",
            "Batch : 1853|Training Loss: 0.250499963760376|Training Accuracy : 0.90625\n",
            "Batch : 1854|Training Loss: 0.27041956782341003|Training Accuracy : 0.90625\n",
            "Batch : 1855|Training Loss: 0.4094296395778656|Training Accuracy : 0.875\n",
            "Batch : 1856|Training Loss: 0.23452071845531464|Training Accuracy : 0.875\n",
            "Batch : 1857|Training Loss: 0.1761813461780548|Training Accuracy : 0.9375\n",
            "Batch : 1858|Training Loss: 0.06910047680139542|Training Accuracy : 0.96875\n",
            "Batch : 1859|Training Loss: 0.2257087528705597|Training Accuracy : 0.90625\n",
            "Batch : 1860|Training Loss: 0.11167842149734497|Training Accuracy : 0.96875\n",
            "Batch : 1861|Training Loss: 0.06665617972612381|Training Accuracy : 0.96875\n",
            "Batch : 1862|Training Loss: 0.17486447095870972|Training Accuracy : 0.9375\n",
            "Batch : 1863|Training Loss: 0.10057510435581207|Training Accuracy : 0.96875\n",
            "Batch : 1864|Training Loss: 0.26594606041908264|Training Accuracy : 0.9375\n",
            "Batch : 1865|Training Loss: 0.12652631103992462|Training Accuracy : 0.9375\n",
            "Batch : 1866|Training Loss: 0.3852880001068115|Training Accuracy : 0.8125\n",
            "Batch : 1867|Training Loss: 0.22334854304790497|Training Accuracy : 0.9375\n",
            "Batch : 1868|Training Loss: 0.26244059205055237|Training Accuracy : 0.9375\n",
            "Batch : 1869|Training Loss: 0.12709085643291473|Training Accuracy : 0.9375\n",
            "Batch : 1870|Training Loss: 0.1038300096988678|Training Accuracy : 0.96875\n",
            "Batch : 1871|Training Loss: 0.06419079750776291|Training Accuracy : 0.96875\n",
            "Batch : 1872|Training Loss: 0.0906297117471695|Training Accuracy : 0.96875\n",
            "Batch : 1873|Training Loss: 0.22573767602443695|Training Accuracy : 0.9375\n",
            "Batch : 1874|Training Loss: 0.11879956722259521|Training Accuracy : 0.96875\n",
            "Test Loss: 0.4646885097026825|Test Accuracy : 0.875\n",
            "Test Loss: 1.0977191925048828|Test Accuracy : 0.84375\n",
            "Test Loss: 0.8897191882133484|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2760590612888336|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7522411942481995|Test Accuracy : 0.8125\n",
            "Test Loss: 0.15642374753952026|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4485851526260376|Test Accuracy : 0.875\n",
            "Test Loss: 0.5526950359344482|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4463036060333252|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5823470950126648|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6775643825531006|Test Accuracy : 0.9375\n",
            "Test Loss: 0.7055712938308716|Test Accuracy : 0.84375\n",
            "Test Loss: 0.601531982421875|Test Accuracy : 0.78125\n",
            "Test Loss: 0.37572190165519714|Test Accuracy : 0.9375\n",
            "Test Loss: 0.30864131450653076|Test Accuracy : 0.875\n",
            "Test Loss: 0.5940127372741699|Test Accuracy : 0.875\n",
            "Test Loss: 0.3719569444656372|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6668727397918701|Test Accuracy : 0.875\n",
            "Test Loss: 0.37783509492874146|Test Accuracy : 0.90625\n",
            "Test Loss: 0.31275680661201477|Test Accuracy : 0.84375\n",
            "Test Loss: 0.33925294876098633|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6445578336715698|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5659028887748718|Test Accuracy : 0.90625\n",
            "Test Loss: 1.230538010597229|Test Accuracy : 0.71875\n",
            "Test Loss: 0.07422665506601334|Test Accuracy : 0.96875\n",
            "Test Loss: 0.5639606714248657|Test Accuracy : 0.8125\n",
            "Test Loss: 0.478486567735672|Test Accuracy : 0.84375\n",
            "Test Loss: 0.38684895634651184|Test Accuracy : 0.875\n",
            "Test Loss: 0.3343759775161743|Test Accuracy : 0.90625\n",
            "Test Loss: 0.8144774436950684|Test Accuracy : 0.8125\n",
            "Test Loss: 0.49098628759384155|Test Accuracy : 0.8125\n",
            "Test Loss: 0.12004175782203674|Test Accuracy : 0.9375\n",
            "Test Loss: 0.631066083908081|Test Accuracy : 0.84375\n",
            "Test Loss: 0.20594659447669983|Test Accuracy : 0.875\n",
            "Test Loss: 0.18175986409187317|Test Accuracy : 0.9375\n",
            "Test Loss: 0.26810115575790405|Test Accuracy : 0.84375\n",
            "Test Loss: 0.46199512481689453|Test Accuracy : 0.875\n",
            "Test Loss: 0.5846667289733887|Test Accuracy : 0.90625\n",
            "Test Loss: 0.26988428831100464|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2706659734249115|Test Accuracy : 0.875\n",
            "Test Loss: 0.7144278287887573|Test Accuracy : 0.9375\n",
            "Test Loss: 0.26343607902526855|Test Accuracy : 0.90625\n",
            "Test Loss: 1.3580654859542847|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6146871447563171|Test Accuracy : 0.875\n",
            "Test Loss: 0.4532371163368225|Test Accuracy : 0.875\n",
            "Test Loss: 1.1242733001708984|Test Accuracy : 0.8125\n",
            "Test Loss: 0.12508802115917206|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6044194102287292|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3154764771461487|Test Accuracy : 0.875\n",
            "Test Loss: 0.5641263723373413|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3645976483821869|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6071621775627136|Test Accuracy : 0.875\n",
            "Test Loss: 0.705668568611145|Test Accuracy : 0.84375\n",
            "Test Loss: 0.48276281356811523|Test Accuracy : 0.75\n",
            "Test Loss: 0.6736401319503784|Test Accuracy : 0.84375\n",
            "Test Loss: 0.922889232635498|Test Accuracy : 0.90625\n",
            "Test Loss: 1.387342929840088|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3816502094268799|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2631847560405731|Test Accuracy : 0.875\n",
            "Test Loss: 0.30865707993507385|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6446880102157593|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5263310670852661|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6056839823722839|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5762655138969421|Test Accuracy : 0.8125\n",
            "Test Loss: 0.8549219369888306|Test Accuracy : 0.78125\n",
            "Test Loss: 0.21382610499858856|Test Accuracy : 0.96875\n",
            "Test Loss: 0.1520175188779831|Test Accuracy : 0.9375\n",
            "Test Loss: 0.09500979632139206|Test Accuracy : 0.9375\n",
            "Test Loss: 0.40168771147727966|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3612101674079895|Test Accuracy : 0.90625\n",
            "Test Loss: 0.8967838883399963|Test Accuracy : 0.8125\n",
            "Test Loss: 0.7589680552482605|Test Accuracy : 0.8125\n",
            "Test Loss: 0.33507463335990906|Test Accuracy : 0.90625\n",
            "Test Loss: 0.44900476932525635|Test Accuracy : 0.90625\n",
            "Test Loss: 0.33765071630477905|Test Accuracy : 0.875\n",
            "Test Loss: 0.5894569754600525|Test Accuracy : 0.875\n",
            "Test Loss: 0.10349708795547485|Test Accuracy : 0.96875\n",
            "Test Loss: 0.39827781915664673|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5708996653556824|Test Accuracy : 0.84375\n",
            "Test Loss: 0.7428023219108582|Test Accuracy : 0.8125\n",
            "Test Loss: 0.587407112121582|Test Accuracy : 0.875\n",
            "Test Loss: 0.5790168642997742|Test Accuracy : 0.875\n",
            "Test Loss: 0.6508326530456543|Test Accuracy : 0.84375\n",
            "Test Loss: 0.33241841197013855|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7252715229988098|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5235109329223633|Test Accuracy : 0.875\n",
            "Test Loss: 0.750773012638092|Test Accuracy : 0.84375\n",
            "Test Loss: 0.9906210899353027|Test Accuracy : 0.75\n",
            "Test Loss: 0.23397068679332733|Test Accuracy : 0.90625\n",
            "Test Loss: 0.0993390679359436|Test Accuracy : 0.96875\n",
            "Test Loss: 0.08503751456737518|Test Accuracy : 1.0\n",
            "Test Loss: 0.4398992657661438|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4065181314945221|Test Accuracy : 0.875\n",
            "Test Loss: 0.3575642704963684|Test Accuracy : 0.875\n",
            "Test Loss: 0.5699255466461182|Test Accuracy : 0.8125\n",
            "Test Loss: 0.07459290325641632|Test Accuracy : 0.96875\n",
            "Test Loss: 0.43237829208374023|Test Accuracy : 0.90625\n",
            "Test Loss: 0.15951897203922272|Test Accuracy : 0.9375\n",
            "Test Loss: 0.8987868428230286|Test Accuracy : 0.71875\n",
            "Test Loss: 0.5859202742576599|Test Accuracy : 0.90625\n",
            "Test Loss: 0.11668793857097626|Test Accuracy : 0.9375\n",
            "Test Loss: 0.23931728303432465|Test Accuracy : 0.9375\n",
            "Test Loss: 0.2843567728996277|Test Accuracy : 0.90625\n",
            "Test Loss: 0.34843361377716064|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2530950903892517|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6469675302505493|Test Accuracy : 0.84375\n",
            "Test Loss: 1.13734769821167|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5254042744636536|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5440160036087036|Test Accuracy : 0.8125\n",
            "Test Loss: 0.11278568208217621|Test Accuracy : 0.96875\n",
            "Test Loss: 0.7658757567405701|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4523657262325287|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5370229482650757|Test Accuracy : 0.875\n",
            "Test Loss: 1.1010719537734985|Test Accuracy : 0.65625\n",
            "Test Loss: 0.17692439258098602|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5419490337371826|Test Accuracy : 0.78125\n",
            "Test Loss: 0.36158689856529236|Test Accuracy : 0.8125\n",
            "Test Loss: 0.15073418617248535|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3818751573562622|Test Accuracy : 0.875\n",
            "Test Loss: 0.19183245301246643|Test Accuracy : 0.9375\n",
            "Test Loss: 0.20882755517959595|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6396583914756775|Test Accuracy : 0.875\n",
            "Test Loss: 0.1978605091571808|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3578672707080841|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3977486491203308|Test Accuracy : 0.875\n",
            "Test Loss: 0.621623694896698|Test Accuracy : 0.78125\n",
            "Test Loss: 0.28107860684394836|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3902377188205719|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4009891450405121|Test Accuracy : 0.90625\n",
            "Test Loss: 0.48447155952453613|Test Accuracy : 0.90625\n",
            "Test Loss: 0.25467610359191895|Test Accuracy : 0.90625\n",
            "Test Loss: 0.221002995967865|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3521259129047394|Test Accuracy : 0.9375\n",
            "Test Loss: 0.35034263134002686|Test Accuracy : 0.9375\n",
            "Test Loss: 2.404195547103882|Test Accuracy : 0.875\n",
            "Test Loss: 0.36846786737442017|Test Accuracy : 0.875\n",
            "Test Loss: 0.09049924463033676|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3996133506298065|Test Accuracy : 0.875\n",
            "Test Loss: 0.28941965103149414|Test Accuracy : 0.90625\n",
            "Test Loss: 0.36255934834480286|Test Accuracy : 0.875\n",
            "Test Loss: 0.4025193452835083|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5994613766670227|Test Accuracy : 0.84375\n",
            "Test Loss: 0.26033130288124084|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7020359635353088|Test Accuracy : 0.75\n",
            "Test Loss: 0.050508011132478714|Test Accuracy : 0.96875\n",
            "Test Loss: 0.19054321944713593|Test Accuracy : 0.96875\n",
            "Test Loss: 0.15655028820037842|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5251930356025696|Test Accuracy : 0.875\n",
            "Test Loss: 0.8135765790939331|Test Accuracy : 0.84375\n",
            "Test Loss: 0.1580134928226471|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3402872383594513|Test Accuracy : 0.9375\n",
            "Test Loss: 0.45100149512290955|Test Accuracy : 0.84375\n",
            "Test Loss: 0.39496397972106934|Test Accuracy : 0.875\n",
            "Test Loss: 1.0288646221160889|Test Accuracy : 0.78125\n",
            "Test Loss: 0.4739622473716736|Test Accuracy : 0.84375\n",
            "Test Loss: 0.12179699540138245|Test Accuracy : 0.96875\n",
            "Test Loss: 0.2703205943107605|Test Accuracy : 0.9375\n",
            "Test Loss: 0.10767561197280884|Test Accuracy : 0.96875\n",
            "Test Loss: 0.9357949495315552|Test Accuracy : 0.875\n",
            "Test Loss: 0.1822877824306488|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3052864372730255|Test Accuracy : 0.84375\n",
            "Test Loss: 0.11911582946777344|Test Accuracy : 0.96875\n",
            "Test Loss: 0.748242974281311|Test Accuracy : 0.875\n",
            "Test Loss: 0.2849937975406647|Test Accuracy : 0.90625\n",
            "Test Loss: 1.2080457210540771|Test Accuracy : 0.8125\n",
            "Test Loss: 0.32635533809661865|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5572890639305115|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5000699758529663|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3615396320819855|Test Accuracy : 0.90625\n",
            "Test Loss: 0.39067214727401733|Test Accuracy : 0.90625\n",
            "Test Loss: 0.746675431728363|Test Accuracy : 0.875\n",
            "Test Loss: 0.09645485877990723|Test Accuracy : 0.96875\n",
            "Test Loss: 0.8609050512313843|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4138406217098236|Test Accuracy : 0.875\n",
            "Test Loss: 1.2673853635787964|Test Accuracy : 0.78125\n",
            "Test Loss: 0.633965015411377|Test Accuracy : 0.84375\n",
            "Test Loss: 0.36476173996925354|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6826322078704834|Test Accuracy : 0.78125\n",
            "Test Loss: 0.978305459022522|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5083708763122559|Test Accuracy : 0.875\n",
            "Test Loss: 0.6269798278808594|Test Accuracy : 0.90625\n",
            "Test Loss: 0.055097803473472595|Test Accuracy : 1.0\n",
            "Test Loss: 0.4029013514518738|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3218836188316345|Test Accuracy : 0.90625\n",
            "Test Loss: 0.47512078285217285|Test Accuracy : 0.875\n",
            "Test Loss: 0.1538180261850357|Test Accuracy : 0.9375\n",
            "Test Loss: 0.48674431443214417|Test Accuracy : 0.75\n",
            "Test Loss: 0.11638955771923065|Test Accuracy : 0.96875\n",
            "Test Loss: 0.9380283355712891|Test Accuracy : 0.9375\n",
            "Test Loss: 0.386849045753479|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3587692379951477|Test Accuracy : 0.875\n",
            "Test Loss: 0.36520618200302124|Test Accuracy : 0.875\n",
            "Test Loss: 0.36470940709114075|Test Accuracy : 0.875\n",
            "Test Loss: 0.3817197382450104|Test Accuracy : 0.8125\n",
            "Test Loss: 1.560091257095337|Test Accuracy : 0.78125\n",
            "Test Loss: 0.094969742000103|Test Accuracy : 0.96875\n",
            "Test Loss: 0.10665693879127502|Test Accuracy : 1.0\n",
            "Test Loss: 0.45818859338760376|Test Accuracy : 0.90625\n",
            "Test Loss: 0.49027130007743835|Test Accuracy : 0.875\n",
            "Test Loss: 1.3763082027435303|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5024800300598145|Test Accuracy : 0.84375\n",
            "Test Loss: 0.20548421144485474|Test Accuracy : 0.875\n",
            "Test Loss: 0.5649763345718384|Test Accuracy : 0.875\n",
            "Test Loss: 0.5330118536949158|Test Accuracy : 0.84375\n",
            "Test Loss: 0.15793776512145996|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5644717216491699|Test Accuracy : 0.90625\n",
            "Test Loss: 1.079470157623291|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5285241603851318|Test Accuracy : 0.84375\n",
            "Test Loss: 0.16332343220710754|Test Accuracy : 0.9375\n",
            "Test Loss: 0.31933581829071045|Test Accuracy : 0.875\n",
            "Test Loss: 0.16005243360996246|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3718171715736389|Test Accuracy : 0.78125\n",
            "Test Loss: 0.4807063937187195|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5221044421195984|Test Accuracy : 0.84375\n",
            "Test Loss: 0.05627967789769173|Test Accuracy : 1.0\n",
            "Test Loss: 0.15886349976062775|Test Accuracy : 0.9375\n",
            "Test Loss: 0.9709686636924744|Test Accuracy : 0.75\n",
            "Test Loss: 1.1783696413040161|Test Accuracy : 0.84375\n",
            "Test Loss: 0.31251612305641174|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2466585487127304|Test Accuracy : 0.875\n",
            "Test Loss: 0.2726171016693115|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7483382821083069|Test Accuracy : 0.875\n",
            "Test Loss: 0.1701643019914627|Test Accuracy : 0.9375\n",
            "Test Loss: 0.24538660049438477|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4634123146533966|Test Accuracy : 0.875\n",
            "Test Loss: 1.261857032775879|Test Accuracy : 0.8125\n",
            "Test Loss: 0.32422930002212524|Test Accuracy : 0.875\n",
            "Test Loss: 0.8488385677337646|Test Accuracy : 0.78125\n",
            "Test Loss: 0.681411862373352|Test Accuracy : 0.84375\n",
            "Test Loss: 0.8988164663314819|Test Accuracy : 0.8125\n",
            "Test Loss: 0.27522385120391846|Test Accuracy : 0.9375\n",
            "Test Loss: 1.0918291807174683|Test Accuracy : 0.90625\n",
            "Test Loss: 0.9182078242301941|Test Accuracy : 0.78125\n",
            "Test Loss: 1.6133986711502075|Test Accuracy : 0.78125\n",
            "Test Loss: 0.4710322320461273|Test Accuracy : 0.875\n",
            "Test Loss: 0.7734403014183044|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3101271390914917|Test Accuracy : 0.84375\n",
            "Test Loss: 0.23461009562015533|Test Accuracy : 0.90625\n",
            "Test Loss: 0.45848900079727173|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4517318904399872|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3668932616710663|Test Accuracy : 0.90625\n",
            "Test Loss: 0.18230709433555603|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5377708673477173|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5378769040107727|Test Accuracy : 0.90625\n",
            "Test Loss: 1.3584750890731812|Test Accuracy : 0.71875\n",
            "Test Loss: 0.5210961103439331|Test Accuracy : 0.8125\n",
            "Test Loss: 0.1492818295955658|Test Accuracy : 0.9375\n",
            "Test Loss: 0.30728888511657715|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5288941264152527|Test Accuracy : 0.8125\n",
            "Test Loss: 0.28364527225494385|Test Accuracy : 0.9375\n",
            "Test Loss: 0.46205827593803406|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3165328800678253|Test Accuracy : 0.875\n",
            "Test Loss: 0.9582144618034363|Test Accuracy : 0.8125\n",
            "Test Loss: 0.20768091082572937|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2854531407356262|Test Accuracy : 0.84375\n",
            "Test Loss: 0.31878986954689026|Test Accuracy : 0.90625\n",
            "Test Loss: 0.575819194316864|Test Accuracy : 0.78125\n",
            "Test Loss: 0.22406332194805145|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5452725887298584|Test Accuracy : 0.875\n",
            "Test Loss: 0.3479648232460022|Test Accuracy : 0.84375\n",
            "Test Loss: 0.9968745112419128|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4324065148830414|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6590547561645508|Test Accuracy : 0.875\n",
            "Test Loss: 0.2414529025554657|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2855091094970703|Test Accuracy : 0.90625\n",
            "Test Loss: 0.29671719670295715|Test Accuracy : 0.875\n",
            "Test Loss: 0.4223645329475403|Test Accuracy : 0.90625\n",
            "Test Loss: 0.18023937940597534|Test Accuracy : 0.9375\n",
            "Test Loss: 0.20184634625911713|Test Accuracy : 0.9375\n",
            "Test Loss: 0.08755216002464294|Test Accuracy : 0.96875\n",
            "Test Loss: 1.0291633605957031|Test Accuracy : 0.75\n",
            "Test Loss: 0.12530997395515442|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3504108786582947|Test Accuracy : 0.875\n",
            "Test Loss: 0.46971723437309265|Test Accuracy : 0.875\n",
            "Test Loss: 0.39892643690109253|Test Accuracy : 0.875\n",
            "Test Loss: 0.3259434103965759|Test Accuracy : 0.875\n",
            "Test Loss: 0.9880037307739258|Test Accuracy : 0.78125\n",
            "Test Loss: 1.063727617263794|Test Accuracy : 0.78125\n",
            "Test Loss: 0.46199408173561096|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6440058946609497|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3574714958667755|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6484602689743042|Test Accuracy : 0.84375\n",
            "Test Loss: 0.9818735122680664|Test Accuracy : 0.84375\n",
            "Test Loss: 1.2520450353622437|Test Accuracy : 0.75\n",
            "Test Loss: 0.8284546136856079|Test Accuracy : 0.8125\n",
            "Test Loss: 0.31503477692604065|Test Accuracy : 0.875\n",
            "Test Loss: 0.3494219183921814|Test Accuracy : 0.875\n",
            "Test Loss: 0.3664225935935974|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4042101800441742|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6371245384216309|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6199295520782471|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6265714764595032|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3046899437904358|Test Accuracy : 0.9375\n",
            "Test Loss: 0.21915581822395325|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2634603977203369|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3976735770702362|Test Accuracy : 0.84375\n",
            "Test Loss: 1.1978034973144531|Test Accuracy : 0.84375\n",
            "Test Loss: 0.13340511918067932|Test Accuracy : 0.96875\n",
            "Test Loss: 0.21341313421726227|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3012334406375885|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3007076680660248|Test Accuracy : 0.90625\n",
            "Test Loss: 0.61403489112854|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5219293832778931|Test Accuracy : 0.8125\n",
            "Test Loss: 0.38247084617614746|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3821033239364624|Test Accuracy : 0.84375\n",
            "Test Loss: 2.28180193901062|Test Accuracy : 0.84375\n",
            "Test Loss: 0.1946014165878296|Test Accuracy : 0.90625\n",
            "Test Loss: 0.546301007270813|Test Accuracy : 0.84375\n",
            "Test Loss: 1.2570115327835083|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5176721811294556|Test Accuracy : 0.75\n",
            "Test Loss: 0.48658257722854614|Test Accuracy : 0.84375\n",
            "Test Loss: 0.43948861956596375|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5781551599502563|Test Accuracy : 0.6875\n",
            "Epoch :98\n",
            "Batch : 0|Training Loss: 0.07277128845453262|Training Accuracy : 1.0\n",
            "Batch : 1|Training Loss: 0.12736137211322784|Training Accuracy : 0.96875\n",
            "Batch : 2|Training Loss: 0.12360449135303497|Training Accuracy : 0.96875\n",
            "Batch : 3|Training Loss: 0.3123871982097626|Training Accuracy : 0.875\n",
            "Batch : 4|Training Loss: 0.0555461123585701|Training Accuracy : 1.0\n",
            "Batch : 5|Training Loss: 0.24586361646652222|Training Accuracy : 0.90625\n",
            "Batch : 6|Training Loss: 0.23862139880657196|Training Accuracy : 0.875\n",
            "Batch : 7|Training Loss: 0.08781855553388596|Training Accuracy : 0.96875\n",
            "Batch : 8|Training Loss: 0.1172257587313652|Training Accuracy : 0.96875\n",
            "Batch : 9|Training Loss: 0.38761672377586365|Training Accuracy : 0.875\n",
            "Batch : 10|Training Loss: 0.2043902724981308|Training Accuracy : 0.9375\n",
            "Batch : 11|Training Loss: 0.12671643495559692|Training Accuracy : 0.96875\n",
            "Batch : 12|Training Loss: 0.1667759120464325|Training Accuracy : 0.9375\n",
            "Batch : 13|Training Loss: 0.09323791414499283|Training Accuracy : 0.96875\n",
            "Batch : 14|Training Loss: 0.046360310167074203|Training Accuracy : 1.0\n",
            "Batch : 15|Training Loss: 0.11106608808040619|Training Accuracy : 0.9375\n",
            "Batch : 16|Training Loss: 0.3330499231815338|Training Accuracy : 0.9375\n",
            "Batch : 17|Training Loss: 0.45410120487213135|Training Accuracy : 0.875\n",
            "Batch : 18|Training Loss: 0.13605521619319916|Training Accuracy : 0.9375\n",
            "Batch : 19|Training Loss: 0.18777942657470703|Training Accuracy : 0.90625\n",
            "Batch : 20|Training Loss: 0.572429895401001|Training Accuracy : 0.8125\n",
            "Batch : 21|Training Loss: 0.2718927264213562|Training Accuracy : 0.90625\n",
            "Batch : 22|Training Loss: 0.041226886212825775|Training Accuracy : 1.0\n",
            "Batch : 23|Training Loss: 0.28981655836105347|Training Accuracy : 0.90625\n",
            "Batch : 24|Training Loss: 0.26311829686164856|Training Accuracy : 0.875\n",
            "Batch : 25|Training Loss: 0.09412571042776108|Training Accuracy : 0.9375\n",
            "Batch : 26|Training Loss: 0.19283492863178253|Training Accuracy : 0.9375\n",
            "Batch : 27|Training Loss: 0.15906788408756256|Training Accuracy : 0.9375\n",
            "Batch : 28|Training Loss: 0.28074946999549866|Training Accuracy : 0.9375\n",
            "Batch : 29|Training Loss: 0.13206394016742706|Training Accuracy : 0.96875\n",
            "Batch : 30|Training Loss: 0.12202491611242294|Training Accuracy : 0.90625\n",
            "Batch : 31|Training Loss: 0.32994329929351807|Training Accuracy : 0.90625\n",
            "Batch : 32|Training Loss: 0.16469912230968475|Training Accuracy : 0.90625\n",
            "Batch : 33|Training Loss: 0.03635162115097046|Training Accuracy : 1.0\n",
            "Batch : 34|Training Loss: 0.08579382300376892|Training Accuracy : 0.96875\n",
            "Batch : 35|Training Loss: 0.2015085220336914|Training Accuracy : 0.9375\n",
            "Batch : 36|Training Loss: 0.18273207545280457|Training Accuracy : 0.9375\n",
            "Batch : 37|Training Loss: 0.16091099381446838|Training Accuracy : 0.9375\n",
            "Batch : 38|Training Loss: 0.22225889563560486|Training Accuracy : 0.9375\n",
            "Batch : 39|Training Loss: 0.056840680539608|Training Accuracy : 0.96875\n",
            "Batch : 40|Training Loss: 0.07996554672718048|Training Accuracy : 0.9375\n",
            "Batch : 41|Training Loss: 0.08566751331090927|Training Accuracy : 0.96875\n",
            "Batch : 42|Training Loss: 0.14090299606323242|Training Accuracy : 0.9375\n",
            "Batch : 43|Training Loss: 0.19324815273284912|Training Accuracy : 0.90625\n",
            "Batch : 44|Training Loss: 0.19869978725910187|Training Accuracy : 0.90625\n",
            "Batch : 45|Training Loss: 0.10065441578626633|Training Accuracy : 0.9375\n",
            "Batch : 46|Training Loss: 0.05933263525366783|Training Accuracy : 1.0\n",
            "Batch : 47|Training Loss: 0.19352935254573822|Training Accuracy : 0.90625\n",
            "Batch : 48|Training Loss: 0.13213995099067688|Training Accuracy : 0.96875\n",
            "Batch : 49|Training Loss: 0.13477204740047455|Training Accuracy : 0.96875\n",
            "Batch : 50|Training Loss: 0.1369089037179947|Training Accuracy : 0.9375\n",
            "Batch : 51|Training Loss: 0.19098146259784698|Training Accuracy : 0.96875\n",
            "Batch : 52|Training Loss: 0.2146664559841156|Training Accuracy : 0.90625\n",
            "Batch : 53|Training Loss: 0.3936860263347626|Training Accuracy : 0.84375\n",
            "Batch : 54|Training Loss: 0.40793436765670776|Training Accuracy : 0.875\n",
            "Batch : 55|Training Loss: 0.27206283807754517|Training Accuracy : 0.9375\n",
            "Batch : 56|Training Loss: 0.19182497262954712|Training Accuracy : 0.875\n",
            "Batch : 57|Training Loss: 0.2109144926071167|Training Accuracy : 0.875\n",
            "Batch : 58|Training Loss: 0.37102580070495605|Training Accuracy : 0.84375\n",
            "Batch : 59|Training Loss: 0.06788168847560883|Training Accuracy : 1.0\n",
            "Batch : 60|Training Loss: 0.18504783511161804|Training Accuracy : 0.90625\n",
            "Batch : 61|Training Loss: 0.2658632695674896|Training Accuracy : 0.90625\n",
            "Batch : 62|Training Loss: 0.0827997624874115|Training Accuracy : 1.0\n",
            "Batch : 63|Training Loss: 0.11244432628154755|Training Accuracy : 0.96875\n",
            "Batch : 64|Training Loss: 0.18242774903774261|Training Accuracy : 0.9375\n",
            "Batch : 65|Training Loss: 0.228383406996727|Training Accuracy : 0.875\n",
            "Batch : 66|Training Loss: 0.28653669357299805|Training Accuracy : 0.875\n",
            "Batch : 67|Training Loss: 0.1397910863161087|Training Accuracy : 0.96875\n",
            "Batch : 68|Training Loss: 0.21346405148506165|Training Accuracy : 0.90625\n",
            "Batch : 69|Training Loss: 0.06433665752410889|Training Accuracy : 1.0\n",
            "Batch : 70|Training Loss: 0.21536973118782043|Training Accuracy : 0.96875\n",
            "Batch : 71|Training Loss: 0.09369754791259766|Training Accuracy : 0.96875\n",
            "Batch : 72|Training Loss: 0.12327519059181213|Training Accuracy : 0.9375\n",
            "Batch : 73|Training Loss: 0.25580334663391113|Training Accuracy : 0.84375\n",
            "Batch : 74|Training Loss: 0.04205457866191864|Training Accuracy : 1.0\n",
            "Batch : 75|Training Loss: 0.08820825815200806|Training Accuracy : 0.96875\n",
            "Batch : 76|Training Loss: 0.3349856734275818|Training Accuracy : 0.90625\n",
            "Batch : 77|Training Loss: 0.21163341403007507|Training Accuracy : 0.90625\n",
            "Batch : 78|Training Loss: 0.23013091087341309|Training Accuracy : 0.875\n",
            "Batch : 79|Training Loss: 0.3978699743747711|Training Accuracy : 0.8125\n",
            "Batch : 80|Training Loss: 0.1104692667722702|Training Accuracy : 0.96875\n",
            "Batch : 81|Training Loss: 0.2831902801990509|Training Accuracy : 0.90625\n",
            "Batch : 82|Training Loss: 0.30968645215034485|Training Accuracy : 0.875\n",
            "Batch : 83|Training Loss: 0.14982910454273224|Training Accuracy : 0.96875\n",
            "Batch : 84|Training Loss: 0.12809422612190247|Training Accuracy : 0.96875\n",
            "Batch : 85|Training Loss: 0.20950502157211304|Training Accuracy : 0.90625\n",
            "Batch : 86|Training Loss: 0.1595846265554428|Training Accuracy : 0.90625\n",
            "Batch : 87|Training Loss: 0.06281154602766037|Training Accuracy : 1.0\n",
            "Batch : 88|Training Loss: 0.15554247796535492|Training Accuracy : 0.9375\n",
            "Batch : 89|Training Loss: 0.23873166739940643|Training Accuracy : 0.9375\n",
            "Batch : 90|Training Loss: 0.2802332043647766|Training Accuracy : 0.90625\n",
            "Batch : 91|Training Loss: 0.315887987613678|Training Accuracy : 0.875\n",
            "Batch : 92|Training Loss: 0.10055164247751236|Training Accuracy : 0.96875\n",
            "Batch : 93|Training Loss: 0.3697499930858612|Training Accuracy : 0.8125\n",
            "Batch : 94|Training Loss: 0.336300253868103|Training Accuracy : 0.84375\n",
            "Batch : 95|Training Loss: 0.026972690597176552|Training Accuracy : 1.0\n",
            "Batch : 96|Training Loss: 0.13412782549858093|Training Accuracy : 0.9375\n",
            "Batch : 97|Training Loss: 0.29696083068847656|Training Accuracy : 0.84375\n",
            "Batch : 98|Training Loss: 0.14124852418899536|Training Accuracy : 0.9375\n",
            "Batch : 99|Training Loss: 0.03245696797966957|Training Accuracy : 1.0\n",
            "Batch : 100|Training Loss: 0.11376582831144333|Training Accuracy : 0.96875\n",
            "Batch : 101|Training Loss: 0.13689765334129333|Training Accuracy : 0.9375\n",
            "Batch : 102|Training Loss: 0.20764712989330292|Training Accuracy : 0.90625\n",
            "Batch : 103|Training Loss: 0.18822531402111053|Training Accuracy : 0.90625\n",
            "Batch : 104|Training Loss: 0.147396519780159|Training Accuracy : 0.9375\n",
            "Batch : 105|Training Loss: 0.20252688229084015|Training Accuracy : 0.9375\n",
            "Batch : 106|Training Loss: 0.030145902186632156|Training Accuracy : 1.0\n",
            "Batch : 107|Training Loss: 0.05636944621801376|Training Accuracy : 1.0\n",
            "Batch : 108|Training Loss: 0.09984980523586273|Training Accuracy : 0.96875\n",
            "Batch : 109|Training Loss: 0.28267112374305725|Training Accuracy : 0.90625\n",
            "Batch : 110|Training Loss: 0.15319035947322845|Training Accuracy : 0.9375\n",
            "Batch : 111|Training Loss: 0.12169230729341507|Training Accuracy : 0.96875\n",
            "Batch : 112|Training Loss: 0.07323296368122101|Training Accuracy : 0.96875\n",
            "Batch : 113|Training Loss: 0.3985964357852936|Training Accuracy : 0.84375\n",
            "Batch : 114|Training Loss: 0.40707215666770935|Training Accuracy : 0.8125\n",
            "Batch : 115|Training Loss: 0.1113879382610321|Training Accuracy : 0.96875\n",
            "Batch : 116|Training Loss: 0.15100634098052979|Training Accuracy : 0.90625\n",
            "Batch : 117|Training Loss: 0.14682185649871826|Training Accuracy : 0.90625\n",
            "Batch : 118|Training Loss: 0.23982292413711548|Training Accuracy : 0.90625\n",
            "Batch : 119|Training Loss: 0.2204279750585556|Training Accuracy : 0.84375\n",
            "Batch : 120|Training Loss: 0.35824495553970337|Training Accuracy : 0.875\n",
            "Batch : 121|Training Loss: 0.14787262678146362|Training Accuracy : 0.9375\n",
            "Batch : 122|Training Loss: 0.3088109493255615|Training Accuracy : 0.90625\n",
            "Batch : 123|Training Loss: 0.11792534589767456|Training Accuracy : 0.96875\n",
            "Batch : 124|Training Loss: 0.2912366986274719|Training Accuracy : 0.875\n",
            "Batch : 125|Training Loss: 0.2008676826953888|Training Accuracy : 0.875\n",
            "Batch : 126|Training Loss: 0.23917247354984283|Training Accuracy : 0.9375\n",
            "Batch : 127|Training Loss: 0.21955423057079315|Training Accuracy : 0.90625\n",
            "Batch : 128|Training Loss: 0.120075523853302|Training Accuracy : 0.9375\n",
            "Batch : 129|Training Loss: 0.07786925137042999|Training Accuracy : 0.96875\n",
            "Batch : 130|Training Loss: 0.12790437042713165|Training Accuracy : 0.9375\n",
            "Batch : 131|Training Loss: 0.05044196918606758|Training Accuracy : 0.96875\n",
            "Batch : 132|Training Loss: 0.2213132232427597|Training Accuracy : 0.875\n",
            "Batch : 133|Training Loss: 0.24821996688842773|Training Accuracy : 0.875\n",
            "Batch : 134|Training Loss: 0.1685001254081726|Training Accuracy : 0.9375\n",
            "Batch : 135|Training Loss: 0.13942036032676697|Training Accuracy : 0.9375\n",
            "Batch : 136|Training Loss: 0.10152082145214081|Training Accuracy : 0.9375\n",
            "Batch : 137|Training Loss: 0.23164436221122742|Training Accuracy : 0.90625\n",
            "Batch : 138|Training Loss: 0.14435012638568878|Training Accuracy : 0.9375\n",
            "Batch : 139|Training Loss: 0.07086766511201859|Training Accuracy : 1.0\n",
            "Batch : 140|Training Loss: 0.17123839259147644|Training Accuracy : 0.90625\n",
            "Batch : 141|Training Loss: 0.25256863236427307|Training Accuracy : 0.9375\n",
            "Batch : 142|Training Loss: 0.14753150939941406|Training Accuracy : 0.96875\n",
            "Batch : 143|Training Loss: 0.061773549765348434|Training Accuracy : 0.96875\n",
            "Batch : 144|Training Loss: 0.20058298110961914|Training Accuracy : 0.875\n",
            "Batch : 145|Training Loss: 0.19106946885585785|Training Accuracy : 0.90625\n",
            "Batch : 146|Training Loss: 0.2191435694694519|Training Accuracy : 0.9375\n",
            "Batch : 147|Training Loss: 0.18650545179843903|Training Accuracy : 0.9375\n",
            "Batch : 148|Training Loss: 0.13834694027900696|Training Accuracy : 0.90625\n",
            "Batch : 149|Training Loss: 0.12767450511455536|Training Accuracy : 0.96875\n",
            "Batch : 150|Training Loss: 0.05067724734544754|Training Accuracy : 1.0\n",
            "Batch : 151|Training Loss: 0.3151342272758484|Training Accuracy : 0.875\n",
            "Batch : 152|Training Loss: 0.07353867590427399|Training Accuracy : 0.96875\n",
            "Batch : 153|Training Loss: 0.2736414670944214|Training Accuracy : 0.90625\n",
            "Batch : 154|Training Loss: 0.29253557324409485|Training Accuracy : 0.875\n",
            "Batch : 155|Training Loss: 0.1918696016073227|Training Accuracy : 0.9375\n",
            "Batch : 156|Training Loss: 0.09311575442552567|Training Accuracy : 0.9375\n",
            "Batch : 157|Training Loss: 0.029692286625504494|Training Accuracy : 1.0\n",
            "Batch : 158|Training Loss: 0.24496160447597504|Training Accuracy : 0.9375\n",
            "Batch : 159|Training Loss: 0.15852271020412445|Training Accuracy : 0.9375\n",
            "Batch : 160|Training Loss: 0.3993116319179535|Training Accuracy : 0.8125\n",
            "Batch : 161|Training Loss: 0.33475935459136963|Training Accuracy : 0.84375\n",
            "Batch : 162|Training Loss: 0.11296364665031433|Training Accuracy : 0.9375\n",
            "Batch : 163|Training Loss: 0.1910020411014557|Training Accuracy : 0.9375\n",
            "Batch : 164|Training Loss: 0.21029244363307953|Training Accuracy : 0.9375\n",
            "Batch : 165|Training Loss: 0.4935246706008911|Training Accuracy : 0.875\n",
            "Batch : 166|Training Loss: 0.15213105082511902|Training Accuracy : 0.9375\n",
            "Batch : 167|Training Loss: 0.2579858601093292|Training Accuracy : 0.90625\n",
            "Batch : 168|Training Loss: 0.2203221321105957|Training Accuracy : 0.9375\n",
            "Batch : 169|Training Loss: 0.27817583084106445|Training Accuracy : 0.90625\n",
            "Batch : 170|Training Loss: 0.3940586447715759|Training Accuracy : 0.875\n",
            "Batch : 171|Training Loss: 0.14820918440818787|Training Accuracy : 0.90625\n",
            "Batch : 172|Training Loss: 0.31574177742004395|Training Accuracy : 0.875\n",
            "Batch : 173|Training Loss: 0.1580015867948532|Training Accuracy : 0.90625\n",
            "Batch : 174|Training Loss: 0.05310307443141937|Training Accuracy : 0.96875\n",
            "Batch : 175|Training Loss: 0.14973798394203186|Training Accuracy : 0.9375\n",
            "Batch : 176|Training Loss: 0.10054868459701538|Training Accuracy : 0.9375\n",
            "Batch : 177|Training Loss: 0.14255930483341217|Training Accuracy : 0.9375\n",
            "Batch : 178|Training Loss: 0.09601631760597229|Training Accuracy : 0.96875\n",
            "Batch : 179|Training Loss: 0.169986292719841|Training Accuracy : 0.9375\n",
            "Batch : 180|Training Loss: 0.1077873706817627|Training Accuracy : 0.9375\n",
            "Batch : 181|Training Loss: 0.1733936369419098|Training Accuracy : 0.9375\n",
            "Batch : 182|Training Loss: 0.18567518889904022|Training Accuracy : 0.9375\n",
            "Batch : 183|Training Loss: 0.22111494839191437|Training Accuracy : 0.90625\n",
            "Batch : 184|Training Loss: 0.18026143312454224|Training Accuracy : 0.875\n",
            "Batch : 185|Training Loss: 0.2410578727722168|Training Accuracy : 0.90625\n",
            "Batch : 186|Training Loss: 0.27232295274734497|Training Accuracy : 0.875\n",
            "Batch : 187|Training Loss: 0.034072548151016235|Training Accuracy : 1.0\n",
            "Batch : 188|Training Loss: 0.07041266560554504|Training Accuracy : 1.0\n",
            "Batch : 189|Training Loss: 0.13066844642162323|Training Accuracy : 0.96875\n",
            "Batch : 190|Training Loss: 0.2968137860298157|Training Accuracy : 0.90625\n",
            "Batch : 191|Training Loss: 0.0945947989821434|Training Accuracy : 0.9375\n",
            "Batch : 192|Training Loss: 0.09535889327526093|Training Accuracy : 1.0\n",
            "Batch : 193|Training Loss: 0.18853110074996948|Training Accuracy : 0.9375\n",
            "Batch : 194|Training Loss: 0.131558358669281|Training Accuracy : 0.90625\n",
            "Batch : 195|Training Loss: 0.31048688292503357|Training Accuracy : 0.84375\n",
            "Batch : 196|Training Loss: 0.028272811323404312|Training Accuracy : 1.0\n",
            "Batch : 197|Training Loss: 0.1817777305841446|Training Accuracy : 0.96875\n",
            "Batch : 198|Training Loss: 0.11261565238237381|Training Accuracy : 0.9375\n",
            "Batch : 199|Training Loss: 0.20110158622264862|Training Accuracy : 0.90625\n",
            "Batch : 200|Training Loss: 0.02483592927455902|Training Accuracy : 1.0\n",
            "Batch : 201|Training Loss: 0.22183384001255035|Training Accuracy : 0.90625\n",
            "Batch : 202|Training Loss: 0.15935969352722168|Training Accuracy : 0.9375\n",
            "Batch : 203|Training Loss: 0.1910829395055771|Training Accuracy : 0.9375\n",
            "Batch : 204|Training Loss: 0.22049030661582947|Training Accuracy : 0.90625\n",
            "Batch : 205|Training Loss: 0.5416363477706909|Training Accuracy : 0.8125\n",
            "Batch : 206|Training Loss: 0.16144894063472748|Training Accuracy : 0.96875\n",
            "Batch : 207|Training Loss: 0.043707337230443954|Training Accuracy : 1.0\n",
            "Batch : 208|Training Loss: 0.1143728494644165|Training Accuracy : 0.96875\n",
            "Batch : 209|Training Loss: 0.2384663224220276|Training Accuracy : 0.875\n",
            "Batch : 210|Training Loss: 0.24029037356376648|Training Accuracy : 0.875\n",
            "Batch : 211|Training Loss: 0.06882613897323608|Training Accuracy : 1.0\n",
            "Batch : 212|Training Loss: 0.176202192902565|Training Accuracy : 0.875\n",
            "Batch : 213|Training Loss: 0.25362062454223633|Training Accuracy : 0.9375\n",
            "Batch : 214|Training Loss: 0.16571661829948425|Training Accuracy : 0.90625\n",
            "Batch : 215|Training Loss: 0.21231086552143097|Training Accuracy : 0.90625\n",
            "Batch : 216|Training Loss: 0.26572731137275696|Training Accuracy : 0.90625\n",
            "Batch : 217|Training Loss: 0.2020075023174286|Training Accuracy : 0.90625\n",
            "Batch : 218|Training Loss: 0.2556834816932678|Training Accuracy : 0.90625\n",
            "Batch : 219|Training Loss: 0.18950869143009186|Training Accuracy : 0.90625\n",
            "Batch : 220|Training Loss: 0.10316531360149384|Training Accuracy : 0.96875\n",
            "Batch : 221|Training Loss: 0.10436563938856125|Training Accuracy : 1.0\n",
            "Batch : 222|Training Loss: 0.06903026252985|Training Accuracy : 1.0\n",
            "Batch : 223|Training Loss: 0.12849381566047668|Training Accuracy : 0.9375\n",
            "Batch : 224|Training Loss: 0.12049320340156555|Training Accuracy : 0.9375\n",
            "Batch : 225|Training Loss: 0.19407536089420319|Training Accuracy : 0.875\n",
            "Batch : 226|Training Loss: 0.1090955138206482|Training Accuracy : 0.96875\n",
            "Batch : 227|Training Loss: 0.2304902821779251|Training Accuracy : 0.90625\n",
            "Batch : 228|Training Loss: 0.179586261510849|Training Accuracy : 0.90625\n",
            "Batch : 229|Training Loss: 0.3192524313926697|Training Accuracy : 0.90625\n",
            "Batch : 230|Training Loss: 0.15886861085891724|Training Accuracy : 0.96875\n",
            "Batch : 231|Training Loss: 0.0780600979924202|Training Accuracy : 0.96875\n",
            "Batch : 232|Training Loss: 0.06458324939012527|Training Accuracy : 1.0\n",
            "Batch : 233|Training Loss: 0.15215903520584106|Training Accuracy : 0.9375\n",
            "Batch : 234|Training Loss: 0.1446980983018875|Training Accuracy : 0.9375\n",
            "Batch : 235|Training Loss: 0.14933092892169952|Training Accuracy : 0.96875\n",
            "Batch : 236|Training Loss: 0.17179065942764282|Training Accuracy : 0.9375\n",
            "Batch : 237|Training Loss: 0.31155866384506226|Training Accuracy : 0.9375\n",
            "Batch : 238|Training Loss: 0.12671945989131927|Training Accuracy : 0.96875\n",
            "Batch : 239|Training Loss: 0.06275603175163269|Training Accuracy : 0.96875\n",
            "Batch : 240|Training Loss: 0.22103747725486755|Training Accuracy : 0.9375\n",
            "Batch : 241|Training Loss: 0.13692514598369598|Training Accuracy : 0.90625\n",
            "Batch : 242|Training Loss: 0.22624532878398895|Training Accuracy : 0.9375\n",
            "Batch : 243|Training Loss: 0.10876864194869995|Training Accuracy : 0.9375\n",
            "Batch : 244|Training Loss: 0.3087617754936218|Training Accuracy : 0.90625\n",
            "Batch : 245|Training Loss: 0.09607844054698944|Training Accuracy : 0.96875\n",
            "Batch : 246|Training Loss: 0.038027696311473846|Training Accuracy : 1.0\n",
            "Batch : 247|Training Loss: 0.371803343296051|Training Accuracy : 0.8125\n",
            "Batch : 248|Training Loss: 0.17766237258911133|Training Accuracy : 0.96875\n",
            "Batch : 249|Training Loss: 0.14554446935653687|Training Accuracy : 0.9375\n",
            "Batch : 250|Training Loss: 0.39075449109077454|Training Accuracy : 0.84375\n",
            "Batch : 251|Training Loss: 0.17260530591011047|Training Accuracy : 0.9375\n",
            "Batch : 252|Training Loss: 0.08259939402341843|Training Accuracy : 0.96875\n",
            "Batch : 253|Training Loss: 0.23711012303829193|Training Accuracy : 0.90625\n",
            "Batch : 254|Training Loss: 0.24004341661930084|Training Accuracy : 0.875\n",
            "Batch : 255|Training Loss: 0.09446652978658676|Training Accuracy : 0.96875\n",
            "Batch : 256|Training Loss: 0.19827643036842346|Training Accuracy : 0.9375\n",
            "Batch : 257|Training Loss: 0.1013818010687828|Training Accuracy : 0.9375\n",
            "Batch : 258|Training Loss: 0.1761920005083084|Training Accuracy : 0.9375\n",
            "Batch : 259|Training Loss: 0.31046873331069946|Training Accuracy : 0.90625\n",
            "Batch : 260|Training Loss: 0.05715968832373619|Training Accuracy : 1.0\n",
            "Batch : 261|Training Loss: 0.0508265346288681|Training Accuracy : 1.0\n",
            "Batch : 262|Training Loss: 0.11521606892347336|Training Accuracy : 0.9375\n",
            "Batch : 263|Training Loss: 0.046960823237895966|Training Accuracy : 1.0\n",
            "Batch : 264|Training Loss: 0.07266250252723694|Training Accuracy : 0.96875\n",
            "Batch : 265|Training Loss: 0.31213629245758057|Training Accuracy : 0.875\n",
            "Batch : 266|Training Loss: 0.02193578891456127|Training Accuracy : 1.0\n",
            "Batch : 267|Training Loss: 0.20769482851028442|Training Accuracy : 0.9375\n",
            "Batch : 268|Training Loss: 0.2673034071922302|Training Accuracy : 0.90625\n",
            "Batch : 269|Training Loss: 0.323946475982666|Training Accuracy : 0.9375\n",
            "Batch : 270|Training Loss: 0.14818993210792542|Training Accuracy : 0.9375\n",
            "Batch : 271|Training Loss: 0.12798312306404114|Training Accuracy : 0.96875\n",
            "Batch : 272|Training Loss: 0.425089031457901|Training Accuracy : 0.84375\n",
            "Batch : 273|Training Loss: 0.11683852970600128|Training Accuracy : 1.0\n",
            "Batch : 274|Training Loss: 0.13004237413406372|Training Accuracy : 0.96875\n",
            "Batch : 275|Training Loss: 0.1631402224302292|Training Accuracy : 0.90625\n",
            "Batch : 276|Training Loss: 0.14153745770454407|Training Accuracy : 0.9375\n",
            "Batch : 277|Training Loss: 0.12704432010650635|Training Accuracy : 0.9375\n",
            "Batch : 278|Training Loss: 0.46277350187301636|Training Accuracy : 0.875\n",
            "Batch : 279|Training Loss: 0.09091408550739288|Training Accuracy : 1.0\n",
            "Batch : 280|Training Loss: 0.13564272224903107|Training Accuracy : 0.9375\n",
            "Batch : 281|Training Loss: 0.2686026096343994|Training Accuracy : 0.9375\n",
            "Batch : 282|Training Loss: 0.19129280745983124|Training Accuracy : 0.90625\n",
            "Batch : 283|Training Loss: 0.04585566371679306|Training Accuracy : 1.0\n",
            "Batch : 284|Training Loss: 0.20772023499011993|Training Accuracy : 0.90625\n",
            "Batch : 285|Training Loss: 0.3233388662338257|Training Accuracy : 0.875\n",
            "Batch : 286|Training Loss: 0.15963232517242432|Training Accuracy : 0.96875\n",
            "Batch : 287|Training Loss: 0.3810228109359741|Training Accuracy : 0.90625\n",
            "Batch : 288|Training Loss: 0.08382423967123032|Training Accuracy : 0.96875\n",
            "Batch : 289|Training Loss: 0.14013561606407166|Training Accuracy : 0.9375\n",
            "Batch : 290|Training Loss: 0.08117066323757172|Training Accuracy : 0.96875\n",
            "Batch : 291|Training Loss: 0.17459450662136078|Training Accuracy : 0.96875\n",
            "Batch : 292|Training Loss: 0.17836011946201324|Training Accuracy : 0.90625\n",
            "Batch : 293|Training Loss: 0.1189308762550354|Training Accuracy : 0.90625\n",
            "Batch : 294|Training Loss: 0.21693915128707886|Training Accuracy : 0.875\n",
            "Batch : 295|Training Loss: 0.042125288397073746|Training Accuracy : 1.0\n",
            "Batch : 296|Training Loss: 0.12879537045955658|Training Accuracy : 0.90625\n",
            "Batch : 297|Training Loss: 0.09796177595853806|Training Accuracy : 0.96875\n",
            "Batch : 298|Training Loss: 0.2329341471195221|Training Accuracy : 0.90625\n",
            "Batch : 299|Training Loss: 0.22636999189853668|Training Accuracy : 0.875\n",
            "Batch : 300|Training Loss: 0.15853597223758698|Training Accuracy : 0.96875\n",
            "Batch : 301|Training Loss: 0.19567587971687317|Training Accuracy : 0.9375\n",
            "Batch : 302|Training Loss: 0.15448050200939178|Training Accuracy : 0.9375\n",
            "Batch : 303|Training Loss: 0.15169858932495117|Training Accuracy : 0.96875\n",
            "Batch : 304|Training Loss: 0.10477610677480698|Training Accuracy : 0.96875\n",
            "Batch : 305|Training Loss: 0.10169447958469391|Training Accuracy : 1.0\n",
            "Batch : 306|Training Loss: 0.31654778122901917|Training Accuracy : 0.84375\n",
            "Batch : 307|Training Loss: 0.29592663049697876|Training Accuracy : 0.875\n",
            "Batch : 308|Training Loss: 0.08372729271650314|Training Accuracy : 0.96875\n",
            "Batch : 309|Training Loss: 0.2123461365699768|Training Accuracy : 0.9375\n",
            "Batch : 310|Training Loss: 0.26871901750564575|Training Accuracy : 0.90625\n",
            "Batch : 311|Training Loss: 0.09357631951570511|Training Accuracy : 0.9375\n",
            "Batch : 312|Training Loss: 0.35197117924690247|Training Accuracy : 0.90625\n",
            "Batch : 313|Training Loss: 0.07955989986658096|Training Accuracy : 0.96875\n",
            "Batch : 314|Training Loss: 0.10315873473882675|Training Accuracy : 1.0\n",
            "Batch : 315|Training Loss: 0.21489430963993073|Training Accuracy : 0.90625\n",
            "Batch : 316|Training Loss: 0.08395830541849136|Training Accuracy : 0.9375\n",
            "Batch : 317|Training Loss: 0.17849211394786835|Training Accuracy : 0.875\n",
            "Batch : 318|Training Loss: 0.09848668426275253|Training Accuracy : 0.96875\n",
            "Batch : 319|Training Loss: 0.36726126074790955|Training Accuracy : 0.875\n",
            "Batch : 320|Training Loss: 0.49156686663627625|Training Accuracy : 0.75\n",
            "Batch : 321|Training Loss: 0.26337650418281555|Training Accuracy : 0.9375\n",
            "Batch : 322|Training Loss: 0.24801518023014069|Training Accuracy : 0.9375\n",
            "Batch : 323|Training Loss: 0.24000166356563568|Training Accuracy : 0.9375\n",
            "Batch : 324|Training Loss: 0.16395887732505798|Training Accuracy : 0.9375\n",
            "Batch : 325|Training Loss: 0.26598528027534485|Training Accuracy : 0.90625\n",
            "Batch : 326|Training Loss: 0.13828815519809723|Training Accuracy : 0.9375\n",
            "Batch : 327|Training Loss: 0.3493404686450958|Training Accuracy : 0.84375\n",
            "Batch : 328|Training Loss: 0.1477789580821991|Training Accuracy : 0.9375\n",
            "Batch : 329|Training Loss: 0.4569412171840668|Training Accuracy : 0.84375\n",
            "Batch : 330|Training Loss: 0.31991690397262573|Training Accuracy : 0.84375\n",
            "Batch : 331|Training Loss: 0.05341224744915962|Training Accuracy : 0.96875\n",
            "Batch : 332|Training Loss: 0.16095201671123505|Training Accuracy : 0.9375\n",
            "Batch : 333|Training Loss: 0.30663567781448364|Training Accuracy : 0.90625\n",
            "Batch : 334|Training Loss: 0.13585536181926727|Training Accuracy : 0.9375\n",
            "Batch : 335|Training Loss: 0.14764586091041565|Training Accuracy : 0.9375\n",
            "Batch : 336|Training Loss: 0.19085688889026642|Training Accuracy : 0.90625\n",
            "Batch : 337|Training Loss: 0.14536423981189728|Training Accuracy : 0.90625\n",
            "Batch : 338|Training Loss: 0.12242622673511505|Training Accuracy : 0.9375\n",
            "Batch : 339|Training Loss: 0.14674608409404755|Training Accuracy : 0.9375\n",
            "Batch : 340|Training Loss: 0.3222839832305908|Training Accuracy : 0.84375\n",
            "Batch : 341|Training Loss: 0.06730937212705612|Training Accuracy : 0.96875\n",
            "Batch : 342|Training Loss: 0.17261210083961487|Training Accuracy : 0.96875\n",
            "Batch : 343|Training Loss: 0.06705212593078613|Training Accuracy : 1.0\n",
            "Batch : 344|Training Loss: 0.16808849573135376|Training Accuracy : 0.9375\n",
            "Batch : 345|Training Loss: 0.0745302215218544|Training Accuracy : 1.0\n",
            "Batch : 346|Training Loss: 0.12903381884098053|Training Accuracy : 0.90625\n",
            "Batch : 347|Training Loss: 0.18058867752552032|Training Accuracy : 0.9375\n",
            "Batch : 348|Training Loss: 0.2206234186887741|Training Accuracy : 0.875\n",
            "Batch : 349|Training Loss: 0.26054733991622925|Training Accuracy : 0.8125\n",
            "Batch : 350|Training Loss: 0.2161649465560913|Training Accuracy : 0.90625\n",
            "Batch : 351|Training Loss: 0.07259143888950348|Training Accuracy : 0.96875\n",
            "Batch : 352|Training Loss: 0.2839505076408386|Training Accuracy : 0.875\n",
            "Batch : 353|Training Loss: 0.13642793893814087|Training Accuracy : 0.9375\n",
            "Batch : 354|Training Loss: 0.13482451438903809|Training Accuracy : 0.9375\n",
            "Batch : 355|Training Loss: 0.10331524908542633|Training Accuracy : 0.9375\n",
            "Batch : 356|Training Loss: 0.10891489684581757|Training Accuracy : 0.9375\n",
            "Batch : 357|Training Loss: 0.09114296734333038|Training Accuracy : 0.96875\n",
            "Batch : 358|Training Loss: 0.05673848092556|Training Accuracy : 1.0\n",
            "Batch : 359|Training Loss: 0.08855133503675461|Training Accuracy : 0.96875\n",
            "Batch : 360|Training Loss: 0.20653332769870758|Training Accuracy : 0.875\n",
            "Batch : 361|Training Loss: 0.21759061515331268|Training Accuracy : 0.90625\n",
            "Batch : 362|Training Loss: 0.26875999569892883|Training Accuracy : 0.8125\n",
            "Batch : 363|Training Loss: 0.1176052913069725|Training Accuracy : 0.96875\n",
            "Batch : 364|Training Loss: 0.21608956158161163|Training Accuracy : 0.90625\n",
            "Batch : 365|Training Loss: 0.107433021068573|Training Accuracy : 0.9375\n",
            "Batch : 366|Training Loss: 0.3014035224914551|Training Accuracy : 0.875\n",
            "Batch : 367|Training Loss: 0.20737963914871216|Training Accuracy : 0.875\n",
            "Batch : 368|Training Loss: 0.1287573128938675|Training Accuracy : 0.9375\n",
            "Batch : 369|Training Loss: 0.05461069941520691|Training Accuracy : 0.96875\n",
            "Batch : 370|Training Loss: 0.10119543969631195|Training Accuracy : 0.96875\n",
            "Batch : 371|Training Loss: 0.123566634953022|Training Accuracy : 0.9375\n",
            "Batch : 372|Training Loss: 0.12212225794792175|Training Accuracy : 0.9375\n",
            "Batch : 373|Training Loss: 0.18600352108478546|Training Accuracy : 0.9375\n",
            "Batch : 374|Training Loss: 0.11381459981203079|Training Accuracy : 0.96875\n",
            "Batch : 375|Training Loss: 0.27341654896736145|Training Accuracy : 0.90625\n",
            "Batch : 376|Training Loss: 0.19290921092033386|Training Accuracy : 0.96875\n",
            "Batch : 377|Training Loss: 0.2500530183315277|Training Accuracy : 0.875\n",
            "Batch : 378|Training Loss: 0.1451316475868225|Training Accuracy : 0.90625\n",
            "Batch : 379|Training Loss: 0.0167178176343441|Training Accuracy : 1.0\n",
            "Batch : 380|Training Loss: 0.01743829995393753|Training Accuracy : 1.0\n",
            "Batch : 381|Training Loss: 0.36791884899139404|Training Accuracy : 0.875\n",
            "Batch : 382|Training Loss: 0.09449700266122818|Training Accuracy : 1.0\n",
            "Batch : 383|Training Loss: 0.40074291825294495|Training Accuracy : 0.875\n",
            "Batch : 384|Training Loss: 0.23474879562854767|Training Accuracy : 0.90625\n",
            "Batch : 385|Training Loss: 0.15505152940750122|Training Accuracy : 0.9375\n",
            "Batch : 386|Training Loss: 0.031735945492982864|Training Accuracy : 1.0\n",
            "Batch : 387|Training Loss: 0.13922592997550964|Training Accuracy : 0.96875\n",
            "Batch : 388|Training Loss: 0.06965882331132889|Training Accuracy : 1.0\n",
            "Batch : 389|Training Loss: 0.13742607831954956|Training Accuracy : 0.96875\n",
            "Batch : 390|Training Loss: 0.1819944530725479|Training Accuracy : 0.9375\n",
            "Batch : 391|Training Loss: 0.06120915710926056|Training Accuracy : 0.96875\n",
            "Batch : 392|Training Loss: 0.10803616791963577|Training Accuracy : 0.96875\n",
            "Batch : 393|Training Loss: 0.21708646416664124|Training Accuracy : 0.875\n",
            "Batch : 394|Training Loss: 0.30459991097450256|Training Accuracy : 0.90625\n",
            "Batch : 395|Training Loss: 0.13044483959674835|Training Accuracy : 0.9375\n",
            "Batch : 396|Training Loss: 0.1544976830482483|Training Accuracy : 0.9375\n",
            "Batch : 397|Training Loss: 0.05904035642743111|Training Accuracy : 1.0\n",
            "Batch : 398|Training Loss: 0.1711033582687378|Training Accuracy : 0.96875\n",
            "Batch : 399|Training Loss: 0.15102095901966095|Training Accuracy : 0.90625\n",
            "Batch : 400|Training Loss: 0.04055551066994667|Training Accuracy : 0.96875\n",
            "Batch : 401|Training Loss: 0.18414363265037537|Training Accuracy : 0.9375\n",
            "Batch : 402|Training Loss: 0.13735716044902802|Training Accuracy : 0.9375\n",
            "Batch : 403|Training Loss: 0.16955290734767914|Training Accuracy : 0.9375\n",
            "Batch : 404|Training Loss: 0.0950857624411583|Training Accuracy : 0.96875\n",
            "Batch : 405|Training Loss: 0.2362307608127594|Training Accuracy : 0.84375\n",
            "Batch : 406|Training Loss: 0.08754732459783554|Training Accuracy : 0.96875\n",
            "Batch : 407|Training Loss: 0.17843468487262726|Training Accuracy : 0.90625\n",
            "Batch : 408|Training Loss: 0.1459546834230423|Training Accuracy : 0.875\n",
            "Batch : 409|Training Loss: 0.24348284304141998|Training Accuracy : 0.90625\n",
            "Batch : 410|Training Loss: 0.0872248038649559|Training Accuracy : 0.96875\n",
            "Batch : 411|Training Loss: 0.28070610761642456|Training Accuracy : 0.875\n",
            "Batch : 412|Training Loss: 0.0674619972705841|Training Accuracy : 0.96875\n",
            "Batch : 413|Training Loss: 0.13507957756519318|Training Accuracy : 0.96875\n",
            "Batch : 414|Training Loss: 0.11002418398857117|Training Accuracy : 0.90625\n",
            "Batch : 415|Training Loss: 0.0455513596534729|Training Accuracy : 1.0\n",
            "Batch : 416|Training Loss: 0.10621325671672821|Training Accuracy : 0.96875\n",
            "Batch : 417|Training Loss: 0.17906664311885834|Training Accuracy : 0.9375\n",
            "Batch : 418|Training Loss: 0.07266686111688614|Training Accuracy : 0.96875\n",
            "Batch : 419|Training Loss: 0.06931453943252563|Training Accuracy : 0.96875\n",
            "Batch : 420|Training Loss: 0.12164618819952011|Training Accuracy : 0.96875\n",
            "Batch : 421|Training Loss: 0.23049747943878174|Training Accuracy : 0.90625\n",
            "Batch : 422|Training Loss: 0.352749228477478|Training Accuracy : 0.875\n",
            "Batch : 423|Training Loss: 0.3139388859272003|Training Accuracy : 0.90625\n",
            "Batch : 424|Training Loss: 0.28053057193756104|Training Accuracy : 0.84375\n",
            "Batch : 425|Training Loss: 0.0955263301730156|Training Accuracy : 0.9375\n",
            "Batch : 426|Training Loss: 0.28881630301475525|Training Accuracy : 0.8125\n",
            "Batch : 427|Training Loss: 0.38720470666885376|Training Accuracy : 0.875\n",
            "Batch : 428|Training Loss: 0.47963306307792664|Training Accuracy : 0.90625\n",
            "Batch : 429|Training Loss: 0.12573695182800293|Training Accuracy : 0.96875\n",
            "Batch : 430|Training Loss: 0.15065640211105347|Training Accuracy : 0.90625\n",
            "Batch : 431|Training Loss: 0.32454580068588257|Training Accuracy : 0.875\n",
            "Batch : 432|Training Loss: 0.16778413951396942|Training Accuracy : 0.875\n",
            "Batch : 433|Training Loss: 0.27596932649612427|Training Accuracy : 0.90625\n",
            "Batch : 434|Training Loss: 0.030945910140872|Training Accuracy : 1.0\n",
            "Batch : 435|Training Loss: 0.15727540850639343|Training Accuracy : 0.9375\n",
            "Batch : 436|Training Loss: 0.09146328270435333|Training Accuracy : 0.96875\n",
            "Batch : 437|Training Loss: 0.07517214119434357|Training Accuracy : 0.96875\n",
            "Batch : 438|Training Loss: 0.1897222399711609|Training Accuracy : 0.90625\n",
            "Batch : 439|Training Loss: 0.21251776814460754|Training Accuracy : 0.84375\n",
            "Batch : 440|Training Loss: 0.06492673605680466|Training Accuracy : 1.0\n",
            "Batch : 441|Training Loss: 0.01867670752108097|Training Accuracy : 1.0\n",
            "Batch : 442|Training Loss: 0.32734155654907227|Training Accuracy : 0.90625\n",
            "Batch : 443|Training Loss: 0.26013246178627014|Training Accuracy : 0.90625\n",
            "Batch : 444|Training Loss: 0.0268445685505867|Training Accuracy : 1.0\n",
            "Batch : 445|Training Loss: 0.3925994038581848|Training Accuracy : 0.875\n",
            "Batch : 446|Training Loss: 0.15892376005649567|Training Accuracy : 0.96875\n",
            "Batch : 447|Training Loss: 0.018691781908273697|Training Accuracy : 1.0\n",
            "Batch : 448|Training Loss: 0.1319987177848816|Training Accuracy : 0.96875\n",
            "Batch : 449|Training Loss: 0.1570338010787964|Training Accuracy : 0.9375\n",
            "Batch : 450|Training Loss: 0.12954160571098328|Training Accuracy : 0.9375\n",
            "Batch : 451|Training Loss: 0.2754875421524048|Training Accuracy : 0.90625\n",
            "Batch : 452|Training Loss: 0.31603461503982544|Training Accuracy : 0.875\n",
            "Batch : 453|Training Loss: 0.06752797961235046|Training Accuracy : 1.0\n",
            "Batch : 454|Training Loss: 0.14571937918663025|Training Accuracy : 0.9375\n",
            "Batch : 455|Training Loss: 0.4933440685272217|Training Accuracy : 0.90625\n",
            "Batch : 456|Training Loss: 0.26161202788352966|Training Accuracy : 0.875\n",
            "Batch : 457|Training Loss: 0.09053932875394821|Training Accuracy : 0.96875\n",
            "Batch : 458|Training Loss: 0.020158961415290833|Training Accuracy : 1.0\n",
            "Batch : 459|Training Loss: 0.18666063249111176|Training Accuracy : 0.9375\n",
            "Batch : 460|Training Loss: 0.2840617895126343|Training Accuracy : 0.84375\n",
            "Batch : 461|Training Loss: 0.10155007988214493|Training Accuracy : 0.96875\n",
            "Batch : 462|Training Loss: 0.15768860280513763|Training Accuracy : 0.9375\n",
            "Batch : 463|Training Loss: 0.24197639524936676|Training Accuracy : 0.96875\n",
            "Batch : 464|Training Loss: 0.08434640616178513|Training Accuracy : 0.96875\n",
            "Batch : 465|Training Loss: 0.08816089481115341|Training Accuracy : 1.0\n",
            "Batch : 466|Training Loss: 0.13492150604724884|Training Accuracy : 0.90625\n",
            "Batch : 467|Training Loss: 0.16564811766147614|Training Accuracy : 0.9375\n",
            "Batch : 468|Training Loss: 0.26599982380867004|Training Accuracy : 0.84375\n",
            "Batch : 469|Training Loss: 0.1592768281698227|Training Accuracy : 0.9375\n",
            "Batch : 470|Training Loss: 0.1682722568511963|Training Accuracy : 0.96875\n",
            "Batch : 471|Training Loss: 0.3865904211997986|Training Accuracy : 0.84375\n",
            "Batch : 472|Training Loss: 0.18290561437606812|Training Accuracy : 0.90625\n",
            "Batch : 473|Training Loss: 0.30877459049224854|Training Accuracy : 0.84375\n",
            "Batch : 474|Training Loss: 0.30999186635017395|Training Accuracy : 0.84375\n",
            "Batch : 475|Training Loss: 0.11002327501773834|Training Accuracy : 0.96875\n",
            "Batch : 476|Training Loss: 0.36394819617271423|Training Accuracy : 0.90625\n",
            "Batch : 477|Training Loss: 0.30742743611335754|Training Accuracy : 0.875\n",
            "Batch : 478|Training Loss: 0.07127152383327484|Training Accuracy : 0.96875\n",
            "Batch : 479|Training Loss: 0.15543663501739502|Training Accuracy : 0.96875\n",
            "Batch : 480|Training Loss: 0.3206881880760193|Training Accuracy : 0.875\n",
            "Batch : 481|Training Loss: 0.23141221702098846|Training Accuracy : 0.90625\n",
            "Batch : 482|Training Loss: 0.13743841648101807|Training Accuracy : 0.96875\n",
            "Batch : 483|Training Loss: 0.18106411397457123|Training Accuracy : 0.9375\n",
            "Batch : 484|Training Loss: 0.11201005429029465|Training Accuracy : 0.9375\n",
            "Batch : 485|Training Loss: 0.21784518659114838|Training Accuracy : 0.96875\n",
            "Batch : 486|Training Loss: 0.13716647028923035|Training Accuracy : 0.9375\n",
            "Batch : 487|Training Loss: 0.2335408478975296|Training Accuracy : 0.9375\n",
            "Batch : 488|Training Loss: 0.13691987097263336|Training Accuracy : 0.96875\n",
            "Batch : 489|Training Loss: 0.10994019359350204|Training Accuracy : 0.96875\n",
            "Batch : 490|Training Loss: 0.15234270691871643|Training Accuracy : 0.9375\n",
            "Batch : 491|Training Loss: 0.1793455183506012|Training Accuracy : 0.9375\n",
            "Batch : 492|Training Loss: 0.38376113772392273|Training Accuracy : 0.90625\n",
            "Batch : 493|Training Loss: 0.21905437111854553|Training Accuracy : 0.90625\n",
            "Batch : 494|Training Loss: 0.14418761432170868|Training Accuracy : 0.9375\n",
            "Batch : 495|Training Loss: 0.12902063131332397|Training Accuracy : 0.9375\n",
            "Batch : 496|Training Loss: 0.24251008033752441|Training Accuracy : 0.90625\n",
            "Batch : 497|Training Loss: 0.1224033460021019|Training Accuracy : 0.96875\n",
            "Batch : 498|Training Loss: 0.2933623790740967|Training Accuracy : 0.875\n",
            "Batch : 499|Training Loss: 0.12081202864646912|Training Accuracy : 0.96875\n",
            "Batch : 500|Training Loss: 0.05981014668941498|Training Accuracy : 1.0\n",
            "Batch : 501|Training Loss: 0.14157453179359436|Training Accuracy : 0.9375\n",
            "Batch : 502|Training Loss: 0.18111282587051392|Training Accuracy : 0.96875\n",
            "Batch : 503|Training Loss: 0.1594236195087433|Training Accuracy : 0.90625\n",
            "Batch : 504|Training Loss: 0.11803927272558212|Training Accuracy : 0.96875\n",
            "Batch : 505|Training Loss: 0.1031336784362793|Training Accuracy : 0.96875\n",
            "Batch : 506|Training Loss: 0.09332850575447083|Training Accuracy : 0.96875\n",
            "Batch : 507|Training Loss: 0.11472809314727783|Training Accuracy : 0.96875\n",
            "Batch : 508|Training Loss: 0.24894101917743683|Training Accuracy : 0.9375\n",
            "Batch : 509|Training Loss: 0.14549563825130463|Training Accuracy : 0.9375\n",
            "Batch : 510|Training Loss: 0.15699422359466553|Training Accuracy : 0.90625\n",
            "Batch : 511|Training Loss: 0.22912555932998657|Training Accuracy : 0.90625\n",
            "Batch : 512|Training Loss: 0.243727445602417|Training Accuracy : 0.90625\n",
            "Batch : 513|Training Loss: 0.07103026658296585|Training Accuracy : 0.96875\n",
            "Batch : 514|Training Loss: 0.194986492395401|Training Accuracy : 0.90625\n",
            "Batch : 515|Training Loss: 0.1329929679632187|Training Accuracy : 0.90625\n",
            "Batch : 516|Training Loss: 0.17548152804374695|Training Accuracy : 0.96875\n",
            "Batch : 517|Training Loss: 0.19598418474197388|Training Accuracy : 0.9375\n",
            "Batch : 518|Training Loss: 0.08420461416244507|Training Accuracy : 0.96875\n",
            "Batch : 519|Training Loss: 0.11082284152507782|Training Accuracy : 0.9375\n",
            "Batch : 520|Training Loss: 0.27490031719207764|Training Accuracy : 0.96875\n",
            "Batch : 521|Training Loss: 0.09968042373657227|Training Accuracy : 0.96875\n",
            "Batch : 522|Training Loss: 0.1634470522403717|Training Accuracy : 0.9375\n",
            "Batch : 523|Training Loss: 0.07117883116006851|Training Accuracy : 1.0\n",
            "Batch : 524|Training Loss: 0.23305852711200714|Training Accuracy : 0.90625\n",
            "Batch : 525|Training Loss: 0.17393800616264343|Training Accuracy : 0.90625\n",
            "Batch : 526|Training Loss: 0.1416502594947815|Training Accuracy : 0.9375\n",
            "Batch : 527|Training Loss: 0.0950796902179718|Training Accuracy : 0.96875\n",
            "Batch : 528|Training Loss: 0.10413553565740585|Training Accuracy : 0.96875\n",
            "Batch : 529|Training Loss: 0.15409153699874878|Training Accuracy : 0.90625\n",
            "Batch : 530|Training Loss: 0.2152138650417328|Training Accuracy : 0.9375\n",
            "Batch : 531|Training Loss: 0.10777503997087479|Training Accuracy : 0.96875\n",
            "Batch : 532|Training Loss: 0.1614132821559906|Training Accuracy : 0.9375\n",
            "Batch : 533|Training Loss: 0.2269214689731598|Training Accuracy : 0.90625\n",
            "Batch : 534|Training Loss: 0.0272737555205822|Training Accuracy : 1.0\n",
            "Batch : 535|Training Loss: 0.22062459588050842|Training Accuracy : 0.875\n",
            "Batch : 536|Training Loss: 0.056863296777009964|Training Accuracy : 0.96875\n",
            "Batch : 537|Training Loss: 0.13600574433803558|Training Accuracy : 0.9375\n",
            "Batch : 538|Training Loss: 0.4241856634616852|Training Accuracy : 0.875\n",
            "Batch : 539|Training Loss: 0.15433067083358765|Training Accuracy : 0.9375\n",
            "Batch : 540|Training Loss: 0.13823604583740234|Training Accuracy : 0.96875\n",
            "Batch : 541|Training Loss: 0.3177298307418823|Training Accuracy : 0.875\n",
            "Batch : 542|Training Loss: 0.15159879624843597|Training Accuracy : 0.90625\n",
            "Batch : 543|Training Loss: 0.278219610452652|Training Accuracy : 0.8125\n",
            "Batch : 544|Training Loss: 0.28932178020477295|Training Accuracy : 0.90625\n",
            "Batch : 545|Training Loss: 0.18346962332725525|Training Accuracy : 0.875\n",
            "Batch : 546|Training Loss: 0.22967718541622162|Training Accuracy : 0.90625\n",
            "Batch : 547|Training Loss: 0.1301426738500595|Training Accuracy : 0.96875\n",
            "Batch : 548|Training Loss: 0.1960269808769226|Training Accuracy : 0.9375\n",
            "Batch : 549|Training Loss: 0.10525360703468323|Training Accuracy : 0.96875\n",
            "Batch : 550|Training Loss: 0.160269632935524|Training Accuracy : 0.90625\n",
            "Batch : 551|Training Loss: 0.08909537643194199|Training Accuracy : 0.96875\n",
            "Batch : 552|Training Loss: 0.23211896419525146|Training Accuracy : 0.9375\n",
            "Batch : 553|Training Loss: 0.12612073123455048|Training Accuracy : 0.9375\n",
            "Batch : 554|Training Loss: 0.060331713408231735|Training Accuracy : 1.0\n",
            "Batch : 555|Training Loss: 0.0826655775308609|Training Accuracy : 1.0\n",
            "Batch : 556|Training Loss: 0.0746566578745842|Training Accuracy : 0.96875\n",
            "Batch : 557|Training Loss: 0.08758857846260071|Training Accuracy : 1.0\n",
            "Batch : 558|Training Loss: 0.11731284111738205|Training Accuracy : 0.96875\n",
            "Batch : 559|Training Loss: 0.2051558494567871|Training Accuracy : 0.90625\n",
            "Batch : 560|Training Loss: 0.13292399048805237|Training Accuracy : 0.9375\n",
            "Batch : 561|Training Loss: 0.07149332761764526|Training Accuracy : 1.0\n",
            "Batch : 562|Training Loss: 0.1223575621843338|Training Accuracy : 0.96875\n",
            "Batch : 563|Training Loss: 0.23200492560863495|Training Accuracy : 0.90625\n",
            "Batch : 564|Training Loss: 0.1955958902835846|Training Accuracy : 0.90625\n",
            "Batch : 565|Training Loss: 0.14644253253936768|Training Accuracy : 0.90625\n",
            "Batch : 566|Training Loss: 0.0709177777171135|Training Accuracy : 0.96875\n",
            "Batch : 567|Training Loss: 0.19482168555259705|Training Accuracy : 0.90625\n",
            "Batch : 568|Training Loss: 0.1497555524110794|Training Accuracy : 0.90625\n",
            "Batch : 569|Training Loss: 0.1946931928396225|Training Accuracy : 0.90625\n",
            "Batch : 570|Training Loss: 0.5896124839782715|Training Accuracy : 0.8125\n",
            "Batch : 571|Training Loss: 0.3616873621940613|Training Accuracy : 0.9375\n",
            "Batch : 572|Training Loss: 0.21978409588336945|Training Accuracy : 0.90625\n",
            "Batch : 573|Training Loss: 0.07812882959842682|Training Accuracy : 0.96875\n",
            "Batch : 574|Training Loss: 0.3040614724159241|Training Accuracy : 0.84375\n",
            "Batch : 575|Training Loss: 0.126743346452713|Training Accuracy : 0.90625\n",
            "Batch : 576|Training Loss: 0.09426965564489365|Training Accuracy : 0.96875\n",
            "Batch : 577|Training Loss: 0.14762412011623383|Training Accuracy : 0.9375\n",
            "Batch : 578|Training Loss: 0.2269551008939743|Training Accuracy : 0.875\n",
            "Batch : 579|Training Loss: 0.1237141415476799|Training Accuracy : 0.96875\n",
            "Batch : 580|Training Loss: 0.07901321351528168|Training Accuracy : 0.96875\n",
            "Batch : 581|Training Loss: 0.08326300978660583|Training Accuracy : 0.96875\n",
            "Batch : 582|Training Loss: 0.1746840476989746|Training Accuracy : 0.90625\n",
            "Batch : 583|Training Loss: 0.21769964694976807|Training Accuracy : 0.9375\n",
            "Batch : 584|Training Loss: 0.3658723831176758|Training Accuracy : 0.875\n",
            "Batch : 585|Training Loss: 0.21122288703918457|Training Accuracy : 0.9375\n",
            "Batch : 586|Training Loss: 0.1755703240633011|Training Accuracy : 0.9375\n",
            "Batch : 587|Training Loss: 0.14137692749500275|Training Accuracy : 0.9375\n",
            "Batch : 588|Training Loss: 0.2820335924625397|Training Accuracy : 0.9375\n",
            "Batch : 589|Training Loss: 0.04275432974100113|Training Accuracy : 1.0\n",
            "Batch : 590|Training Loss: 0.17136284708976746|Training Accuracy : 0.90625\n",
            "Batch : 591|Training Loss: 0.08025288581848145|Training Accuracy : 0.96875\n",
            "Batch : 592|Training Loss: 0.03709898889064789|Training Accuracy : 1.0\n",
            "Batch : 593|Training Loss: 0.2915186285972595|Training Accuracy : 0.9375\n",
            "Batch : 594|Training Loss: 0.09449736028909683|Training Accuracy : 0.96875\n",
            "Batch : 595|Training Loss: 0.21125991642475128|Training Accuracy : 0.90625\n",
            "Batch : 596|Training Loss: 0.47736236453056335|Training Accuracy : 0.8125\n",
            "Batch : 597|Training Loss: 0.09476426243782043|Training Accuracy : 1.0\n",
            "Batch : 598|Training Loss: 0.08137039095163345|Training Accuracy : 0.96875\n",
            "Batch : 599|Training Loss: 0.32463338971138|Training Accuracy : 0.90625\n",
            "Batch : 600|Training Loss: 0.1673155277967453|Training Accuracy : 0.9375\n",
            "Batch : 601|Training Loss: 0.4110831022262573|Training Accuracy : 0.875\n",
            "Batch : 602|Training Loss: 0.14251422882080078|Training Accuracy : 0.9375\n",
            "Batch : 603|Training Loss: 0.19587713479995728|Training Accuracy : 0.9375\n",
            "Batch : 604|Training Loss: 0.11283745616674423|Training Accuracy : 0.96875\n",
            "Batch : 605|Training Loss: 0.043114956468343735|Training Accuracy : 1.0\n",
            "Batch : 606|Training Loss: 0.20959538221359253|Training Accuracy : 0.9375\n",
            "Batch : 607|Training Loss: 0.15122325718402863|Training Accuracy : 0.96875\n",
            "Batch : 608|Training Loss: 0.03721068426966667|Training Accuracy : 1.0\n",
            "Batch : 609|Training Loss: 0.197695791721344|Training Accuracy : 0.90625\n",
            "Batch : 610|Training Loss: 0.15461565554141998|Training Accuracy : 0.90625\n",
            "Batch : 611|Training Loss: 0.10012030601501465|Training Accuracy : 0.96875\n",
            "Batch : 612|Training Loss: 0.13394862413406372|Training Accuracy : 0.9375\n",
            "Batch : 613|Training Loss: 0.18512924015522003|Training Accuracy : 0.9375\n",
            "Batch : 614|Training Loss: 0.25945132970809937|Training Accuracy : 0.96875\n",
            "Batch : 615|Training Loss: 0.20163868367671967|Training Accuracy : 0.875\n",
            "Batch : 616|Training Loss: 0.13068455457687378|Training Accuracy : 0.96875\n",
            "Batch : 617|Training Loss: 0.24909377098083496|Training Accuracy : 0.9375\n",
            "Batch : 618|Training Loss: 0.20173312723636627|Training Accuracy : 0.9375\n",
            "Batch : 619|Training Loss: 0.016364287585020065|Training Accuracy : 1.0\n",
            "Batch : 620|Training Loss: 0.09070590883493423|Training Accuracy : 0.96875\n",
            "Batch : 621|Training Loss: 0.048733700066804886|Training Accuracy : 0.96875\n",
            "Batch : 622|Training Loss: 0.151083841919899|Training Accuracy : 0.96875\n",
            "Batch : 623|Training Loss: 0.2721732258796692|Training Accuracy : 0.9375\n",
            "Batch : 624|Training Loss: 0.4210796654224396|Training Accuracy : 0.84375\n",
            "Batch : 625|Training Loss: 0.2335750311613083|Training Accuracy : 0.875\n",
            "Batch : 626|Training Loss: 0.2335744947195053|Training Accuracy : 0.875\n",
            "Batch : 627|Training Loss: 0.10266377776861191|Training Accuracy : 0.96875\n",
            "Batch : 628|Training Loss: 0.24293121695518494|Training Accuracy : 0.90625\n",
            "Batch : 629|Training Loss: 0.26107078790664673|Training Accuracy : 0.90625\n",
            "Batch : 630|Training Loss: 0.13837704062461853|Training Accuracy : 0.96875\n",
            "Batch : 631|Training Loss: 0.27785196900367737|Training Accuracy : 0.90625\n",
            "Batch : 632|Training Loss: 0.14364665746688843|Training Accuracy : 0.9375\n",
            "Batch : 633|Training Loss: 0.2325402945280075|Training Accuracy : 0.875\n",
            "Batch : 634|Training Loss: 0.25972023606300354|Training Accuracy : 0.96875\n",
            "Batch : 635|Training Loss: 0.12219883501529694|Training Accuracy : 0.96875\n",
            "Batch : 636|Training Loss: 0.07926500588655472|Training Accuracy : 0.96875\n",
            "Batch : 637|Training Loss: 0.2908819019794464|Training Accuracy : 0.9375\n",
            "Batch : 638|Training Loss: 0.1525495946407318|Training Accuracy : 0.9375\n",
            "Batch : 639|Training Loss: 0.4819606840610504|Training Accuracy : 0.78125\n",
            "Batch : 640|Training Loss: 0.2988626956939697|Training Accuracy : 0.84375\n",
            "Batch : 641|Training Loss: 0.21605363488197327|Training Accuracy : 0.90625\n",
            "Batch : 642|Training Loss: 0.042345523834228516|Training Accuracy : 1.0\n",
            "Batch : 643|Training Loss: 0.05231434479355812|Training Accuracy : 1.0\n",
            "Batch : 644|Training Loss: 0.13404859602451324|Training Accuracy : 0.96875\n",
            "Batch : 645|Training Loss: 0.0511406771838665|Training Accuracy : 1.0\n",
            "Batch : 646|Training Loss: 0.15142175555229187|Training Accuracy : 0.9375\n",
            "Batch : 647|Training Loss: 0.13495445251464844|Training Accuracy : 0.9375\n",
            "Batch : 648|Training Loss: 0.05301618203520775|Training Accuracy : 1.0\n",
            "Batch : 649|Training Loss: 0.06017503887414932|Training Accuracy : 0.96875\n",
            "Batch : 650|Training Loss: 0.4493154287338257|Training Accuracy : 0.75\n",
            "Batch : 651|Training Loss: 0.22102822363376617|Training Accuracy : 0.90625\n",
            "Batch : 652|Training Loss: 0.04478093981742859|Training Accuracy : 1.0\n",
            "Batch : 653|Training Loss: 0.06686368584632874|Training Accuracy : 0.96875\n",
            "Batch : 654|Training Loss: 0.1267762929201126|Training Accuracy : 0.9375\n",
            "Batch : 655|Training Loss: 0.08107800036668777|Training Accuracy : 0.96875\n",
            "Batch : 656|Training Loss: 0.10681425034999847|Training Accuracy : 0.96875\n",
            "Batch : 657|Training Loss: 0.22009135782718658|Training Accuracy : 0.90625\n",
            "Batch : 658|Training Loss: 0.23323211073875427|Training Accuracy : 0.90625\n",
            "Batch : 659|Training Loss: 0.30960214138031006|Training Accuracy : 0.90625\n",
            "Batch : 660|Training Loss: 0.41511034965515137|Training Accuracy : 0.8125\n",
            "Batch : 661|Training Loss: 0.05717607960104942|Training Accuracy : 0.96875\n",
            "Batch : 662|Training Loss: 0.3280643820762634|Training Accuracy : 0.875\n",
            "Batch : 663|Training Loss: 0.2076529860496521|Training Accuracy : 0.875\n",
            "Batch : 664|Training Loss: 0.27280107140541077|Training Accuracy : 0.9375\n",
            "Batch : 665|Training Loss: 0.17493560910224915|Training Accuracy : 0.875\n",
            "Batch : 666|Training Loss: 0.1340889036655426|Training Accuracy : 0.90625\n",
            "Batch : 667|Training Loss: 0.08805421739816666|Training Accuracy : 0.96875\n",
            "Batch : 668|Training Loss: 0.02674165368080139|Training Accuracy : 1.0\n",
            "Batch : 669|Training Loss: 0.15849803388118744|Training Accuracy : 0.9375\n",
            "Batch : 670|Training Loss: 0.3036518692970276|Training Accuracy : 0.875\n",
            "Batch : 671|Training Loss: 0.3634801208972931|Training Accuracy : 0.875\n",
            "Batch : 672|Training Loss: 0.14271768927574158|Training Accuracy : 0.9375\n",
            "Batch : 673|Training Loss: 0.17625294625759125|Training Accuracy : 0.90625\n",
            "Batch : 674|Training Loss: 0.20226922631263733|Training Accuracy : 0.90625\n",
            "Batch : 675|Training Loss: 0.42378056049346924|Training Accuracy : 0.875\n",
            "Batch : 676|Training Loss: 0.0884605348110199|Training Accuracy : 0.96875\n",
            "Batch : 677|Training Loss: 0.292529433965683|Training Accuracy : 0.84375\n",
            "Batch : 678|Training Loss: 0.14092953503131866|Training Accuracy : 0.90625\n",
            "Batch : 679|Training Loss: 0.31949111819267273|Training Accuracy : 0.90625\n",
            "Batch : 680|Training Loss: 0.0049831438809633255|Training Accuracy : 1.0\n",
            "Batch : 681|Training Loss: 0.17366786301136017|Training Accuracy : 0.90625\n",
            "Batch : 682|Training Loss: 0.11716219782829285|Training Accuracy : 0.9375\n",
            "Batch : 683|Training Loss: 0.20201654732227325|Training Accuracy : 0.875\n",
            "Batch : 684|Training Loss: 0.06223640218377113|Training Accuracy : 0.96875\n",
            "Batch : 685|Training Loss: 0.4590757191181183|Training Accuracy : 0.84375\n",
            "Batch : 686|Training Loss: 0.23643042147159576|Training Accuracy : 0.90625\n",
            "Batch : 687|Training Loss: 0.3174918293952942|Training Accuracy : 0.96875\n",
            "Batch : 688|Training Loss: 0.24190010130405426|Training Accuracy : 0.875\n",
            "Batch : 689|Training Loss: 0.11161207407712936|Training Accuracy : 0.96875\n",
            "Batch : 690|Training Loss: 0.1448580026626587|Training Accuracy : 0.96875\n",
            "Batch : 691|Training Loss: 0.053032971918582916|Training Accuracy : 1.0\n",
            "Batch : 692|Training Loss: 0.2016615867614746|Training Accuracy : 0.96875\n",
            "Batch : 693|Training Loss: 0.41852304339408875|Training Accuracy : 0.875\n",
            "Batch : 694|Training Loss: 0.2987811863422394|Training Accuracy : 0.875\n",
            "Batch : 695|Training Loss: 0.1479959636926651|Training Accuracy : 0.90625\n",
            "Batch : 696|Training Loss: 0.09907394647598267|Training Accuracy : 0.96875\n",
            "Batch : 697|Training Loss: 0.1800277829170227|Training Accuracy : 0.9375\n",
            "Batch : 698|Training Loss: 0.121608205139637|Training Accuracy : 0.9375\n",
            "Batch : 699|Training Loss: 0.16579599678516388|Training Accuracy : 0.9375\n",
            "Batch : 700|Training Loss: 0.0694013237953186|Training Accuracy : 1.0\n",
            "Batch : 701|Training Loss: 0.06726037710905075|Training Accuracy : 1.0\n",
            "Batch : 702|Training Loss: 0.12705369293689728|Training Accuracy : 0.9375\n",
            "Batch : 703|Training Loss: 0.19503527879714966|Training Accuracy : 0.96875\n",
            "Batch : 704|Training Loss: 0.10574520379304886|Training Accuracy : 0.96875\n",
            "Batch : 705|Training Loss: 0.1850574016571045|Training Accuracy : 0.90625\n",
            "Batch : 706|Training Loss: 0.3460477888584137|Training Accuracy : 0.875\n",
            "Batch : 707|Training Loss: 0.2619119882583618|Training Accuracy : 0.90625\n",
            "Batch : 708|Training Loss: 0.17108087241649628|Training Accuracy : 0.90625\n",
            "Batch : 709|Training Loss: 0.19225464761257172|Training Accuracy : 0.9375\n",
            "Batch : 710|Training Loss: 0.12072844803333282|Training Accuracy : 0.9375\n",
            "Batch : 711|Training Loss: 0.046938348561525345|Training Accuracy : 1.0\n",
            "Batch : 712|Training Loss: 0.43040943145751953|Training Accuracy : 0.875\n",
            "Batch : 713|Training Loss: 0.06667589396238327|Training Accuracy : 1.0\n",
            "Batch : 714|Training Loss: 0.35385316610336304|Training Accuracy : 0.875\n",
            "Batch : 715|Training Loss: 0.2717632055282593|Training Accuracy : 0.90625\n",
            "Batch : 716|Training Loss: 0.25265294313430786|Training Accuracy : 0.90625\n",
            "Batch : 717|Training Loss: 0.35366928577423096|Training Accuracy : 0.84375\n",
            "Batch : 718|Training Loss: 0.1060873419046402|Training Accuracy : 1.0\n",
            "Batch : 719|Training Loss: 0.14765116572380066|Training Accuracy : 0.96875\n",
            "Batch : 720|Training Loss: 0.1088617593050003|Training Accuracy : 0.9375\n",
            "Batch : 721|Training Loss: 0.09761839359998703|Training Accuracy : 0.9375\n",
            "Batch : 722|Training Loss: 0.10969613492488861|Training Accuracy : 0.96875\n",
            "Batch : 723|Training Loss: 0.17029458284378052|Training Accuracy : 0.96875\n",
            "Batch : 724|Training Loss: 0.26935073733329773|Training Accuracy : 0.9375\n",
            "Batch : 725|Training Loss: 0.12319741398096085|Training Accuracy : 0.9375\n",
            "Batch : 726|Training Loss: 0.0876186266541481|Training Accuracy : 0.9375\n",
            "Batch : 727|Training Loss: 0.27554818987846375|Training Accuracy : 0.875\n",
            "Batch : 728|Training Loss: 0.24742171168327332|Training Accuracy : 0.90625\n",
            "Batch : 729|Training Loss: 0.04752793535590172|Training Accuracy : 1.0\n",
            "Batch : 730|Training Loss: 0.11134208738803864|Training Accuracy : 1.0\n",
            "Batch : 731|Training Loss: 0.15378643572330475|Training Accuracy : 0.9375\n",
            "Batch : 732|Training Loss: 0.1504022777080536|Training Accuracy : 0.9375\n",
            "Batch : 733|Training Loss: 0.07506820559501648|Training Accuracy : 1.0\n",
            "Batch : 734|Training Loss: 0.0926084816455841|Training Accuracy : 0.96875\n",
            "Batch : 735|Training Loss: 0.28526896238327026|Training Accuracy : 0.90625\n",
            "Batch : 736|Training Loss: 0.23046422004699707|Training Accuracy : 0.90625\n",
            "Batch : 737|Training Loss: 0.2723788619041443|Training Accuracy : 0.90625\n",
            "Batch : 738|Training Loss: 0.18893404304981232|Training Accuracy : 0.9375\n",
            "Batch : 739|Training Loss: 0.15511828660964966|Training Accuracy : 0.9375\n",
            "Batch : 740|Training Loss: 0.18858692049980164|Training Accuracy : 0.9375\n",
            "Batch : 741|Training Loss: 0.03838121145963669|Training Accuracy : 0.96875\n",
            "Batch : 742|Training Loss: 0.24202144145965576|Training Accuracy : 0.96875\n",
            "Batch : 743|Training Loss: 0.03830393776297569|Training Accuracy : 0.96875\n",
            "Batch : 744|Training Loss: 0.15166790783405304|Training Accuracy : 0.9375\n",
            "Batch : 745|Training Loss: 0.1684401035308838|Training Accuracy : 0.96875\n",
            "Batch : 746|Training Loss: 0.15451674163341522|Training Accuracy : 0.90625\n",
            "Batch : 747|Training Loss: 0.08052454888820648|Training Accuracy : 0.96875\n",
            "Batch : 748|Training Loss: 0.17475511133670807|Training Accuracy : 0.9375\n",
            "Batch : 749|Training Loss: 0.18251590430736542|Training Accuracy : 0.90625\n",
            "Batch : 750|Training Loss: 0.06214917451143265|Training Accuracy : 1.0\n",
            "Batch : 751|Training Loss: 0.029952477663755417|Training Accuracy : 1.0\n",
            "Batch : 752|Training Loss: 0.034433599561452866|Training Accuracy : 1.0\n",
            "Batch : 753|Training Loss: 0.09616811573505402|Training Accuracy : 0.96875\n",
            "Batch : 754|Training Loss: 0.1585802435874939|Training Accuracy : 0.90625\n",
            "Batch : 755|Training Loss: 0.1056014746427536|Training Accuracy : 0.9375\n",
            "Batch : 756|Training Loss: 0.0959952250123024|Training Accuracy : 0.96875\n",
            "Batch : 757|Training Loss: 0.07754095643758774|Training Accuracy : 1.0\n",
            "Batch : 758|Training Loss: 0.20174585282802582|Training Accuracy : 0.875\n",
            "Batch : 759|Training Loss: 0.1943073719739914|Training Accuracy : 0.9375\n",
            "Batch : 760|Training Loss: 0.08166053891181946|Training Accuracy : 0.96875\n",
            "Batch : 761|Training Loss: 0.12956422567367554|Training Accuracy : 0.90625\n",
            "Batch : 762|Training Loss: 0.16206185519695282|Training Accuracy : 0.96875\n",
            "Batch : 763|Training Loss: 0.060512643307447433|Training Accuracy : 1.0\n",
            "Batch : 764|Training Loss: 0.12423194944858551|Training Accuracy : 0.9375\n",
            "Batch : 765|Training Loss: 0.14056354761123657|Training Accuracy : 0.96875\n",
            "Batch : 766|Training Loss: 0.11459611356258392|Training Accuracy : 0.9375\n",
            "Batch : 767|Training Loss: 0.016668405383825302|Training Accuracy : 1.0\n",
            "Batch : 768|Training Loss: 0.21001450717449188|Training Accuracy : 0.90625\n",
            "Batch : 769|Training Loss: 0.34526327252388|Training Accuracy : 0.9375\n",
            "Batch : 770|Training Loss: 0.027828173711895943|Training Accuracy : 1.0\n",
            "Batch : 771|Training Loss: 0.20299407839775085|Training Accuracy : 0.96875\n",
            "Batch : 772|Training Loss: 0.14175236225128174|Training Accuracy : 0.96875\n",
            "Batch : 773|Training Loss: 0.1179635226726532|Training Accuracy : 0.9375\n",
            "Batch : 774|Training Loss: 0.11425641179084778|Training Accuracy : 0.9375\n",
            "Batch : 775|Training Loss: 0.05555377155542374|Training Accuracy : 1.0\n",
            "Batch : 776|Training Loss: 0.19565092027187347|Training Accuracy : 0.96875\n",
            "Batch : 777|Training Loss: 0.12948963046073914|Training Accuracy : 0.9375\n",
            "Batch : 778|Training Loss: 0.0764305517077446|Training Accuracy : 0.96875\n",
            "Batch : 779|Training Loss: 0.04995530843734741|Training Accuracy : 0.96875\n",
            "Batch : 780|Training Loss: 0.08576738834381104|Training Accuracy : 0.96875\n",
            "Batch : 781|Training Loss: 0.4208865761756897|Training Accuracy : 0.875\n",
            "Batch : 782|Training Loss: 0.12309186905622482|Training Accuracy : 0.90625\n",
            "Batch : 783|Training Loss: 0.08687511086463928|Training Accuracy : 0.96875\n",
            "Batch : 784|Training Loss: 0.2742486596107483|Training Accuracy : 0.90625\n",
            "Batch : 785|Training Loss: 0.1846684217453003|Training Accuracy : 0.90625\n",
            "Batch : 786|Training Loss: 0.10889628529548645|Training Accuracy : 0.9375\n",
            "Batch : 787|Training Loss: 0.3765071630477905|Training Accuracy : 0.8125\n",
            "Batch : 788|Training Loss: 0.0648316890001297|Training Accuracy : 0.96875\n",
            "Batch : 789|Training Loss: 0.01928171142935753|Training Accuracy : 1.0\n",
            "Batch : 790|Training Loss: 0.4929671883583069|Training Accuracy : 0.8125\n",
            "Batch : 791|Training Loss: 0.1294693499803543|Training Accuracy : 0.9375\n",
            "Batch : 792|Training Loss: 0.14724832773208618|Training Accuracy : 0.9375\n",
            "Batch : 793|Training Loss: 0.1027567982673645|Training Accuracy : 0.96875\n",
            "Batch : 794|Training Loss: 0.15083754062652588|Training Accuracy : 0.96875\n",
            "Batch : 795|Training Loss: 0.11703747510910034|Training Accuracy : 0.96875\n",
            "Batch : 796|Training Loss: 0.042613185942173004|Training Accuracy : 1.0\n",
            "Batch : 797|Training Loss: 0.15066571533679962|Training Accuracy : 0.9375\n",
            "Batch : 798|Training Loss: 0.2563600540161133|Training Accuracy : 0.90625\n",
            "Batch : 799|Training Loss: 0.1492312103509903|Training Accuracy : 0.96875\n",
            "Batch : 800|Training Loss: 0.1478879153728485|Training Accuracy : 0.9375\n",
            "Batch : 801|Training Loss: 0.1764640212059021|Training Accuracy : 0.90625\n",
            "Batch : 802|Training Loss: 0.22285360097885132|Training Accuracy : 0.875\n",
            "Batch : 803|Training Loss: 0.1695113331079483|Training Accuracy : 0.90625\n",
            "Batch : 804|Training Loss: 0.15403488278388977|Training Accuracy : 0.96875\n",
            "Batch : 805|Training Loss: 0.09178073704242706|Training Accuracy : 0.96875\n",
            "Batch : 806|Training Loss: 0.17168593406677246|Training Accuracy : 0.9375\n",
            "Batch : 807|Training Loss: 0.13078171014785767|Training Accuracy : 0.90625\n",
            "Batch : 808|Training Loss: 0.20291666686534882|Training Accuracy : 0.90625\n",
            "Batch : 809|Training Loss: 0.11244720965623856|Training Accuracy : 0.9375\n",
            "Batch : 810|Training Loss: 0.1896633505821228|Training Accuracy : 0.96875\n",
            "Batch : 811|Training Loss: 0.32401493191719055|Training Accuracy : 0.90625\n",
            "Batch : 812|Training Loss: 0.11416573822498322|Training Accuracy : 0.96875\n",
            "Batch : 813|Training Loss: 0.05011963099241257|Training Accuracy : 1.0\n",
            "Batch : 814|Training Loss: 0.32434436678886414|Training Accuracy : 0.90625\n",
            "Batch : 815|Training Loss: 0.2362186312675476|Training Accuracy : 0.90625\n",
            "Batch : 816|Training Loss: 0.1315334588289261|Training Accuracy : 0.9375\n",
            "Batch : 817|Training Loss: 0.19963619112968445|Training Accuracy : 0.90625\n",
            "Batch : 818|Training Loss: 0.21253375709056854|Training Accuracy : 0.90625\n",
            "Batch : 819|Training Loss: 0.25849422812461853|Training Accuracy : 0.84375\n",
            "Batch : 820|Training Loss: 0.09015518426895142|Training Accuracy : 0.96875\n",
            "Batch : 821|Training Loss: 0.07057438790798187|Training Accuracy : 1.0\n",
            "Batch : 822|Training Loss: 0.112849161028862|Training Accuracy : 0.96875\n",
            "Batch : 823|Training Loss: 0.03537185117602348|Training Accuracy : 1.0\n",
            "Batch : 824|Training Loss: 0.13367176055908203|Training Accuracy : 0.9375\n",
            "Batch : 825|Training Loss: 0.014458144083619118|Training Accuracy : 1.0\n",
            "Batch : 826|Training Loss: 0.1630137413740158|Training Accuracy : 0.9375\n",
            "Batch : 827|Training Loss: 0.20000456273555756|Training Accuracy : 0.9375\n",
            "Batch : 828|Training Loss: 0.04593375697731972|Training Accuracy : 0.96875\n",
            "Batch : 829|Training Loss: 0.1281350702047348|Training Accuracy : 0.9375\n",
            "Batch : 830|Training Loss: 0.31227508187294006|Training Accuracy : 0.9375\n",
            "Batch : 831|Training Loss: 0.37784841656684875|Training Accuracy : 0.84375\n",
            "Batch : 832|Training Loss: 0.1399730145931244|Training Accuracy : 0.9375\n",
            "Batch : 833|Training Loss: 0.2993893027305603|Training Accuracy : 0.90625\n",
            "Batch : 834|Training Loss: 0.17213371396064758|Training Accuracy : 0.875\n",
            "Batch : 835|Training Loss: 0.38348913192749023|Training Accuracy : 0.90625\n",
            "Batch : 836|Training Loss: 0.13977213203907013|Training Accuracy : 0.96875\n",
            "Batch : 837|Training Loss: 0.42702552676200867|Training Accuracy : 0.90625\n",
            "Batch : 838|Training Loss: 0.22069329023361206|Training Accuracy : 0.9375\n",
            "Batch : 839|Training Loss: 0.09162735939025879|Training Accuracy : 1.0\n",
            "Batch : 840|Training Loss: 0.13823197782039642|Training Accuracy : 0.9375\n",
            "Batch : 841|Training Loss: 0.12848061323165894|Training Accuracy : 0.96875\n",
            "Batch : 842|Training Loss: 0.18012571334838867|Training Accuracy : 0.9375\n",
            "Batch : 843|Training Loss: 0.14825832843780518|Training Accuracy : 0.90625\n",
            "Batch : 844|Training Loss: 0.12802989780902863|Training Accuracy : 0.9375\n",
            "Batch : 845|Training Loss: 0.33105456829071045|Training Accuracy : 0.875\n",
            "Batch : 846|Training Loss: 0.08656662702560425|Training Accuracy : 1.0\n",
            "Batch : 847|Training Loss: 0.06249105557799339|Training Accuracy : 1.0\n",
            "Batch : 848|Training Loss: 0.16614843904972076|Training Accuracy : 0.9375\n",
            "Batch : 849|Training Loss: 0.07778893411159515|Training Accuracy : 0.96875\n",
            "Batch : 850|Training Loss: 0.2435503900051117|Training Accuracy : 0.90625\n",
            "Batch : 851|Training Loss: 0.1320042610168457|Training Accuracy : 0.9375\n",
            "Batch : 852|Training Loss: 0.26549866795539856|Training Accuracy : 0.9375\n",
            "Batch : 853|Training Loss: 0.25716814398765564|Training Accuracy : 0.84375\n",
            "Batch : 854|Training Loss: 0.1796516478061676|Training Accuracy : 0.875\n",
            "Batch : 855|Training Loss: 0.10993564128875732|Training Accuracy : 0.9375\n",
            "Batch : 856|Training Loss: 0.20469622313976288|Training Accuracy : 0.90625\n",
            "Batch : 857|Training Loss: 0.2869469225406647|Training Accuracy : 0.875\n",
            "Batch : 858|Training Loss: 0.20583127439022064|Training Accuracy : 0.9375\n",
            "Batch : 859|Training Loss: 0.13343924283981323|Training Accuracy : 0.96875\n",
            "Batch : 860|Training Loss: 0.08448976278305054|Training Accuracy : 0.96875\n",
            "Batch : 861|Training Loss: 0.22059239447116852|Training Accuracy : 0.875\n",
            "Batch : 862|Training Loss: 0.1699165552854538|Training Accuracy : 0.9375\n",
            "Batch : 863|Training Loss: 0.47013819217681885|Training Accuracy : 0.875\n",
            "Batch : 864|Training Loss: 0.13277137279510498|Training Accuracy : 0.96875\n",
            "Batch : 865|Training Loss: 0.10739059746265411|Training Accuracy : 0.9375\n",
            "Batch : 866|Training Loss: 0.03898368030786514|Training Accuracy : 1.0\n",
            "Batch : 867|Training Loss: 0.07025329768657684|Training Accuracy : 0.96875\n",
            "Batch : 868|Training Loss: 0.09154005348682404|Training Accuracy : 0.96875\n",
            "Batch : 869|Training Loss: 0.1507377177476883|Training Accuracy : 0.90625\n",
            "Batch : 870|Training Loss: 0.15103623270988464|Training Accuracy : 0.90625\n",
            "Batch : 871|Training Loss: 0.24293546378612518|Training Accuracy : 0.9375\n",
            "Batch : 872|Training Loss: 0.25691014528274536|Training Accuracy : 0.90625\n",
            "Batch : 873|Training Loss: 0.2961147427558899|Training Accuracy : 0.84375\n",
            "Batch : 874|Training Loss: 0.09086732566356659|Training Accuracy : 0.96875\n",
            "Batch : 875|Training Loss: 0.27456197142601013|Training Accuracy : 0.875\n",
            "Batch : 876|Training Loss: 0.1986631453037262|Training Accuracy : 0.9375\n",
            "Batch : 877|Training Loss: 0.06767067313194275|Training Accuracy : 0.96875\n",
            "Batch : 878|Training Loss: 0.07641692459583282|Training Accuracy : 0.96875\n",
            "Batch : 879|Training Loss: 0.11416596174240112|Training Accuracy : 0.9375\n",
            "Batch : 880|Training Loss: 0.3651813566684723|Training Accuracy : 0.84375\n",
            "Batch : 881|Training Loss: 0.14111235737800598|Training Accuracy : 0.90625\n",
            "Batch : 882|Training Loss: 0.19368182122707367|Training Accuracy : 0.90625\n",
            "Batch : 883|Training Loss: 0.07507754117250443|Training Accuracy : 0.96875\n",
            "Batch : 884|Training Loss: 0.13857212662696838|Training Accuracy : 0.9375\n",
            "Batch : 885|Training Loss: 0.1763598471879959|Training Accuracy : 0.96875\n",
            "Batch : 886|Training Loss: 0.14840103685855865|Training Accuracy : 0.96875\n",
            "Batch : 887|Training Loss: 0.17333662509918213|Training Accuracy : 0.90625\n",
            "Batch : 888|Training Loss: 0.15797027945518494|Training Accuracy : 0.90625\n",
            "Batch : 889|Training Loss: 0.33292990922927856|Training Accuracy : 0.8125\n",
            "Batch : 890|Training Loss: 0.2572283446788788|Training Accuracy : 0.875\n",
            "Batch : 891|Training Loss: 0.08703108131885529|Training Accuracy : 0.96875\n",
            "Batch : 892|Training Loss: 0.04554758220911026|Training Accuracy : 1.0\n",
            "Batch : 893|Training Loss: 0.30423158407211304|Training Accuracy : 0.84375\n",
            "Batch : 894|Training Loss: 0.08200568705797195|Training Accuracy : 1.0\n",
            "Batch : 895|Training Loss: 0.1519809067249298|Training Accuracy : 0.90625\n",
            "Batch : 896|Training Loss: 0.1268240511417389|Training Accuracy : 0.96875\n",
            "Batch : 897|Training Loss: 0.20321206748485565|Training Accuracy : 0.96875\n",
            "Batch : 898|Training Loss: 0.1037394180893898|Training Accuracy : 0.96875\n",
            "Batch : 899|Training Loss: 0.3779945969581604|Training Accuracy : 0.8125\n",
            "Batch : 900|Training Loss: 0.19019941985607147|Training Accuracy : 0.875\n",
            "Batch : 901|Training Loss: 0.17527088522911072|Training Accuracy : 0.9375\n",
            "Batch : 902|Training Loss: 0.21305972337722778|Training Accuracy : 0.84375\n",
            "Batch : 903|Training Loss: 0.23533223569393158|Training Accuracy : 0.9375\n",
            "Batch : 904|Training Loss: 0.1324286311864853|Training Accuracy : 0.9375\n",
            "Batch : 905|Training Loss: 0.1652398556470871|Training Accuracy : 0.96875\n",
            "Batch : 906|Training Loss: 0.08283297717571259|Training Accuracy : 0.96875\n",
            "Batch : 907|Training Loss: 0.15705913305282593|Training Accuracy : 0.90625\n",
            "Batch : 908|Training Loss: 0.2712603807449341|Training Accuracy : 0.90625\n",
            "Batch : 909|Training Loss: 0.31622564792633057|Training Accuracy : 0.84375\n",
            "Batch : 910|Training Loss: 0.08511137962341309|Training Accuracy : 0.9375\n",
            "Batch : 911|Training Loss: 0.10848216712474823|Training Accuracy : 0.9375\n",
            "Batch : 912|Training Loss: 0.11342358589172363|Training Accuracy : 0.9375\n",
            "Batch : 913|Training Loss: 0.09283554553985596|Training Accuracy : 0.9375\n",
            "Batch : 914|Training Loss: 0.12940602004528046|Training Accuracy : 0.96875\n",
            "Batch : 915|Training Loss: 0.28205442428588867|Training Accuracy : 0.875\n",
            "Batch : 916|Training Loss: 0.17881634831428528|Training Accuracy : 0.96875\n",
            "Batch : 917|Training Loss: 0.3338382840156555|Training Accuracy : 0.84375\n",
            "Batch : 918|Training Loss: 0.19980734586715698|Training Accuracy : 0.875\n",
            "Batch : 919|Training Loss: 0.37965208292007446|Training Accuracy : 0.84375\n",
            "Batch : 920|Training Loss: 0.2727580666542053|Training Accuracy : 0.90625\n",
            "Batch : 921|Training Loss: 0.1463206559419632|Training Accuracy : 0.9375\n",
            "Batch : 922|Training Loss: 0.1131117194890976|Training Accuracy : 0.96875\n",
            "Batch : 923|Training Loss: 0.22507590055465698|Training Accuracy : 0.9375\n",
            "Batch : 924|Training Loss: 0.08794742077589035|Training Accuracy : 0.96875\n",
            "Batch : 925|Training Loss: 0.2049909234046936|Training Accuracy : 0.90625\n",
            "Batch : 926|Training Loss: 0.298709899187088|Training Accuracy : 0.875\n",
            "Batch : 927|Training Loss: 0.1059097871184349|Training Accuracy : 0.96875\n",
            "Batch : 928|Training Loss: 0.19553861021995544|Training Accuracy : 0.90625\n",
            "Batch : 929|Training Loss: 0.13498668372631073|Training Accuracy : 0.9375\n",
            "Batch : 930|Training Loss: 0.32729145884513855|Training Accuracy : 0.84375\n",
            "Batch : 931|Training Loss: 0.14519678056240082|Training Accuracy : 0.96875\n",
            "Batch : 932|Training Loss: 0.12490660697221756|Training Accuracy : 0.9375\n",
            "Batch : 933|Training Loss: 0.14580364525318146|Training Accuracy : 0.90625\n",
            "Batch : 934|Training Loss: 0.17756043374538422|Training Accuracy : 0.90625\n",
            "Batch : 935|Training Loss: 0.06821980327367783|Training Accuracy : 0.96875\n",
            "Batch : 936|Training Loss: 0.2436215877532959|Training Accuracy : 0.90625\n",
            "Batch : 937|Training Loss: 0.1988345831632614|Training Accuracy : 0.90625\n",
            "Batch : 938|Training Loss: 0.18115630745887756|Training Accuracy : 0.90625\n",
            "Batch : 939|Training Loss: 0.13700881600379944|Training Accuracy : 0.9375\n",
            "Batch : 940|Training Loss: 0.26289042830467224|Training Accuracy : 0.875\n",
            "Batch : 941|Training Loss: 0.02927440218627453|Training Accuracy : 0.96875\n",
            "Batch : 942|Training Loss: 0.19223491847515106|Training Accuracy : 0.875\n",
            "Batch : 943|Training Loss: 0.2762889266014099|Training Accuracy : 0.90625\n",
            "Batch : 944|Training Loss: 0.09123203903436661|Training Accuracy : 0.96875\n",
            "Batch : 945|Training Loss: 0.13770322501659393|Training Accuracy : 0.96875\n",
            "Batch : 946|Training Loss: 0.0860305055975914|Training Accuracy : 0.96875\n",
            "Batch : 947|Training Loss: 0.061067234724760056|Training Accuracy : 1.0\n",
            "Batch : 948|Training Loss: 0.17283351719379425|Training Accuracy : 0.96875\n",
            "Batch : 949|Training Loss: 0.2959177792072296|Training Accuracy : 0.90625\n",
            "Batch : 950|Training Loss: 0.12086139619350433|Training Accuracy : 0.9375\n",
            "Batch : 951|Training Loss: 0.11019743978977203|Training Accuracy : 0.9375\n",
            "Batch : 952|Training Loss: 0.14490197598934174|Training Accuracy : 0.9375\n",
            "Batch : 953|Training Loss: 0.07769941538572311|Training Accuracy : 0.9375\n",
            "Batch : 954|Training Loss: 0.1743682324886322|Training Accuracy : 0.96875\n",
            "Batch : 955|Training Loss: 0.07090356945991516|Training Accuracy : 0.96875\n",
            "Batch : 956|Training Loss: 0.2880311608314514|Training Accuracy : 0.84375\n",
            "Batch : 957|Training Loss: 0.35358259081840515|Training Accuracy : 0.875\n",
            "Batch : 958|Training Loss: 0.224460169672966|Training Accuracy : 0.90625\n",
            "Batch : 959|Training Loss: 0.21619866788387299|Training Accuracy : 0.9375\n",
            "Batch : 960|Training Loss: 0.1228209137916565|Training Accuracy : 0.9375\n",
            "Batch : 961|Training Loss: 0.03279520571231842|Training Accuracy : 1.0\n",
            "Batch : 962|Training Loss: 0.3467225432395935|Training Accuracy : 0.90625\n",
            "Batch : 963|Training Loss: 0.222557932138443|Training Accuracy : 0.90625\n",
            "Batch : 964|Training Loss: 0.03676838055253029|Training Accuracy : 1.0\n",
            "Batch : 965|Training Loss: 0.05540740489959717|Training Accuracy : 0.96875\n",
            "Batch : 966|Training Loss: 0.13636039197444916|Training Accuracy : 0.9375\n",
            "Batch : 967|Training Loss: 0.28636762499809265|Training Accuracy : 0.875\n",
            "Batch : 968|Training Loss: 0.2788400948047638|Training Accuracy : 0.84375\n",
            "Batch : 969|Training Loss: 0.24033164978027344|Training Accuracy : 0.875\n",
            "Batch : 970|Training Loss: 0.1005949005484581|Training Accuracy : 0.96875\n",
            "Batch : 971|Training Loss: 0.13256919384002686|Training Accuracy : 0.96875\n",
            "Batch : 972|Training Loss: 0.24982400238513947|Training Accuracy : 0.875\n",
            "Batch : 973|Training Loss: 0.15530617535114288|Training Accuracy : 0.9375\n",
            "Batch : 974|Training Loss: 0.22474519908428192|Training Accuracy : 0.90625\n",
            "Batch : 975|Training Loss: 0.2204783856868744|Training Accuracy : 0.9375\n",
            "Batch : 976|Training Loss: 0.30543386936187744|Training Accuracy : 0.9375\n",
            "Batch : 977|Training Loss: 0.2743062973022461|Training Accuracy : 0.9375\n",
            "Batch : 978|Training Loss: 0.21113121509552002|Training Accuracy : 0.90625\n",
            "Batch : 979|Training Loss: 0.14769521355628967|Training Accuracy : 0.9375\n",
            "Batch : 980|Training Loss: 0.15713092684745789|Training Accuracy : 0.90625\n",
            "Batch : 981|Training Loss: 0.27216753363609314|Training Accuracy : 0.875\n",
            "Batch : 982|Training Loss: 0.15151160955429077|Training Accuracy : 0.90625\n",
            "Batch : 983|Training Loss: 0.23809674382209778|Training Accuracy : 0.90625\n",
            "Batch : 984|Training Loss: 0.030475575476884842|Training Accuracy : 1.0\n",
            "Batch : 985|Training Loss: 0.06264155358076096|Training Accuracy : 0.96875\n",
            "Batch : 986|Training Loss: 0.13215485215187073|Training Accuracy : 0.96875\n",
            "Batch : 987|Training Loss: 0.12638676166534424|Training Accuracy : 0.90625\n",
            "Batch : 988|Training Loss: 0.10578420758247375|Training Accuracy : 0.9375\n",
            "Batch : 989|Training Loss: 0.111251100897789|Training Accuracy : 1.0\n",
            "Batch : 990|Training Loss: 0.23750655353069305|Training Accuracy : 0.9375\n",
            "Batch : 991|Training Loss: 0.15952295064926147|Training Accuracy : 0.9375\n",
            "Batch : 992|Training Loss: 0.07797960191965103|Training Accuracy : 1.0\n",
            "Batch : 993|Training Loss: 0.02622903510928154|Training Accuracy : 1.0\n",
            "Batch : 994|Training Loss: 0.20607887208461761|Training Accuracy : 0.90625\n",
            "Batch : 995|Training Loss: 0.20855216681957245|Training Accuracy : 0.9375\n",
            "Batch : 996|Training Loss: 0.1275702267885208|Training Accuracy : 0.96875\n",
            "Batch : 997|Training Loss: 0.1546156257390976|Training Accuracy : 0.90625\n",
            "Batch : 998|Training Loss: 0.10914456099271774|Training Accuracy : 0.9375\n",
            "Batch : 999|Training Loss: 0.11565789580345154|Training Accuracy : 0.90625\n",
            "Batch : 1000|Training Loss: 0.2500353455543518|Training Accuracy : 0.9375\n",
            "Batch : 1001|Training Loss: 0.17642724514007568|Training Accuracy : 0.9375\n",
            "Batch : 1002|Training Loss: 0.13907423615455627|Training Accuracy : 0.90625\n",
            "Batch : 1003|Training Loss: 0.19650840759277344|Training Accuracy : 0.96875\n",
            "Batch : 1004|Training Loss: 0.3462027907371521|Training Accuracy : 0.875\n",
            "Batch : 1005|Training Loss: 0.13452869653701782|Training Accuracy : 0.9375\n",
            "Batch : 1006|Training Loss: 0.08112527430057526|Training Accuracy : 0.9375\n",
            "Batch : 1007|Training Loss: 0.1254531592130661|Training Accuracy : 0.96875\n",
            "Batch : 1008|Training Loss: 0.12551647424697876|Training Accuracy : 0.96875\n",
            "Batch : 1009|Training Loss: 0.14098629355430603|Training Accuracy : 0.9375\n",
            "Batch : 1010|Training Loss: 0.12174242734909058|Training Accuracy : 0.96875\n",
            "Batch : 1011|Training Loss: 0.10841070115566254|Training Accuracy : 0.9375\n",
            "Batch : 1012|Training Loss: 0.19561021029949188|Training Accuracy : 0.90625\n",
            "Batch : 1013|Training Loss: 0.20502227544784546|Training Accuracy : 0.90625\n",
            "Batch : 1014|Training Loss: 0.22295865416526794|Training Accuracy : 0.90625\n",
            "Batch : 1015|Training Loss: 0.3576151430606842|Training Accuracy : 0.875\n",
            "Batch : 1016|Training Loss: 0.17577266693115234|Training Accuracy : 0.90625\n",
            "Batch : 1017|Training Loss: 0.3718632459640503|Training Accuracy : 0.75\n",
            "Batch : 1018|Training Loss: 0.1470571905374527|Training Accuracy : 0.90625\n",
            "Batch : 1019|Training Loss: 0.18849964439868927|Training Accuracy : 0.90625\n",
            "Batch : 1020|Training Loss: 0.19514361023902893|Training Accuracy : 0.9375\n",
            "Batch : 1021|Training Loss: 0.30264535546302795|Training Accuracy : 0.90625\n",
            "Batch : 1022|Training Loss: 0.2043146789073944|Training Accuracy : 0.9375\n",
            "Batch : 1023|Training Loss: 0.16015315055847168|Training Accuracy : 0.96875\n",
            "Batch : 1024|Training Loss: 0.30359217524528503|Training Accuracy : 0.90625\n",
            "Batch : 1025|Training Loss: 0.19086171686649323|Training Accuracy : 0.9375\n",
            "Batch : 1026|Training Loss: 0.19087378680706024|Training Accuracy : 0.9375\n",
            "Batch : 1027|Training Loss: 0.13995906710624695|Training Accuracy : 0.9375\n",
            "Batch : 1028|Training Loss: 0.6384279727935791|Training Accuracy : 0.8125\n",
            "Batch : 1029|Training Loss: 0.19277681410312653|Training Accuracy : 0.90625\n",
            "Batch : 1030|Training Loss: 0.16874580085277557|Training Accuracy : 0.9375\n",
            "Batch : 1031|Training Loss: 0.19850236177444458|Training Accuracy : 0.9375\n",
            "Batch : 1032|Training Loss: 0.06292293220758438|Training Accuracy : 1.0\n",
            "Batch : 1033|Training Loss: 0.20202891528606415|Training Accuracy : 0.9375\n",
            "Batch : 1034|Training Loss: 0.2693805694580078|Training Accuracy : 0.90625\n",
            "Batch : 1035|Training Loss: 0.4172893762588501|Training Accuracy : 0.90625\n",
            "Batch : 1036|Training Loss: 0.1344466507434845|Training Accuracy : 0.96875\n",
            "Batch : 1037|Training Loss: 0.14358004927635193|Training Accuracy : 0.9375\n",
            "Batch : 1038|Training Loss: 0.11780218780040741|Training Accuracy : 0.96875\n",
            "Batch : 1039|Training Loss: 0.20765401422977448|Training Accuracy : 0.9375\n",
            "Batch : 1040|Training Loss: 0.23766562342643738|Training Accuracy : 0.90625\n",
            "Batch : 1041|Training Loss: 0.37073731422424316|Training Accuracy : 0.90625\n",
            "Batch : 1042|Training Loss: 0.36848708987236023|Training Accuracy : 0.875\n",
            "Batch : 1043|Training Loss: 0.18380340933799744|Training Accuracy : 0.9375\n",
            "Batch : 1044|Training Loss: 0.12721000611782074|Training Accuracy : 0.96875\n",
            "Batch : 1045|Training Loss: 0.07454366981983185|Training Accuracy : 1.0\n",
            "Batch : 1046|Training Loss: 0.142068013548851|Training Accuracy : 0.96875\n",
            "Batch : 1047|Training Loss: 0.019327614456415176|Training Accuracy : 1.0\n",
            "Batch : 1048|Training Loss: 0.2859302759170532|Training Accuracy : 0.90625\n",
            "Batch : 1049|Training Loss: 0.26734909415245056|Training Accuracy : 0.84375\n",
            "Batch : 1050|Training Loss: 0.22554926574230194|Training Accuracy : 0.90625\n",
            "Batch : 1051|Training Loss: 0.1440824568271637|Training Accuracy : 0.90625\n",
            "Batch : 1052|Training Loss: 0.5140815377235413|Training Accuracy : 0.875\n",
            "Batch : 1053|Training Loss: 0.05516423285007477|Training Accuracy : 1.0\n",
            "Batch : 1054|Training Loss: 0.34307244420051575|Training Accuracy : 0.90625\n",
            "Batch : 1055|Training Loss: 0.15877023339271545|Training Accuracy : 0.96875\n",
            "Batch : 1056|Training Loss: 0.14461033046245575|Training Accuracy : 0.90625\n",
            "Batch : 1057|Training Loss: 0.2947181165218353|Training Accuracy : 0.90625\n",
            "Batch : 1058|Training Loss: 0.12586018443107605|Training Accuracy : 0.96875\n",
            "Batch : 1059|Training Loss: 0.08980593830347061|Training Accuracy : 0.96875\n",
            "Batch : 1060|Training Loss: 0.13845807313919067|Training Accuracy : 0.90625\n",
            "Batch : 1061|Training Loss: 0.45756784081459045|Training Accuracy : 0.875\n",
            "Batch : 1062|Training Loss: 0.1414039134979248|Training Accuracy : 0.96875\n",
            "Batch : 1063|Training Loss: 0.11108720302581787|Training Accuracy : 0.96875\n",
            "Batch : 1064|Training Loss: 0.13069157302379608|Training Accuracy : 0.96875\n",
            "Batch : 1065|Training Loss: 0.24117203056812286|Training Accuracy : 0.84375\n",
            "Batch : 1066|Training Loss: 0.11720921844244003|Training Accuracy : 0.96875\n",
            "Batch : 1067|Training Loss: 0.1405695378780365|Training Accuracy : 0.9375\n",
            "Batch : 1068|Training Loss: 0.1606677770614624|Training Accuracy : 0.9375\n",
            "Batch : 1069|Training Loss: 0.38623401522636414|Training Accuracy : 0.8125\n",
            "Batch : 1070|Training Loss: 0.16496968269348145|Training Accuracy : 0.96875\n",
            "Batch : 1071|Training Loss: 0.05348248779773712|Training Accuracy : 0.96875\n",
            "Batch : 1072|Training Loss: 0.1320023238658905|Training Accuracy : 0.96875\n",
            "Batch : 1073|Training Loss: 0.24456632137298584|Training Accuracy : 0.875\n",
            "Batch : 1074|Training Loss: 0.16388465464115143|Training Accuracy : 0.875\n",
            "Batch : 1075|Training Loss: 0.16710269451141357|Training Accuracy : 0.9375\n",
            "Batch : 1076|Training Loss: 0.11975648254156113|Training Accuracy : 0.96875\n",
            "Batch : 1077|Training Loss: 0.11486908793449402|Training Accuracy : 0.9375\n",
            "Batch : 1078|Training Loss: 0.4184587895870209|Training Accuracy : 0.84375\n",
            "Batch : 1079|Training Loss: 0.21165700256824493|Training Accuracy : 0.90625\n",
            "Batch : 1080|Training Loss: 0.2100260704755783|Training Accuracy : 0.90625\n",
            "Batch : 1081|Training Loss: 0.09652340412139893|Training Accuracy : 0.9375\n",
            "Batch : 1082|Training Loss: 0.1674366444349289|Training Accuracy : 0.9375\n",
            "Batch : 1083|Training Loss: 0.21479432284832|Training Accuracy : 0.9375\n",
            "Batch : 1084|Training Loss: 0.09948749095201492|Training Accuracy : 0.9375\n",
            "Batch : 1085|Training Loss: 0.2859823405742645|Training Accuracy : 0.84375\n",
            "Batch : 1086|Training Loss: 0.19582776725292206|Training Accuracy : 0.90625\n",
            "Batch : 1087|Training Loss: 0.15030071139335632|Training Accuracy : 0.90625\n",
            "Batch : 1088|Training Loss: 0.10255856812000275|Training Accuracy : 0.96875\n",
            "Batch : 1089|Training Loss: 0.15119707584381104|Training Accuracy : 0.90625\n",
            "Batch : 1090|Training Loss: 0.10749107599258423|Training Accuracy : 0.9375\n",
            "Batch : 1091|Training Loss: 0.22037138044834137|Training Accuracy : 0.90625\n",
            "Batch : 1092|Training Loss: 0.13694117963314056|Training Accuracy : 0.9375\n",
            "Batch : 1093|Training Loss: 0.0661398321390152|Training Accuracy : 1.0\n",
            "Batch : 1094|Training Loss: 0.2217380255460739|Training Accuracy : 0.90625\n",
            "Batch : 1095|Training Loss: 0.10038616508245468|Training Accuracy : 0.96875\n",
            "Batch : 1096|Training Loss: 0.21471136808395386|Training Accuracy : 0.90625\n",
            "Batch : 1097|Training Loss: 0.2676992118358612|Training Accuracy : 0.9375\n",
            "Batch : 1098|Training Loss: 0.46415606141090393|Training Accuracy : 0.84375\n",
            "Batch : 1099|Training Loss: 0.05707228183746338|Training Accuracy : 1.0\n",
            "Batch : 1100|Training Loss: 0.16783997416496277|Training Accuracy : 0.9375\n",
            "Batch : 1101|Training Loss: 0.2524934709072113|Training Accuracy : 0.90625\n",
            "Batch : 1102|Training Loss: 0.07461239397525787|Training Accuracy : 0.96875\n",
            "Batch : 1103|Training Loss: 0.23487816751003265|Training Accuracy : 0.90625\n",
            "Batch : 1104|Training Loss: 0.31274765729904175|Training Accuracy : 0.90625\n",
            "Batch : 1105|Training Loss: 0.14535623788833618|Training Accuracy : 0.90625\n",
            "Batch : 1106|Training Loss: 0.23889222741127014|Training Accuracy : 0.9375\n",
            "Batch : 1107|Training Loss: 0.1597525179386139|Training Accuracy : 0.9375\n",
            "Batch : 1108|Training Loss: 0.00882663019001484|Training Accuracy : 1.0\n",
            "Batch : 1109|Training Loss: 0.14352694153785706|Training Accuracy : 0.9375\n",
            "Batch : 1110|Training Loss: 0.20700639486312866|Training Accuracy : 0.9375\n",
            "Batch : 1111|Training Loss: 0.03318419307470322|Training Accuracy : 1.0\n",
            "Batch : 1112|Training Loss: 0.2872815430164337|Training Accuracy : 0.90625\n",
            "Batch : 1113|Training Loss: 0.15177559852600098|Training Accuracy : 0.9375\n",
            "Batch : 1114|Training Loss: 0.27064403891563416|Training Accuracy : 0.84375\n",
            "Batch : 1115|Training Loss: 0.5239293575286865|Training Accuracy : 0.8125\n",
            "Batch : 1116|Training Loss: 0.3021507263183594|Training Accuracy : 0.875\n",
            "Batch : 1117|Training Loss: 0.29464098811149597|Training Accuracy : 0.875\n",
            "Batch : 1118|Training Loss: 0.05293554067611694|Training Accuracy : 1.0\n",
            "Batch : 1119|Training Loss: 0.06085687130689621|Training Accuracy : 0.96875\n",
            "Batch : 1120|Training Loss: 0.15575820207595825|Training Accuracy : 0.90625\n",
            "Batch : 1121|Training Loss: 0.16363772749900818|Training Accuracy : 0.9375\n",
            "Batch : 1122|Training Loss: 0.11649155616760254|Training Accuracy : 0.96875\n",
            "Batch : 1123|Training Loss: 0.31756821274757385|Training Accuracy : 0.9375\n",
            "Batch : 1124|Training Loss: 0.06420039385557175|Training Accuracy : 0.96875\n",
            "Batch : 1125|Training Loss: 0.1319608986377716|Training Accuracy : 0.9375\n",
            "Batch : 1126|Training Loss: 0.253435879945755|Training Accuracy : 0.875\n",
            "Batch : 1127|Training Loss: 0.14039826393127441|Training Accuracy : 0.9375\n",
            "Batch : 1128|Training Loss: 0.05944768711924553|Training Accuracy : 0.96875\n",
            "Batch : 1129|Training Loss: 0.34193792939186096|Training Accuracy : 0.9375\n",
            "Batch : 1130|Training Loss: 0.23426760733127594|Training Accuracy : 0.90625\n",
            "Batch : 1131|Training Loss: 0.2334727644920349|Training Accuracy : 0.90625\n",
            "Batch : 1132|Training Loss: 0.18591031432151794|Training Accuracy : 0.9375\n",
            "Batch : 1133|Training Loss: 0.16974323987960815|Training Accuracy : 0.96875\n",
            "Batch : 1134|Training Loss: 0.11090321838855743|Training Accuracy : 0.96875\n",
            "Batch : 1135|Training Loss: 0.2130490392446518|Training Accuracy : 0.90625\n",
            "Batch : 1136|Training Loss: 0.20107334852218628|Training Accuracy : 0.90625\n",
            "Batch : 1137|Training Loss: 0.08042450249195099|Training Accuracy : 1.0\n",
            "Batch : 1138|Training Loss: 0.2786923050880432|Training Accuracy : 0.84375\n",
            "Batch : 1139|Training Loss: 0.1231033205986023|Training Accuracy : 0.9375\n",
            "Batch : 1140|Training Loss: 0.46574893593788147|Training Accuracy : 0.84375\n",
            "Batch : 1141|Training Loss: 0.12932556867599487|Training Accuracy : 0.96875\n",
            "Batch : 1142|Training Loss: 0.12171894311904907|Training Accuracy : 0.96875\n",
            "Batch : 1143|Training Loss: 0.197929248213768|Training Accuracy : 0.90625\n",
            "Batch : 1144|Training Loss: 0.10496558248996735|Training Accuracy : 0.9375\n",
            "Batch : 1145|Training Loss: 0.33276695013046265|Training Accuracy : 0.875\n",
            "Batch : 1146|Training Loss: 0.19614864885807037|Training Accuracy : 0.9375\n",
            "Batch : 1147|Training Loss: 0.2165924459695816|Training Accuracy : 0.875\n",
            "Batch : 1148|Training Loss: 0.05792972445487976|Training Accuracy : 1.0\n",
            "Batch : 1149|Training Loss: 0.08229847252368927|Training Accuracy : 0.96875\n",
            "Batch : 1150|Training Loss: 0.2088596671819687|Training Accuracy : 0.90625\n",
            "Batch : 1151|Training Loss: 0.16473786532878876|Training Accuracy : 0.9375\n",
            "Batch : 1152|Training Loss: 0.1853007674217224|Training Accuracy : 0.9375\n",
            "Batch : 1153|Training Loss: 0.12300828099250793|Training Accuracy : 0.96875\n",
            "Batch : 1154|Training Loss: 0.07753218710422516|Training Accuracy : 0.9375\n",
            "Batch : 1155|Training Loss: 0.13206616044044495|Training Accuracy : 0.9375\n",
            "Batch : 1156|Training Loss: 0.14907479286193848|Training Accuracy : 0.9375\n",
            "Batch : 1157|Training Loss: 0.03613327816128731|Training Accuracy : 1.0\n",
            "Batch : 1158|Training Loss: 0.12099479138851166|Training Accuracy : 0.96875\n",
            "Batch : 1159|Training Loss: 0.11541866511106491|Training Accuracy : 0.96875\n",
            "Batch : 1160|Training Loss: 0.22607995569705963|Training Accuracy : 0.90625\n",
            "Batch : 1161|Training Loss: 0.05162188038229942|Training Accuracy : 0.96875\n",
            "Batch : 1162|Training Loss: 0.13304707407951355|Training Accuracy : 0.9375\n",
            "Batch : 1163|Training Loss: 0.2759009599685669|Training Accuracy : 0.875\n",
            "Batch : 1164|Training Loss: 0.17196258902549744|Training Accuracy : 0.9375\n",
            "Batch : 1165|Training Loss: 0.43855512142181396|Training Accuracy : 0.8125\n",
            "Batch : 1166|Training Loss: 0.18577614426612854|Training Accuracy : 0.96875\n",
            "Batch : 1167|Training Loss: 0.22226352989673615|Training Accuracy : 0.90625\n",
            "Batch : 1168|Training Loss: 0.19210460782051086|Training Accuracy : 0.90625\n",
            "Batch : 1169|Training Loss: 0.3716520667076111|Training Accuracy : 0.875\n",
            "Batch : 1170|Training Loss: 0.3528481125831604|Training Accuracy : 0.90625\n",
            "Batch : 1171|Training Loss: 0.17765752971172333|Training Accuracy : 0.9375\n",
            "Batch : 1172|Training Loss: 0.13775505125522614|Training Accuracy : 0.96875\n",
            "Batch : 1173|Training Loss: 0.08146708458662033|Training Accuracy : 1.0\n",
            "Batch : 1174|Training Loss: 0.354057252407074|Training Accuracy : 0.90625\n",
            "Batch : 1175|Training Loss: 0.1290634274482727|Training Accuracy : 0.96875\n",
            "Batch : 1176|Training Loss: 0.38104772567749023|Training Accuracy : 0.84375\n",
            "Batch : 1177|Training Loss: 0.3486843407154083|Training Accuracy : 0.875\n",
            "Batch : 1178|Training Loss: 0.054790981113910675|Training Accuracy : 1.0\n",
            "Batch : 1179|Training Loss: 0.07574864476919174|Training Accuracy : 1.0\n",
            "Batch : 1180|Training Loss: 0.1516781896352768|Training Accuracy : 0.9375\n",
            "Batch : 1181|Training Loss: 0.22629022598266602|Training Accuracy : 0.90625\n",
            "Batch : 1182|Training Loss: 0.03606836125254631|Training Accuracy : 0.96875\n",
            "Batch : 1183|Training Loss: 0.13251352310180664|Training Accuracy : 0.96875\n",
            "Batch : 1184|Training Loss: 0.3188932240009308|Training Accuracy : 0.9375\n",
            "Batch : 1185|Training Loss: 0.2531125247478485|Training Accuracy : 0.875\n",
            "Batch : 1186|Training Loss: 0.061619218438863754|Training Accuracy : 1.0\n",
            "Batch : 1187|Training Loss: 0.09352124482393265|Training Accuracy : 0.96875\n",
            "Batch : 1188|Training Loss: 0.09644018113613129|Training Accuracy : 0.96875\n",
            "Batch : 1189|Training Loss: 0.09118840098381042|Training Accuracy : 0.96875\n",
            "Batch : 1190|Training Loss: 0.37811538577079773|Training Accuracy : 0.90625\n",
            "Batch : 1191|Training Loss: 0.10427695512771606|Training Accuracy : 1.0\n",
            "Batch : 1192|Training Loss: 0.10192376375198364|Training Accuracy : 0.9375\n",
            "Batch : 1193|Training Loss: 0.22777602076530457|Training Accuracy : 0.9375\n",
            "Batch : 1194|Training Loss: 0.1431378275156021|Training Accuracy : 0.9375\n",
            "Batch : 1195|Training Loss: 0.2648734748363495|Training Accuracy : 0.84375\n",
            "Batch : 1196|Training Loss: 0.1991652250289917|Training Accuracy : 0.9375\n",
            "Batch : 1197|Training Loss: 0.2733447551727295|Training Accuracy : 0.90625\n",
            "Batch : 1198|Training Loss: 0.33681368827819824|Training Accuracy : 0.84375\n",
            "Batch : 1199|Training Loss: 0.09910877048969269|Training Accuracy : 0.9375\n",
            "Batch : 1200|Training Loss: 0.3894397020339966|Training Accuracy : 0.84375\n",
            "Batch : 1201|Training Loss: 0.22558258473873138|Training Accuracy : 0.9375\n",
            "Batch : 1202|Training Loss: 0.15400028228759766|Training Accuracy : 0.9375\n",
            "Batch : 1203|Training Loss: 0.2855428159236908|Training Accuracy : 0.9375\n",
            "Batch : 1204|Training Loss: 0.4168035089969635|Training Accuracy : 0.8125\n",
            "Batch : 1205|Training Loss: 0.08820412307977676|Training Accuracy : 1.0\n",
            "Batch : 1206|Training Loss: 0.2302929013967514|Training Accuracy : 0.90625\n",
            "Batch : 1207|Training Loss: 0.06867913901805878|Training Accuracy : 0.96875\n",
            "Batch : 1208|Training Loss: 0.1754244863986969|Training Accuracy : 0.90625\n",
            "Batch : 1209|Training Loss: 0.15278789401054382|Training Accuracy : 0.9375\n",
            "Batch : 1210|Training Loss: 0.07389405369758606|Training Accuracy : 0.96875\n",
            "Batch : 1211|Training Loss: 0.3648001253604889|Training Accuracy : 0.84375\n",
            "Batch : 1212|Training Loss: 0.14184637367725372|Training Accuracy : 0.9375\n",
            "Batch : 1213|Training Loss: 0.15137213468551636|Training Accuracy : 0.96875\n",
            "Batch : 1214|Training Loss: 0.22175399959087372|Training Accuracy : 0.90625\n",
            "Batch : 1215|Training Loss: 0.09955964982509613|Training Accuracy : 0.96875\n",
            "Batch : 1216|Training Loss: 0.25500738620758057|Training Accuracy : 0.875\n",
            "Batch : 1217|Training Loss: 0.0842432752251625|Training Accuracy : 0.96875\n",
            "Batch : 1218|Training Loss: 0.2223043143749237|Training Accuracy : 0.90625\n",
            "Batch : 1219|Training Loss: 0.45350703597068787|Training Accuracy : 0.84375\n",
            "Batch : 1220|Training Loss: 0.23255276679992676|Training Accuracy : 0.875\n",
            "Batch : 1221|Training Loss: 0.1403791308403015|Training Accuracy : 0.9375\n",
            "Batch : 1222|Training Loss: 0.19077995419502258|Training Accuracy : 0.9375\n",
            "Batch : 1223|Training Loss: 0.12005272507667542|Training Accuracy : 0.96875\n",
            "Batch : 1224|Training Loss: 0.22880563139915466|Training Accuracy : 0.96875\n",
            "Batch : 1225|Training Loss: 0.26399660110473633|Training Accuracy : 0.90625\n",
            "Batch : 1226|Training Loss: 0.06338270008563995|Training Accuracy : 1.0\n",
            "Batch : 1227|Training Loss: 0.2213074415922165|Training Accuracy : 0.90625\n",
            "Batch : 1228|Training Loss: 0.13388529419898987|Training Accuracy : 0.96875\n",
            "Batch : 1229|Training Loss: 0.16913656890392303|Training Accuracy : 0.9375\n",
            "Batch : 1230|Training Loss: 0.3458416163921356|Training Accuracy : 0.875\n",
            "Batch : 1231|Training Loss: 0.10341180860996246|Training Accuracy : 0.96875\n",
            "Batch : 1232|Training Loss: 0.12160944193601608|Training Accuracy : 0.96875\n",
            "Batch : 1233|Training Loss: 0.03352599963545799|Training Accuracy : 1.0\n",
            "Batch : 1234|Training Loss: 0.23259195685386658|Training Accuracy : 0.90625\n",
            "Batch : 1235|Training Loss: 0.12803488969802856|Training Accuracy : 0.96875\n",
            "Batch : 1236|Training Loss: 0.09341973066329956|Training Accuracy : 0.96875\n",
            "Batch : 1237|Training Loss: 0.21576575934886932|Training Accuracy : 0.9375\n",
            "Batch : 1238|Training Loss: 0.19437673687934875|Training Accuracy : 0.90625\n",
            "Batch : 1239|Training Loss: 0.07932519912719727|Training Accuracy : 0.9375\n",
            "Batch : 1240|Training Loss: 0.21143096685409546|Training Accuracy : 0.875\n",
            "Batch : 1241|Training Loss: 0.33728936314582825|Training Accuracy : 0.875\n",
            "Batch : 1242|Training Loss: 0.12273457646369934|Training Accuracy : 0.9375\n",
            "Batch : 1243|Training Loss: 0.06436106562614441|Training Accuracy : 0.96875\n",
            "Batch : 1244|Training Loss: 0.22350084781646729|Training Accuracy : 0.9375\n",
            "Batch : 1245|Training Loss: 0.04550179839134216|Training Accuracy : 1.0\n",
            "Batch : 1246|Training Loss: 0.14176136255264282|Training Accuracy : 0.96875\n",
            "Batch : 1247|Training Loss: 0.21356678009033203|Training Accuracy : 0.875\n",
            "Batch : 1248|Training Loss: 0.19822408258914948|Training Accuracy : 0.9375\n",
            "Batch : 1249|Training Loss: 0.0937725231051445|Training Accuracy : 0.9375\n",
            "Batch : 1250|Training Loss: 0.23636461794376373|Training Accuracy : 0.875\n",
            "Batch : 1251|Training Loss: 0.22280147671699524|Training Accuracy : 0.9375\n",
            "Batch : 1252|Training Loss: 0.08734806627035141|Training Accuracy : 0.96875\n",
            "Batch : 1253|Training Loss: 0.09452592581510544|Training Accuracy : 0.96875\n",
            "Batch : 1254|Training Loss: 0.13922318816184998|Training Accuracy : 0.9375\n",
            "Batch : 1255|Training Loss: 0.05410083755850792|Training Accuracy : 1.0\n",
            "Batch : 1256|Training Loss: 0.37413352727890015|Training Accuracy : 0.875\n",
            "Batch : 1257|Training Loss: 0.30664271116256714|Training Accuracy : 0.90625\n",
            "Batch : 1258|Training Loss: 0.24523182213306427|Training Accuracy : 0.875\n",
            "Batch : 1259|Training Loss: 0.23428025841712952|Training Accuracy : 0.90625\n",
            "Batch : 1260|Training Loss: 0.08980315923690796|Training Accuracy : 0.96875\n",
            "Batch : 1261|Training Loss: 0.06818678975105286|Training Accuracy : 0.96875\n",
            "Batch : 1262|Training Loss: 0.12468108534812927|Training Accuracy : 0.96875\n",
            "Batch : 1263|Training Loss: 0.1292041689157486|Training Accuracy : 0.96875\n",
            "Batch : 1264|Training Loss: 0.18045713007450104|Training Accuracy : 0.9375\n",
            "Batch : 1265|Training Loss: 0.09524915367364883|Training Accuracy : 0.9375\n",
            "Batch : 1266|Training Loss: 0.1323731243610382|Training Accuracy : 0.9375\n",
            "Batch : 1267|Training Loss: 0.020264366641640663|Training Accuracy : 1.0\n",
            "Batch : 1268|Training Loss: 0.18538153171539307|Training Accuracy : 0.90625\n",
            "Batch : 1269|Training Loss: 0.09735304117202759|Training Accuracy : 0.90625\n",
            "Batch : 1270|Training Loss: 0.26702794432640076|Training Accuracy : 0.875\n",
            "Batch : 1271|Training Loss: 0.06668698787689209|Training Accuracy : 0.96875\n",
            "Batch : 1272|Training Loss: 0.09012450277805328|Training Accuracy : 0.96875\n",
            "Batch : 1273|Training Loss: 0.21670308709144592|Training Accuracy : 0.96875\n",
            "Batch : 1274|Training Loss: 0.42577841877937317|Training Accuracy : 0.90625\n",
            "Batch : 1275|Training Loss: 0.27386829257011414|Training Accuracy : 0.9375\n",
            "Batch : 1276|Training Loss: 0.11523328721523285|Training Accuracy : 0.96875\n",
            "Batch : 1277|Training Loss: 0.11756865680217743|Training Accuracy : 0.9375\n",
            "Batch : 1278|Training Loss: 0.09255031496286392|Training Accuracy : 0.96875\n",
            "Batch : 1279|Training Loss: 0.45937052369117737|Training Accuracy : 0.78125\n",
            "Batch : 1280|Training Loss: 0.07171452790498734|Training Accuracy : 1.0\n",
            "Batch : 1281|Training Loss: 0.035590559244155884|Training Accuracy : 1.0\n",
            "Batch : 1282|Training Loss: 0.18058675527572632|Training Accuracy : 0.90625\n",
            "Batch : 1283|Training Loss: 0.20617833733558655|Training Accuracy : 0.9375\n",
            "Batch : 1284|Training Loss: 0.2233111709356308|Training Accuracy : 0.90625\n",
            "Batch : 1285|Training Loss: 0.1325412094593048|Training Accuracy : 0.96875\n",
            "Batch : 1286|Training Loss: 0.13820244371891022|Training Accuracy : 0.9375\n",
            "Batch : 1287|Training Loss: 0.40131187438964844|Training Accuracy : 0.84375\n",
            "Batch : 1288|Training Loss: 0.24248532950878143|Training Accuracy : 0.9375\n",
            "Batch : 1289|Training Loss: 0.1757601499557495|Training Accuracy : 0.9375\n",
            "Batch : 1290|Training Loss: 0.20067572593688965|Training Accuracy : 0.9375\n",
            "Batch : 1291|Training Loss: 0.27181631326675415|Training Accuracy : 0.84375\n",
            "Batch : 1292|Training Loss: 0.07807310670614243|Training Accuracy : 0.96875\n",
            "Batch : 1293|Training Loss: 0.2275463491678238|Training Accuracy : 0.90625\n",
            "Batch : 1294|Training Loss: 0.14006458222866058|Training Accuracy : 0.9375\n",
            "Batch : 1295|Training Loss: 0.16683724522590637|Training Accuracy : 0.9375\n",
            "Batch : 1296|Training Loss: 0.2614744007587433|Training Accuracy : 0.90625\n",
            "Batch : 1297|Training Loss: 0.2725467085838318|Training Accuracy : 0.90625\n",
            "Batch : 1298|Training Loss: 0.14271900057792664|Training Accuracy : 0.96875\n",
            "Batch : 1299|Training Loss: 0.1295979917049408|Training Accuracy : 0.96875\n",
            "Batch : 1300|Training Loss: 0.20884788036346436|Training Accuracy : 0.96875\n",
            "Batch : 1301|Training Loss: 0.1556481271982193|Training Accuracy : 0.90625\n",
            "Batch : 1302|Training Loss: 0.29582831263542175|Training Accuracy : 0.84375\n",
            "Batch : 1303|Training Loss: 0.3350932002067566|Training Accuracy : 0.78125\n",
            "Batch : 1304|Training Loss: 0.134073406457901|Training Accuracy : 0.96875\n",
            "Batch : 1305|Training Loss: 0.3460661768913269|Training Accuracy : 0.8125\n",
            "Batch : 1306|Training Loss: 0.19745184481143951|Training Accuracy : 0.90625\n",
            "Batch : 1307|Training Loss: 0.33449846506118774|Training Accuracy : 0.875\n",
            "Batch : 1308|Training Loss: 0.1878504753112793|Training Accuracy : 0.90625\n",
            "Batch : 1309|Training Loss: 0.23620441555976868|Training Accuracy : 0.875\n",
            "Batch : 1310|Training Loss: 0.26785117387771606|Training Accuracy : 0.9375\n",
            "Batch : 1311|Training Loss: 0.2276943325996399|Training Accuracy : 0.90625\n",
            "Batch : 1312|Training Loss: 0.23102955520153046|Training Accuracy : 0.875\n",
            "Batch : 1313|Training Loss: 0.17697669565677643|Training Accuracy : 0.96875\n",
            "Batch : 1314|Training Loss: 0.1060703694820404|Training Accuracy : 0.9375\n",
            "Batch : 1315|Training Loss: 0.1246984675526619|Training Accuracy : 0.96875\n",
            "Batch : 1316|Training Loss: 0.2611420154571533|Training Accuracy : 0.90625\n",
            "Batch : 1317|Training Loss: 0.2190583348274231|Training Accuracy : 0.90625\n",
            "Batch : 1318|Training Loss: 0.047475192695856094|Training Accuracy : 1.0\n",
            "Batch : 1319|Training Loss: 0.28827065229415894|Training Accuracy : 0.90625\n",
            "Batch : 1320|Training Loss: 0.23528295755386353|Training Accuracy : 0.9375\n",
            "Batch : 1321|Training Loss: 0.2888886630535126|Training Accuracy : 0.9375\n",
            "Batch : 1322|Training Loss: 0.13442182540893555|Training Accuracy : 0.96875\n",
            "Batch : 1323|Training Loss: 0.2201559990644455|Training Accuracy : 0.90625\n",
            "Batch : 1324|Training Loss: 0.15557101368904114|Training Accuracy : 0.9375\n",
            "Batch : 1325|Training Loss: 0.20425888895988464|Training Accuracy : 0.875\n",
            "Batch : 1326|Training Loss: 0.21673297882080078|Training Accuracy : 0.875\n",
            "Batch : 1327|Training Loss: 0.11934428662061691|Training Accuracy : 0.96875\n",
            "Batch : 1328|Training Loss: 0.39469581842422485|Training Accuracy : 0.875\n",
            "Batch : 1329|Training Loss: 0.22583939135074615|Training Accuracy : 0.90625\n",
            "Batch : 1330|Training Loss: 0.1735771745443344|Training Accuracy : 0.9375\n",
            "Batch : 1331|Training Loss: 0.17321665585041046|Training Accuracy : 0.90625\n",
            "Batch : 1332|Training Loss: 0.21811218559741974|Training Accuracy : 0.90625\n",
            "Batch : 1333|Training Loss: 0.12084639072418213|Training Accuracy : 0.96875\n",
            "Batch : 1334|Training Loss: 0.17167165875434875|Training Accuracy : 0.9375\n",
            "Batch : 1335|Training Loss: 0.2685639262199402|Training Accuracy : 0.875\n",
            "Batch : 1336|Training Loss: 0.08727018535137177|Training Accuracy : 1.0\n",
            "Batch : 1337|Training Loss: 0.07914382219314575|Training Accuracy : 0.9375\n",
            "Batch : 1338|Training Loss: 0.38610023260116577|Training Accuracy : 0.875\n",
            "Batch : 1339|Training Loss: 0.24176687002182007|Training Accuracy : 0.9375\n",
            "Batch : 1340|Training Loss: 0.07156482338905334|Training Accuracy : 0.96875\n",
            "Batch : 1341|Training Loss: 0.27830269932746887|Training Accuracy : 0.875\n",
            "Batch : 1342|Training Loss: 0.34784287214279175|Training Accuracy : 0.90625\n",
            "Batch : 1343|Training Loss: 0.1261567622423172|Training Accuracy : 0.9375\n",
            "Batch : 1344|Training Loss: 0.09928090125322342|Training Accuracy : 0.96875\n",
            "Batch : 1345|Training Loss: 0.23717834055423737|Training Accuracy : 0.90625\n",
            "Batch : 1346|Training Loss: 0.18832457065582275|Training Accuracy : 0.9375\n",
            "Batch : 1347|Training Loss: 0.4314063787460327|Training Accuracy : 0.84375\n",
            "Batch : 1348|Training Loss: 0.13731642067432404|Training Accuracy : 0.9375\n",
            "Batch : 1349|Training Loss: 0.1368219554424286|Training Accuracy : 0.9375\n",
            "Batch : 1350|Training Loss: 0.08816741406917572|Training Accuracy : 0.9375\n",
            "Batch : 1351|Training Loss: 0.24778112769126892|Training Accuracy : 0.96875\n",
            "Batch : 1352|Training Loss: 0.13742603361606598|Training Accuracy : 0.9375\n",
            "Batch : 1353|Training Loss: 0.2107325792312622|Training Accuracy : 0.875\n",
            "Batch : 1354|Training Loss: 0.25314056873321533|Training Accuracy : 0.90625\n",
            "Batch : 1355|Training Loss: 0.0910048559308052|Training Accuracy : 0.96875\n",
            "Batch : 1356|Training Loss: 0.10141859203577042|Training Accuracy : 0.96875\n",
            "Batch : 1357|Training Loss: 0.15313251316547394|Training Accuracy : 0.9375\n",
            "Batch : 1358|Training Loss: 0.020891724154353142|Training Accuracy : 1.0\n",
            "Batch : 1359|Training Loss: 0.2218507081270218|Training Accuracy : 0.90625\n",
            "Batch : 1360|Training Loss: 0.2849019765853882|Training Accuracy : 0.90625\n",
            "Batch : 1361|Training Loss: 0.20066148042678833|Training Accuracy : 0.90625\n",
            "Batch : 1362|Training Loss: 0.19259294867515564|Training Accuracy : 0.90625\n",
            "Batch : 1363|Training Loss: 0.11883040517568588|Training Accuracy : 0.96875\n",
            "Batch : 1364|Training Loss: 0.17785778641700745|Training Accuracy : 0.90625\n",
            "Batch : 1365|Training Loss: 0.3191415071487427|Training Accuracy : 0.875\n",
            "Batch : 1366|Training Loss: 0.29151833057403564|Training Accuracy : 0.875\n",
            "Batch : 1367|Training Loss: 0.19704560935497284|Training Accuracy : 0.9375\n",
            "Batch : 1368|Training Loss: 0.32947292923927307|Training Accuracy : 0.875\n",
            "Batch : 1369|Training Loss: 0.13153879344463348|Training Accuracy : 0.90625\n",
            "Batch : 1370|Training Loss: 0.07679566740989685|Training Accuracy : 0.96875\n",
            "Batch : 1371|Training Loss: 0.3809990882873535|Training Accuracy : 0.875\n",
            "Batch : 1372|Training Loss: 0.2630668878555298|Training Accuracy : 0.875\n",
            "Batch : 1373|Training Loss: 0.1741841435432434|Training Accuracy : 0.9375\n",
            "Batch : 1374|Training Loss: 0.12102411687374115|Training Accuracy : 0.9375\n",
            "Batch : 1375|Training Loss: 0.21311187744140625|Training Accuracy : 0.90625\n",
            "Batch : 1376|Training Loss: 0.09037114679813385|Training Accuracy : 1.0\n",
            "Batch : 1377|Training Loss: 0.1484733521938324|Training Accuracy : 0.90625\n",
            "Batch : 1378|Training Loss: 0.17189563810825348|Training Accuracy : 0.96875\n",
            "Batch : 1379|Training Loss: 0.22014670073986053|Training Accuracy : 0.9375\n",
            "Batch : 1380|Training Loss: 0.28875207901000977|Training Accuracy : 0.875\n",
            "Batch : 1381|Training Loss: 0.11050613224506378|Training Accuracy : 0.9375\n",
            "Batch : 1382|Training Loss: 0.19748875498771667|Training Accuracy : 0.875\n",
            "Batch : 1383|Training Loss: 0.20783159136772156|Training Accuracy : 0.90625\n",
            "Batch : 1384|Training Loss: 0.03162778168916702|Training Accuracy : 1.0\n",
            "Batch : 1385|Training Loss: 0.1434279978275299|Training Accuracy : 0.9375\n",
            "Batch : 1386|Training Loss: 0.03844773396849632|Training Accuracy : 1.0\n",
            "Batch : 1387|Training Loss: 0.1771639585494995|Training Accuracy : 0.9375\n",
            "Batch : 1388|Training Loss: 0.21335537731647491|Training Accuracy : 0.90625\n",
            "Batch : 1389|Training Loss: 0.11754593253135681|Training Accuracy : 0.96875\n",
            "Batch : 1390|Training Loss: 0.35664790868759155|Training Accuracy : 0.90625\n",
            "Batch : 1391|Training Loss: 0.17085716128349304|Training Accuracy : 0.9375\n",
            "Batch : 1392|Training Loss: 0.191076397895813|Training Accuracy : 0.875\n",
            "Batch : 1393|Training Loss: 0.1166183277964592|Training Accuracy : 0.9375\n",
            "Batch : 1394|Training Loss: 0.26105788350105286|Training Accuracy : 0.84375\n",
            "Batch : 1395|Training Loss: 0.11526062339544296|Training Accuracy : 0.96875\n",
            "Batch : 1396|Training Loss: 0.062429845333099365|Training Accuracy : 0.9375\n",
            "Batch : 1397|Training Loss: 0.2832806408405304|Training Accuracy : 0.90625\n",
            "Batch : 1398|Training Loss: 0.5253859758377075|Training Accuracy : 0.875\n",
            "Batch : 1399|Training Loss: 0.10157398134469986|Training Accuracy : 0.96875\n",
            "Batch : 1400|Training Loss: 0.20110754668712616|Training Accuracy : 0.90625\n",
            "Batch : 1401|Training Loss: 0.20302170515060425|Training Accuracy : 0.90625\n",
            "Batch : 1402|Training Loss: 0.21487456560134888|Training Accuracy : 0.9375\n",
            "Batch : 1403|Training Loss: 0.2985541522502899|Training Accuracy : 0.84375\n",
            "Batch : 1404|Training Loss: 0.1276157945394516|Training Accuracy : 0.96875\n",
            "Batch : 1405|Training Loss: 0.1267337203025818|Training Accuracy : 0.9375\n",
            "Batch : 1406|Training Loss: 0.09693204611539841|Training Accuracy : 0.9375\n",
            "Batch : 1407|Training Loss: 0.12929785251617432|Training Accuracy : 0.9375\n",
            "Batch : 1408|Training Loss: 0.2817971408367157|Training Accuracy : 0.90625\n",
            "Batch : 1409|Training Loss: 0.11025907844305038|Training Accuracy : 0.96875\n",
            "Batch : 1410|Training Loss: 0.26151958107948303|Training Accuracy : 0.90625\n",
            "Batch : 1411|Training Loss: 0.36010977625846863|Training Accuracy : 0.875\n",
            "Batch : 1412|Training Loss: 0.1936030089855194|Training Accuracy : 0.90625\n",
            "Batch : 1413|Training Loss: 0.3575538396835327|Training Accuracy : 0.90625\n",
            "Batch : 1414|Training Loss: 0.19556546211242676|Training Accuracy : 0.9375\n",
            "Batch : 1415|Training Loss: 0.35936328768730164|Training Accuracy : 0.875\n",
            "Batch : 1416|Training Loss: 0.12304959446191788|Training Accuracy : 0.96875\n",
            "Batch : 1417|Training Loss: 0.09647759795188904|Training Accuracy : 1.0\n",
            "Batch : 1418|Training Loss: 0.15660926699638367|Training Accuracy : 0.9375\n",
            "Batch : 1419|Training Loss: 0.2530018091201782|Training Accuracy : 0.96875\n",
            "Batch : 1420|Training Loss: 0.23449821770191193|Training Accuracy : 0.875\n",
            "Batch : 1421|Training Loss: 0.16620376706123352|Training Accuracy : 0.90625\n",
            "Batch : 1422|Training Loss: 0.1405981332063675|Training Accuracy : 0.9375\n",
            "Batch : 1423|Training Loss: 0.14703285694122314|Training Accuracy : 0.9375\n",
            "Batch : 1424|Training Loss: 0.1188386008143425|Training Accuracy : 0.9375\n",
            "Batch : 1425|Training Loss: 0.252433717250824|Training Accuracy : 0.875\n",
            "Batch : 1426|Training Loss: 0.09355089068412781|Training Accuracy : 1.0\n",
            "Batch : 1427|Training Loss: 0.21295982599258423|Training Accuracy : 0.90625\n",
            "Batch : 1428|Training Loss: 0.19854220747947693|Training Accuracy : 0.9375\n",
            "Batch : 1429|Training Loss: 0.16566765308380127|Training Accuracy : 0.9375\n",
            "Batch : 1430|Training Loss: 0.133933424949646|Training Accuracy : 0.9375\n",
            "Batch : 1431|Training Loss: 0.08317684382200241|Training Accuracy : 0.96875\n",
            "Batch : 1432|Training Loss: 0.09909124672412872|Training Accuracy : 0.96875\n",
            "Batch : 1433|Training Loss: 0.14893420040607452|Training Accuracy : 0.96875\n",
            "Batch : 1434|Training Loss: 0.12305320799350739|Training Accuracy : 0.96875\n",
            "Batch : 1435|Training Loss: 0.1208406388759613|Training Accuracy : 0.96875\n",
            "Batch : 1436|Training Loss: 0.10001576691865921|Training Accuracy : 0.96875\n",
            "Batch : 1437|Training Loss: 0.16539910435676575|Training Accuracy : 0.90625\n",
            "Batch : 1438|Training Loss: 0.08534232527017593|Training Accuracy : 1.0\n",
            "Batch : 1439|Training Loss: 0.43758827447891235|Training Accuracy : 0.90625\n",
            "Batch : 1440|Training Loss: 0.17123523354530334|Training Accuracy : 0.9375\n",
            "Batch : 1441|Training Loss: 0.2584421634674072|Training Accuracy : 0.9375\n",
            "Batch : 1442|Training Loss: 0.08130408823490143|Training Accuracy : 1.0\n",
            "Batch : 1443|Training Loss: 0.09109031409025192|Training Accuracy : 0.96875\n",
            "Batch : 1444|Training Loss: 0.26582032442092896|Training Accuracy : 0.875\n",
            "Batch : 1445|Training Loss: 0.0255743358284235|Training Accuracy : 1.0\n",
            "Batch : 1446|Training Loss: 0.034821685403585434|Training Accuracy : 1.0\n",
            "Batch : 1447|Training Loss: 0.05910911411046982|Training Accuracy : 0.96875\n",
            "Batch : 1448|Training Loss: 0.137588769197464|Training Accuracy : 0.90625\n",
            "Batch : 1449|Training Loss: 0.16370539367198944|Training Accuracy : 0.9375\n",
            "Batch : 1450|Training Loss: 0.16663037240505219|Training Accuracy : 0.96875\n",
            "Batch : 1451|Training Loss: 0.09210828691720963|Training Accuracy : 0.96875\n",
            "Batch : 1452|Training Loss: 0.15456418693065643|Training Accuracy : 0.9375\n",
            "Batch : 1453|Training Loss: 0.1119697168469429|Training Accuracy : 0.9375\n",
            "Batch : 1454|Training Loss: 0.43831682205200195|Training Accuracy : 0.90625\n",
            "Batch : 1455|Training Loss: 0.1148037239909172|Training Accuracy : 0.9375\n",
            "Batch : 1456|Training Loss: 0.0495288223028183|Training Accuracy : 0.96875\n",
            "Batch : 1457|Training Loss: 0.06820424646139145|Training Accuracy : 1.0\n",
            "Batch : 1458|Training Loss: 0.11200770735740662|Training Accuracy : 0.96875\n",
            "Batch : 1459|Training Loss: 0.09265313297510147|Training Accuracy : 0.96875\n",
            "Batch : 1460|Training Loss: 0.1992666870355606|Training Accuracy : 0.9375\n",
            "Batch : 1461|Training Loss: 0.19714361429214478|Training Accuracy : 0.875\n",
            "Batch : 1462|Training Loss: 0.02566344290971756|Training Accuracy : 1.0\n",
            "Batch : 1463|Training Loss: 0.06121306121349335|Training Accuracy : 1.0\n",
            "Batch : 1464|Training Loss: 0.03042561560869217|Training Accuracy : 1.0\n",
            "Batch : 1465|Training Loss: 0.06709862500429153|Training Accuracy : 1.0\n",
            "Batch : 1466|Training Loss: 0.058990586549043655|Training Accuracy : 0.96875\n",
            "Batch : 1467|Training Loss: 0.19032427668571472|Training Accuracy : 0.90625\n",
            "Batch : 1468|Training Loss: 0.09668443351984024|Training Accuracy : 0.96875\n",
            "Batch : 1469|Training Loss: 0.040082890540361404|Training Accuracy : 1.0\n",
            "Batch : 1470|Training Loss: 0.33867451548576355|Training Accuracy : 0.90625\n",
            "Batch : 1471|Training Loss: 0.07149352133274078|Training Accuracy : 0.96875\n",
            "Batch : 1472|Training Loss: 0.24713134765625|Training Accuracy : 0.875\n",
            "Batch : 1473|Training Loss: 0.19005566835403442|Training Accuracy : 0.90625\n",
            "Batch : 1474|Training Loss: 0.07560819387435913|Training Accuracy : 0.96875\n",
            "Batch : 1475|Training Loss: 0.27341577410697937|Training Accuracy : 0.84375\n",
            "Batch : 1476|Training Loss: 0.15510796010494232|Training Accuracy : 0.9375\n",
            "Batch : 1477|Training Loss: 0.10981816053390503|Training Accuracy : 0.9375\n",
            "Batch : 1478|Training Loss: 0.08233539015054703|Training Accuracy : 0.96875\n",
            "Batch : 1479|Training Loss: 0.036003679037094116|Training Accuracy : 1.0\n",
            "Batch : 1480|Training Loss: 0.05438593402504921|Training Accuracy : 1.0\n",
            "Batch : 1481|Training Loss: 0.35474899411201477|Training Accuracy : 0.90625\n",
            "Batch : 1482|Training Loss: 0.3910124897956848|Training Accuracy : 0.90625\n",
            "Batch : 1483|Training Loss: 0.102135568857193|Training Accuracy : 0.9375\n",
            "Batch : 1484|Training Loss: 0.21671536564826965|Training Accuracy : 0.90625\n",
            "Batch : 1485|Training Loss: 0.10311894118785858|Training Accuracy : 0.96875\n",
            "Batch : 1486|Training Loss: 0.17143872380256653|Training Accuracy : 0.9375\n",
            "Batch : 1487|Training Loss: 0.33726581931114197|Training Accuracy : 0.90625\n",
            "Batch : 1488|Training Loss: 0.31673017144203186|Training Accuracy : 0.875\n",
            "Batch : 1489|Training Loss: 0.23841655254364014|Training Accuracy : 0.90625\n",
            "Batch : 1490|Training Loss: 0.1500173956155777|Training Accuracy : 0.96875\n",
            "Batch : 1491|Training Loss: 0.1377670019865036|Training Accuracy : 0.96875\n",
            "Batch : 1492|Training Loss: 0.10094073414802551|Training Accuracy : 0.96875\n",
            "Batch : 1493|Training Loss: 0.10708507150411606|Training Accuracy : 0.9375\n",
            "Batch : 1494|Training Loss: 0.19620941579341888|Training Accuracy : 0.875\n",
            "Batch : 1495|Training Loss: 0.12986114621162415|Training Accuracy : 0.9375\n",
            "Batch : 1496|Training Loss: 0.04015115648508072|Training Accuracy : 1.0\n",
            "Batch : 1497|Training Loss: 0.0952194333076477|Training Accuracy : 0.96875\n",
            "Batch : 1498|Training Loss: 0.17966096103191376|Training Accuracy : 0.9375\n",
            "Batch : 1499|Training Loss: 0.40500250458717346|Training Accuracy : 0.8125\n",
            "Batch : 1500|Training Loss: 0.09957104921340942|Training Accuracy : 0.9375\n",
            "Batch : 1501|Training Loss: 0.30627113580703735|Training Accuracy : 0.90625\n",
            "Batch : 1502|Training Loss: 0.16921541094779968|Training Accuracy : 0.9375\n",
            "Batch : 1503|Training Loss: 0.07007782906293869|Training Accuracy : 0.96875\n",
            "Batch : 1504|Training Loss: 0.09303227812051773|Training Accuracy : 0.96875\n",
            "Batch : 1505|Training Loss: 0.12703384459018707|Training Accuracy : 0.9375\n",
            "Batch : 1506|Training Loss: 0.3853546679019928|Training Accuracy : 0.84375\n",
            "Batch : 1507|Training Loss: 0.18749509751796722|Training Accuracy : 0.90625\n",
            "Batch : 1508|Training Loss: 0.045676276087760925|Training Accuracy : 1.0\n",
            "Batch : 1509|Training Loss: 0.15014342963695526|Training Accuracy : 0.90625\n",
            "Batch : 1510|Training Loss: 0.3063085675239563|Training Accuracy : 0.90625\n",
            "Batch : 1511|Training Loss: 0.1865728199481964|Training Accuracy : 0.90625\n",
            "Batch : 1512|Training Loss: 0.08866435289382935|Training Accuracy : 0.9375\n",
            "Batch : 1513|Training Loss: 0.17435237765312195|Training Accuracy : 0.9375\n",
            "Batch : 1514|Training Loss: 0.14889149367809296|Training Accuracy : 0.9375\n",
            "Batch : 1515|Training Loss: 0.39821377396583557|Training Accuracy : 0.875\n",
            "Batch : 1516|Training Loss: 0.28110820055007935|Training Accuracy : 0.875\n",
            "Batch : 1517|Training Loss: 0.3309883177280426|Training Accuracy : 0.84375\n",
            "Batch : 1518|Training Loss: 0.3154108226299286|Training Accuracy : 0.875\n",
            "Batch : 1519|Training Loss: 0.18832625448703766|Training Accuracy : 0.875\n",
            "Batch : 1520|Training Loss: 0.17677953839302063|Training Accuracy : 0.9375\n",
            "Batch : 1521|Training Loss: 0.2405613660812378|Training Accuracy : 0.9375\n",
            "Batch : 1522|Training Loss: 0.03922737389802933|Training Accuracy : 1.0\n",
            "Batch : 1523|Training Loss: 0.1414993405342102|Training Accuracy : 0.9375\n",
            "Batch : 1524|Training Loss: 0.14148053526878357|Training Accuracy : 0.9375\n",
            "Batch : 1525|Training Loss: 0.26783254742622375|Training Accuracy : 0.875\n",
            "Batch : 1526|Training Loss: 0.04806582257151604|Training Accuracy : 1.0\n",
            "Batch : 1527|Training Loss: 0.07276075333356857|Training Accuracy : 1.0\n",
            "Batch : 1528|Training Loss: 0.1402631551027298|Training Accuracy : 0.9375\n",
            "Batch : 1529|Training Loss: 0.09699375182390213|Training Accuracy : 0.96875\n",
            "Batch : 1530|Training Loss: 0.15876340866088867|Training Accuracy : 0.9375\n",
            "Batch : 1531|Training Loss: 0.035198744386434555|Training Accuracy : 1.0\n",
            "Batch : 1532|Training Loss: 0.11105974018573761|Training Accuracy : 0.96875\n",
            "Batch : 1533|Training Loss: 0.28205788135528564|Training Accuracy : 0.875\n",
            "Batch : 1534|Training Loss: 0.20001961290836334|Training Accuracy : 0.9375\n",
            "Batch : 1535|Training Loss: 0.03447924926877022|Training Accuracy : 1.0\n",
            "Batch : 1536|Training Loss: 0.10374578833580017|Training Accuracy : 0.96875\n",
            "Batch : 1537|Training Loss: 0.7030644416809082|Training Accuracy : 0.84375\n",
            "Batch : 1538|Training Loss: 0.23892049491405487|Training Accuracy : 0.90625\n",
            "Batch : 1539|Training Loss: 0.10205157101154327|Training Accuracy : 0.9375\n",
            "Batch : 1540|Training Loss: 0.13391995429992676|Training Accuracy : 0.9375\n",
            "Batch : 1541|Training Loss: 0.13348489999771118|Training Accuracy : 0.96875\n",
            "Batch : 1542|Training Loss: 0.3700331449508667|Training Accuracy : 0.8125\n",
            "Batch : 1543|Training Loss: 0.17406147718429565|Training Accuracy : 0.9375\n",
            "Batch : 1544|Training Loss: 0.222505122423172|Training Accuracy : 0.90625\n",
            "Batch : 1545|Training Loss: 0.31475725769996643|Training Accuracy : 0.84375\n",
            "Batch : 1546|Training Loss: 0.08416705578565598|Training Accuracy : 0.96875\n",
            "Batch : 1547|Training Loss: 0.18214575946331024|Training Accuracy : 0.875\n",
            "Batch : 1548|Training Loss: 0.13395565748214722|Training Accuracy : 0.9375\n",
            "Batch : 1549|Training Loss: 0.08288755267858505|Training Accuracy : 1.0\n",
            "Batch : 1550|Training Loss: 0.1726924628019333|Training Accuracy : 0.96875\n",
            "Batch : 1551|Training Loss: 0.3603581488132477|Training Accuracy : 0.9375\n",
            "Batch : 1552|Training Loss: 0.5465307831764221|Training Accuracy : 0.875\n",
            "Batch : 1553|Training Loss: 0.14555592834949493|Training Accuracy : 0.96875\n",
            "Batch : 1554|Training Loss: 0.09800669550895691|Training Accuracy : 0.9375\n",
            "Batch : 1555|Training Loss: 0.5246139764785767|Training Accuracy : 0.84375\n",
            "Batch : 1556|Training Loss: 0.15395843982696533|Training Accuracy : 0.9375\n",
            "Batch : 1557|Training Loss: 0.30154576897621155|Training Accuracy : 0.84375\n",
            "Batch : 1558|Training Loss: 0.10515088587999344|Training Accuracy : 0.96875\n",
            "Batch : 1559|Training Loss: 0.19733555614948273|Training Accuracy : 0.90625\n",
            "Batch : 1560|Training Loss: 0.25883689522743225|Training Accuracy : 0.90625\n",
            "Batch : 1561|Training Loss: 0.11488376557826996|Training Accuracy : 0.96875\n",
            "Batch : 1562|Training Loss: 0.34891265630722046|Training Accuracy : 0.875\n",
            "Batch : 1563|Training Loss: 0.16322781145572662|Training Accuracy : 0.9375\n",
            "Batch : 1564|Training Loss: 0.08267685770988464|Training Accuracy : 0.96875\n",
            "Batch : 1565|Training Loss: 0.07289794087409973|Training Accuracy : 0.96875\n",
            "Batch : 1566|Training Loss: 0.1497301161289215|Training Accuracy : 0.9375\n",
            "Batch : 1567|Training Loss: 0.33021312952041626|Training Accuracy : 0.90625\n",
            "Batch : 1568|Training Loss: 0.24862511456012726|Training Accuracy : 0.90625\n",
            "Batch : 1569|Training Loss: 0.1823512762784958|Training Accuracy : 0.9375\n",
            "Batch : 1570|Training Loss: 0.11289629340171814|Training Accuracy : 0.9375\n",
            "Batch : 1571|Training Loss: 0.1518111526966095|Training Accuracy : 0.96875\n",
            "Batch : 1572|Training Loss: 0.20928643643856049|Training Accuracy : 0.875\n",
            "Batch : 1573|Training Loss: 0.179836243391037|Training Accuracy : 0.90625\n",
            "Batch : 1574|Training Loss: 0.11929861456155777|Training Accuracy : 0.96875\n",
            "Batch : 1575|Training Loss: 0.16100642085075378|Training Accuracy : 0.90625\n",
            "Batch : 1576|Training Loss: 0.14057762920856476|Training Accuracy : 0.90625\n",
            "Batch : 1577|Training Loss: 0.17953409254550934|Training Accuracy : 0.9375\n",
            "Batch : 1578|Training Loss: 0.09778650850057602|Training Accuracy : 0.9375\n",
            "Batch : 1579|Training Loss: 0.08794790506362915|Training Accuracy : 0.96875\n",
            "Batch : 1580|Training Loss: 0.2008378952741623|Training Accuracy : 0.9375\n",
            "Batch : 1581|Training Loss: 0.26131635904312134|Training Accuracy : 0.9375\n",
            "Batch : 1582|Training Loss: 0.3082263469696045|Training Accuracy : 0.90625\n",
            "Batch : 1583|Training Loss: 0.17010042071342468|Training Accuracy : 0.90625\n",
            "Batch : 1584|Training Loss: 0.38125079870224|Training Accuracy : 0.84375\n",
            "Batch : 1585|Training Loss: 0.36605924367904663|Training Accuracy : 0.78125\n",
            "Batch : 1586|Training Loss: 0.19645586609840393|Training Accuracy : 0.9375\n",
            "Batch : 1587|Training Loss: 0.47486838698387146|Training Accuracy : 0.84375\n",
            "Batch : 1588|Training Loss: 0.23040816187858582|Training Accuracy : 0.9375\n",
            "Batch : 1589|Training Loss: 0.08277595788240433|Training Accuracy : 0.96875\n",
            "Batch : 1590|Training Loss: 0.18671084940433502|Training Accuracy : 0.90625\n",
            "Batch : 1591|Training Loss: 0.18172048032283783|Training Accuracy : 0.90625\n",
            "Batch : 1592|Training Loss: 0.1457309126853943|Training Accuracy : 0.9375\n",
            "Batch : 1593|Training Loss: 0.5663452744483948|Training Accuracy : 0.84375\n",
            "Batch : 1594|Training Loss: 0.1643046885728836|Training Accuracy : 0.9375\n",
            "Batch : 1595|Training Loss: 0.19321098923683167|Training Accuracy : 0.9375\n",
            "Batch : 1596|Training Loss: 0.09722240269184113|Training Accuracy : 1.0\n",
            "Batch : 1597|Training Loss: 0.14011020958423615|Training Accuracy : 0.96875\n",
            "Batch : 1598|Training Loss: 0.11257421970367432|Training Accuracy : 0.96875\n",
            "Batch : 1599|Training Loss: 0.140222430229187|Training Accuracy : 0.9375\n",
            "Batch : 1600|Training Loss: 0.043162859976291656|Training Accuracy : 1.0\n",
            "Batch : 1601|Training Loss: 0.13981536030769348|Training Accuracy : 0.9375\n",
            "Batch : 1602|Training Loss: 0.22674192488193512|Training Accuracy : 0.90625\n",
            "Batch : 1603|Training Loss: 0.10771074146032333|Training Accuracy : 0.9375\n",
            "Batch : 1604|Training Loss: 0.19595909118652344|Training Accuracy : 0.96875\n",
            "Batch : 1605|Training Loss: 0.21634075045585632|Training Accuracy : 0.9375\n",
            "Batch : 1606|Training Loss: 0.056427158415317535|Training Accuracy : 1.0\n",
            "Batch : 1607|Training Loss: 0.026165343821048737|Training Accuracy : 1.0\n",
            "Batch : 1608|Training Loss: 0.2102718949317932|Training Accuracy : 0.90625\n",
            "Batch : 1609|Training Loss: 0.22522062063217163|Training Accuracy : 0.90625\n",
            "Batch : 1610|Training Loss: 0.2115488499403|Training Accuracy : 0.9375\n",
            "Batch : 1611|Training Loss: 0.2076745331287384|Training Accuracy : 0.90625\n",
            "Batch : 1612|Training Loss: 0.2550298571586609|Training Accuracy : 0.9375\n",
            "Batch : 1613|Training Loss: 0.10545997321605682|Training Accuracy : 0.9375\n",
            "Batch : 1614|Training Loss: 0.07715290039777756|Training Accuracy : 1.0\n",
            "Batch : 1615|Training Loss: 0.05933589115738869|Training Accuracy : 0.96875\n",
            "Batch : 1616|Training Loss: 0.18393829464912415|Training Accuracy : 0.90625\n",
            "Batch : 1617|Training Loss: 0.2652500867843628|Training Accuracy : 0.875\n",
            "Batch : 1618|Training Loss: 0.22423624992370605|Training Accuracy : 0.875\n",
            "Batch : 1619|Training Loss: 0.2552282512187958|Training Accuracy : 0.90625\n",
            "Batch : 1620|Training Loss: 0.14734867215156555|Training Accuracy : 0.9375\n",
            "Batch : 1621|Training Loss: 0.11353539675474167|Training Accuracy : 0.96875\n",
            "Batch : 1622|Training Loss: 0.1475551575422287|Training Accuracy : 0.9375\n",
            "Batch : 1623|Training Loss: 0.1426866352558136|Training Accuracy : 0.9375\n",
            "Batch : 1624|Training Loss: 0.3261503279209137|Training Accuracy : 0.84375\n",
            "Batch : 1625|Training Loss: 0.3706416189670563|Training Accuracy : 0.875\n",
            "Batch : 1626|Training Loss: 0.28776052594184875|Training Accuracy : 0.84375\n",
            "Batch : 1627|Training Loss: 0.3076150417327881|Training Accuracy : 0.84375\n",
            "Batch : 1628|Training Loss: 0.47156932950019836|Training Accuracy : 0.84375\n",
            "Batch : 1629|Training Loss: 0.2566152513027191|Training Accuracy : 0.875\n",
            "Batch : 1630|Training Loss: 0.3990756571292877|Training Accuracy : 0.8125\n",
            "Batch : 1631|Training Loss: 0.3084217309951782|Training Accuracy : 0.90625\n",
            "Batch : 1632|Training Loss: 0.15794047713279724|Training Accuracy : 0.9375\n",
            "Batch : 1633|Training Loss: 0.17503714561462402|Training Accuracy : 0.90625\n",
            "Batch : 1634|Training Loss: 0.18680302798748016|Training Accuracy : 0.90625\n",
            "Batch : 1635|Training Loss: 0.09748439490795135|Training Accuracy : 0.96875\n",
            "Batch : 1636|Training Loss: 0.1933603733778|Training Accuracy : 0.9375\n",
            "Batch : 1637|Training Loss: 0.26114752888679504|Training Accuracy : 0.90625\n",
            "Batch : 1638|Training Loss: 0.2980307340621948|Training Accuracy : 0.84375\n",
            "Batch : 1639|Training Loss: 0.18546190857887268|Training Accuracy : 0.90625\n",
            "Batch : 1640|Training Loss: 0.18765732645988464|Training Accuracy : 0.90625\n",
            "Batch : 1641|Training Loss: 0.050623007118701935|Training Accuracy : 1.0\n",
            "Batch : 1642|Training Loss: 0.13652224838733673|Training Accuracy : 0.96875\n",
            "Batch : 1643|Training Loss: 0.3125321865081787|Training Accuracy : 0.90625\n",
            "Batch : 1644|Training Loss: 0.05045159161090851|Training Accuracy : 1.0\n",
            "Batch : 1645|Training Loss: 0.06686849892139435|Training Accuracy : 0.96875\n",
            "Batch : 1646|Training Loss: 0.20212002098560333|Training Accuracy : 0.90625\n",
            "Batch : 1647|Training Loss: 0.1032358855009079|Training Accuracy : 0.96875\n",
            "Batch : 1648|Training Loss: 0.23657146096229553|Training Accuracy : 0.90625\n",
            "Batch : 1649|Training Loss: 0.32102566957473755|Training Accuracy : 0.90625\n",
            "Batch : 1650|Training Loss: 0.1399499475955963|Training Accuracy : 0.96875\n",
            "Batch : 1651|Training Loss: 0.2320997714996338|Training Accuracy : 0.90625\n",
            "Batch : 1652|Training Loss: 0.09710957109928131|Training Accuracy : 0.96875\n",
            "Batch : 1653|Training Loss: 0.06041897088289261|Training Accuracy : 1.0\n",
            "Batch : 1654|Training Loss: 0.263679176568985|Training Accuracy : 0.90625\n",
            "Batch : 1655|Training Loss: 0.2587176263332367|Training Accuracy : 0.90625\n",
            "Batch : 1656|Training Loss: 0.27506768703460693|Training Accuracy : 0.875\n",
            "Batch : 1657|Training Loss: 0.11374875158071518|Training Accuracy : 1.0\n",
            "Batch : 1658|Training Loss: 0.12125059962272644|Training Accuracy : 0.9375\n",
            "Batch : 1659|Training Loss: 0.2544238269329071|Training Accuracy : 0.875\n",
            "Batch : 1660|Training Loss: 0.1749747097492218|Training Accuracy : 0.96875\n",
            "Batch : 1661|Training Loss: 0.03526853397488594|Training Accuracy : 0.96875\n",
            "Batch : 1662|Training Loss: 0.08783331513404846|Training Accuracy : 0.96875\n",
            "Batch : 1663|Training Loss: 0.0693965032696724|Training Accuracy : 0.96875\n",
            "Batch : 1664|Training Loss: 0.13424313068389893|Training Accuracy : 0.9375\n",
            "Batch : 1665|Training Loss: 0.13219280540943146|Training Accuracy : 0.90625\n",
            "Batch : 1666|Training Loss: 0.07473421096801758|Training Accuracy : 0.96875\n",
            "Batch : 1667|Training Loss: 0.19997498393058777|Training Accuracy : 0.96875\n",
            "Batch : 1668|Training Loss: 0.035976167768239975|Training Accuracy : 0.96875\n",
            "Batch : 1669|Training Loss: 0.2972070574760437|Training Accuracy : 0.9375\n",
            "Batch : 1670|Training Loss: 0.13701248168945312|Training Accuracy : 0.96875\n",
            "Batch : 1671|Training Loss: 0.4699714779853821|Training Accuracy : 0.8125\n",
            "Batch : 1672|Training Loss: 0.14862576127052307|Training Accuracy : 0.9375\n",
            "Batch : 1673|Training Loss: 0.056014399975538254|Training Accuracy : 1.0\n",
            "Batch : 1674|Training Loss: 0.24997924268245697|Training Accuracy : 0.875\n",
            "Batch : 1675|Training Loss: 0.32254713773727417|Training Accuracy : 0.84375\n",
            "Batch : 1676|Training Loss: 0.13843706250190735|Training Accuracy : 0.96875\n",
            "Batch : 1677|Training Loss: 0.0882904976606369|Training Accuracy : 0.96875\n",
            "Batch : 1678|Training Loss: 0.13888579607009888|Training Accuracy : 0.96875\n",
            "Batch : 1679|Training Loss: 0.11771991848945618|Training Accuracy : 0.9375\n",
            "Batch : 1680|Training Loss: 0.03744892776012421|Training Accuracy : 1.0\n",
            "Batch : 1681|Training Loss: 0.15735097229480743|Training Accuracy : 0.96875\n",
            "Batch : 1682|Training Loss: 0.2312871366739273|Training Accuracy : 0.84375\n",
            "Batch : 1683|Training Loss: 0.28324830532073975|Training Accuracy : 0.84375\n",
            "Batch : 1684|Training Loss: 0.1294269859790802|Training Accuracy : 0.9375\n",
            "Batch : 1685|Training Loss: 0.13053195178508759|Training Accuracy : 0.96875\n",
            "Batch : 1686|Training Loss: 0.1328124701976776|Training Accuracy : 0.9375\n",
            "Batch : 1687|Training Loss: 0.43517830967903137|Training Accuracy : 0.78125\n",
            "Batch : 1688|Training Loss: 0.16938181221485138|Training Accuracy : 0.9375\n",
            "Batch : 1689|Training Loss: 0.09007391333580017|Training Accuracy : 0.96875\n",
            "Batch : 1690|Training Loss: 0.1969597339630127|Training Accuracy : 0.90625\n",
            "Batch : 1691|Training Loss: 0.23687134683132172|Training Accuracy : 0.9375\n",
            "Batch : 1692|Training Loss: 0.3337210416793823|Training Accuracy : 0.90625\n",
            "Batch : 1693|Training Loss: 0.13260287046432495|Training Accuracy : 0.96875\n",
            "Batch : 1694|Training Loss: 0.12752559781074524|Training Accuracy : 0.9375\n",
            "Batch : 1695|Training Loss: 0.18854735791683197|Training Accuracy : 0.9375\n",
            "Batch : 1696|Training Loss: 0.15471597015857697|Training Accuracy : 0.96875\n",
            "Batch : 1697|Training Loss: 0.10111115872859955|Training Accuracy : 1.0\n",
            "Batch : 1698|Training Loss: 0.12359339743852615|Training Accuracy : 0.96875\n",
            "Batch : 1699|Training Loss: 0.09677281975746155|Training Accuracy : 0.9375\n",
            "Batch : 1700|Training Loss: 0.05256108567118645|Training Accuracy : 1.0\n",
            "Batch : 1701|Training Loss: 0.0959324762225151|Training Accuracy : 0.9375\n",
            "Batch : 1702|Training Loss: 0.05636516585946083|Training Accuracy : 1.0\n",
            "Batch : 1703|Training Loss: 0.024849066510796547|Training Accuracy : 1.0\n",
            "Batch : 1704|Training Loss: 0.09016431868076324|Training Accuracy : 0.9375\n",
            "Batch : 1705|Training Loss: 0.07829633355140686|Training Accuracy : 1.0\n",
            "Batch : 1706|Training Loss: 0.2765369415283203|Training Accuracy : 0.9375\n",
            "Batch : 1707|Training Loss: 0.18484613299369812|Training Accuracy : 0.9375\n",
            "Batch : 1708|Training Loss: 0.16027645766735077|Training Accuracy : 0.96875\n",
            "Batch : 1709|Training Loss: 0.38931557536125183|Training Accuracy : 0.8125\n",
            "Batch : 1710|Training Loss: 0.22301119565963745|Training Accuracy : 0.875\n",
            "Batch : 1711|Training Loss: 0.34864452481269836|Training Accuracy : 0.875\n",
            "Batch : 1712|Training Loss: 0.08767497539520264|Training Accuracy : 0.96875\n",
            "Batch : 1713|Training Loss: 0.32253479957580566|Training Accuracy : 0.90625\n",
            "Batch : 1714|Training Loss: 0.33588331937789917|Training Accuracy : 0.8125\n",
            "Batch : 1715|Training Loss: 0.08600647747516632|Training Accuracy : 0.96875\n",
            "Batch : 1716|Training Loss: 0.19830402731895447|Training Accuracy : 0.9375\n",
            "Batch : 1717|Training Loss: 0.2771851122379303|Training Accuracy : 0.84375\n",
            "Batch : 1718|Training Loss: 0.08172476291656494|Training Accuracy : 0.96875\n",
            "Batch : 1719|Training Loss: 0.2778232991695404|Training Accuracy : 0.875\n",
            "Batch : 1720|Training Loss: 0.07379896938800812|Training Accuracy : 0.96875\n",
            "Batch : 1721|Training Loss: 0.06971432268619537|Training Accuracy : 0.96875\n",
            "Batch : 1722|Training Loss: 0.05409672111272812|Training Accuracy : 0.96875\n",
            "Batch : 1723|Training Loss: 0.2945021688938141|Training Accuracy : 0.9375\n",
            "Batch : 1724|Training Loss: 0.2730863094329834|Training Accuracy : 0.9375\n",
            "Batch : 1725|Training Loss: 0.08356756716966629|Training Accuracy : 0.96875\n",
            "Batch : 1726|Training Loss: 0.21739628911018372|Training Accuracy : 0.90625\n",
            "Batch : 1727|Training Loss: 0.16744886338710785|Training Accuracy : 0.9375\n",
            "Batch : 1728|Training Loss: 0.21616315841674805|Training Accuracy : 0.9375\n",
            "Batch : 1729|Training Loss: 0.09986376762390137|Training Accuracy : 0.9375\n",
            "Batch : 1730|Training Loss: 0.05635610222816467|Training Accuracy : 1.0\n",
            "Batch : 1731|Training Loss: 0.013276053592562675|Training Accuracy : 1.0\n",
            "Batch : 1732|Training Loss: 0.06697071343660355|Training Accuracy : 1.0\n",
            "Batch : 1733|Training Loss: 0.03979422152042389|Training Accuracy : 1.0\n",
            "Batch : 1734|Training Loss: 0.09533261507749557|Training Accuracy : 0.96875\n",
            "Batch : 1735|Training Loss: 0.2778373956680298|Training Accuracy : 0.875\n",
            "Batch : 1736|Training Loss: 0.06247803568840027|Training Accuracy : 0.96875\n",
            "Batch : 1737|Training Loss: 0.1619589626789093|Training Accuracy : 0.9375\n",
            "Batch : 1738|Training Loss: 0.17107294499874115|Training Accuracy : 0.90625\n",
            "Batch : 1739|Training Loss: 0.16256321966648102|Training Accuracy : 0.9375\n",
            "Batch : 1740|Training Loss: 0.02347196452319622|Training Accuracy : 1.0\n",
            "Batch : 1741|Training Loss: 0.20174336433410645|Training Accuracy : 0.90625\n",
            "Batch : 1742|Training Loss: 0.29417768120765686|Training Accuracy : 0.90625\n",
            "Batch : 1743|Training Loss: 0.12577742338180542|Training Accuracy : 0.9375\n",
            "Batch : 1744|Training Loss: 0.01875198818743229|Training Accuracy : 1.0\n",
            "Batch : 1745|Training Loss: 0.1293978989124298|Training Accuracy : 0.9375\n",
            "Batch : 1746|Training Loss: 0.16224077343940735|Training Accuracy : 0.9375\n",
            "Batch : 1747|Training Loss: 0.095547616481781|Training Accuracy : 0.96875\n",
            "Batch : 1748|Training Loss: 0.10939452052116394|Training Accuracy : 0.96875\n",
            "Batch : 1749|Training Loss: 0.2914167046546936|Training Accuracy : 0.90625\n",
            "Batch : 1750|Training Loss: 0.2107749581336975|Training Accuracy : 0.9375\n",
            "Batch : 1751|Training Loss: 0.19497743248939514|Training Accuracy : 0.9375\n",
            "Batch : 1752|Training Loss: 0.3829638957977295|Training Accuracy : 0.90625\n",
            "Batch : 1753|Training Loss: 0.0067904950119555|Training Accuracy : 1.0\n",
            "Batch : 1754|Training Loss: 0.08629918098449707|Training Accuracy : 1.0\n",
            "Batch : 1755|Training Loss: 0.1087723821401596|Training Accuracy : 0.96875\n",
            "Batch : 1756|Training Loss: 0.0227346271276474|Training Accuracy : 1.0\n",
            "Batch : 1757|Training Loss: 0.09561660140752792|Training Accuracy : 0.96875\n",
            "Batch : 1758|Training Loss: 0.40270310640335083|Training Accuracy : 0.84375\n",
            "Batch : 1759|Training Loss: 0.2643758952617645|Training Accuracy : 0.84375\n",
            "Batch : 1760|Training Loss: 0.13582748174667358|Training Accuracy : 0.96875\n",
            "Batch : 1761|Training Loss: 0.19284875690937042|Training Accuracy : 0.96875\n",
            "Batch : 1762|Training Loss: 0.08284716308116913|Training Accuracy : 0.96875\n",
            "Batch : 1763|Training Loss: 0.06951182335615158|Training Accuracy : 0.96875\n",
            "Batch : 1764|Training Loss: 0.2382403016090393|Training Accuracy : 0.90625\n",
            "Batch : 1765|Training Loss: 0.15627814829349518|Training Accuracy : 0.96875\n",
            "Batch : 1766|Training Loss: 0.05978646129369736|Training Accuracy : 0.96875\n",
            "Batch : 1767|Training Loss: 0.16262799501419067|Training Accuracy : 0.90625\n",
            "Batch : 1768|Training Loss: 0.2238062173128128|Training Accuracy : 0.90625\n",
            "Batch : 1769|Training Loss: 0.058953799307346344|Training Accuracy : 0.96875\n",
            "Batch : 1770|Training Loss: 0.07958821952342987|Training Accuracy : 0.96875\n",
            "Batch : 1771|Training Loss: 0.22898221015930176|Training Accuracy : 0.90625\n",
            "Batch : 1772|Training Loss: 0.20623579621315002|Training Accuracy : 0.9375\n",
            "Batch : 1773|Training Loss: 0.20688174664974213|Training Accuracy : 0.9375\n",
            "Batch : 1774|Training Loss: 0.28401434421539307|Training Accuracy : 0.875\n",
            "Batch : 1775|Training Loss: 0.20090125501155853|Training Accuracy : 0.90625\n",
            "Batch : 1776|Training Loss: 0.1826801896095276|Training Accuracy : 0.9375\n",
            "Batch : 1777|Training Loss: 0.37756359577178955|Training Accuracy : 0.84375\n",
            "Batch : 1778|Training Loss: 0.08829723298549652|Training Accuracy : 1.0\n",
            "Batch : 1779|Training Loss: 0.21381762623786926|Training Accuracy : 0.90625\n",
            "Batch : 1780|Training Loss: 0.09800964593887329|Training Accuracy : 0.9375\n",
            "Batch : 1781|Training Loss: 0.19766229391098022|Training Accuracy : 0.90625\n",
            "Batch : 1782|Training Loss: 0.07456148415803909|Training Accuracy : 1.0\n",
            "Batch : 1783|Training Loss: 0.18090860545635223|Training Accuracy : 0.875\n",
            "Batch : 1784|Training Loss: 0.061082880944013596|Training Accuracy : 0.96875\n",
            "Batch : 1785|Training Loss: 0.04572197422385216|Training Accuracy : 1.0\n",
            "Batch : 1786|Training Loss: 0.04181217402219772|Training Accuracy : 0.96875\n",
            "Batch : 1787|Training Loss: 0.21012619137763977|Training Accuracy : 0.96875\n",
            "Batch : 1788|Training Loss: 0.05964432284235954|Training Accuracy : 0.96875\n",
            "Batch : 1789|Training Loss: 0.15501391887664795|Training Accuracy : 0.9375\n",
            "Batch : 1790|Training Loss: 0.09724664688110352|Training Accuracy : 0.96875\n",
            "Batch : 1791|Training Loss: 0.25155210494995117|Training Accuracy : 0.84375\n",
            "Batch : 1792|Training Loss: 0.08117721974849701|Training Accuracy : 0.96875\n",
            "Batch : 1793|Training Loss: 0.2784304916858673|Training Accuracy : 0.90625\n",
            "Batch : 1794|Training Loss: 0.24120557308197021|Training Accuracy : 0.875\n",
            "Batch : 1795|Training Loss: 0.16876941919326782|Training Accuracy : 0.90625\n",
            "Batch : 1796|Training Loss: 0.2079569697380066|Training Accuracy : 0.9375\n",
            "Batch : 1797|Training Loss: 0.19743111729621887|Training Accuracy : 0.90625\n",
            "Batch : 1798|Training Loss: 0.18296502530574799|Training Accuracy : 0.9375\n",
            "Batch : 1799|Training Loss: 0.10831660032272339|Training Accuracy : 0.9375\n",
            "Batch : 1800|Training Loss: 0.1931304782629013|Training Accuracy : 0.9375\n",
            "Batch : 1801|Training Loss: 0.33100637793540955|Training Accuracy : 0.90625\n",
            "Batch : 1802|Training Loss: 0.10290277749300003|Training Accuracy : 1.0\n",
            "Batch : 1803|Training Loss: 0.18248175084590912|Training Accuracy : 0.90625\n",
            "Batch : 1804|Training Loss: 0.358262300491333|Training Accuracy : 0.8125\n",
            "Batch : 1805|Training Loss: 0.35232415795326233|Training Accuracy : 0.8125\n",
            "Batch : 1806|Training Loss: 0.24352680146694183|Training Accuracy : 0.90625\n",
            "Batch : 1807|Training Loss: 0.22997014224529266|Training Accuracy : 0.9375\n",
            "Batch : 1808|Training Loss: 0.10509993135929108|Training Accuracy : 0.90625\n",
            "Batch : 1809|Training Loss: 0.3385540544986725|Training Accuracy : 0.9375\n",
            "Batch : 1810|Training Loss: 0.03167856112122536|Training Accuracy : 0.96875\n",
            "Batch : 1811|Training Loss: 0.07995343208312988|Training Accuracy : 1.0\n",
            "Batch : 1812|Training Loss: 0.07404565066099167|Training Accuracy : 0.96875\n",
            "Batch : 1813|Training Loss: 0.12149681150913239|Training Accuracy : 0.90625\n",
            "Batch : 1814|Training Loss: 0.08295808732509613|Training Accuracy : 0.96875\n",
            "Batch : 1815|Training Loss: 0.4658782482147217|Training Accuracy : 0.875\n",
            "Batch : 1816|Training Loss: 0.20432499051094055|Training Accuracy : 0.9375\n",
            "Batch : 1817|Training Loss: 0.23489132523536682|Training Accuracy : 0.9375\n",
            "Batch : 1818|Training Loss: 0.08356174826622009|Training Accuracy : 0.96875\n",
            "Batch : 1819|Training Loss: 0.1375754475593567|Training Accuracy : 0.96875\n",
            "Batch : 1820|Training Loss: 0.030985292047262192|Training Accuracy : 1.0\n",
            "Batch : 1821|Training Loss: 0.12109138071537018|Training Accuracy : 0.96875\n",
            "Batch : 1822|Training Loss: 0.033493682742118835|Training Accuracy : 1.0\n",
            "Batch : 1823|Training Loss: 0.09456060826778412|Training Accuracy : 0.96875\n",
            "Batch : 1824|Training Loss: 0.1393151879310608|Training Accuracy : 0.96875\n",
            "Batch : 1825|Training Loss: 0.23731841146945953|Training Accuracy : 0.90625\n",
            "Batch : 1826|Training Loss: 0.14159005880355835|Training Accuracy : 0.9375\n",
            "Batch : 1827|Training Loss: 0.2359810769557953|Training Accuracy : 0.90625\n",
            "Batch : 1828|Training Loss: 0.05133998021483421|Training Accuracy : 0.96875\n",
            "Batch : 1829|Training Loss: 0.35189977288246155|Training Accuracy : 0.8125\n",
            "Batch : 1830|Training Loss: 0.07113804668188095|Training Accuracy : 0.96875\n",
            "Batch : 1831|Training Loss: 0.2198977917432785|Training Accuracy : 0.90625\n",
            "Batch : 1832|Training Loss: 0.13144421577453613|Training Accuracy : 0.9375\n",
            "Batch : 1833|Training Loss: 0.47162216901779175|Training Accuracy : 0.8125\n",
            "Batch : 1834|Training Loss: 0.3305097818374634|Training Accuracy : 0.875\n",
            "Batch : 1835|Training Loss: 0.2871047258377075|Training Accuracy : 0.875\n",
            "Batch : 1836|Training Loss: 0.2659298777580261|Training Accuracy : 0.90625\n",
            "Batch : 1837|Training Loss: 0.32400012016296387|Training Accuracy : 0.84375\n",
            "Batch : 1838|Training Loss: 0.025336861610412598|Training Accuracy : 1.0\n",
            "Batch : 1839|Training Loss: 0.24974066019058228|Training Accuracy : 0.875\n",
            "Batch : 1840|Training Loss: 0.17730946838855743|Training Accuracy : 0.96875\n",
            "Batch : 1841|Training Loss: 0.14178737998008728|Training Accuracy : 0.96875\n",
            "Batch : 1842|Training Loss: 0.17530302703380585|Training Accuracy : 0.90625\n",
            "Batch : 1843|Training Loss: 0.08257555961608887|Training Accuracy : 0.96875\n",
            "Batch : 1844|Training Loss: 0.1781625896692276|Training Accuracy : 0.96875\n",
            "Batch : 1845|Training Loss: 0.132362499833107|Training Accuracy : 0.9375\n",
            "Batch : 1846|Training Loss: 0.05558393895626068|Training Accuracy : 1.0\n",
            "Batch : 1847|Training Loss: 0.1373986154794693|Training Accuracy : 0.96875\n",
            "Batch : 1848|Training Loss: 0.2537092864513397|Training Accuracy : 0.9375\n",
            "Batch : 1849|Training Loss: 0.06460810452699661|Training Accuracy : 1.0\n",
            "Batch : 1850|Training Loss: 0.26436498761177063|Training Accuracy : 0.9375\n",
            "Batch : 1851|Training Loss: 0.1662614345550537|Training Accuracy : 0.90625\n",
            "Batch : 1852|Training Loss: 0.0935908704996109|Training Accuracy : 0.96875\n",
            "Batch : 1853|Training Loss: 0.17164677381515503|Training Accuracy : 0.9375\n",
            "Batch : 1854|Training Loss: 0.1694088578224182|Training Accuracy : 0.9375\n",
            "Batch : 1855|Training Loss: 0.21367622911930084|Training Accuracy : 0.875\n",
            "Batch : 1856|Training Loss: 0.13392500579357147|Training Accuracy : 0.96875\n",
            "Batch : 1857|Training Loss: 0.15278324484825134|Training Accuracy : 0.9375\n",
            "Batch : 1858|Training Loss: 0.1581582874059677|Training Accuracy : 0.96875\n",
            "Batch : 1859|Training Loss: 0.25859150290489197|Training Accuracy : 0.875\n",
            "Batch : 1860|Training Loss: 0.26780474185943604|Training Accuracy : 0.875\n",
            "Batch : 1861|Training Loss: 0.2568020820617676|Training Accuracy : 0.9375\n",
            "Batch : 1862|Training Loss: 0.09145016968250275|Training Accuracy : 0.9375\n",
            "Batch : 1863|Training Loss: 0.13159474730491638|Training Accuracy : 0.9375\n",
            "Batch : 1864|Training Loss: 0.173151433467865|Training Accuracy : 0.9375\n",
            "Batch : 1865|Training Loss: 0.15466749668121338|Training Accuracy : 0.96875\n",
            "Batch : 1866|Training Loss: 0.10307608544826508|Training Accuracy : 1.0\n",
            "Batch : 1867|Training Loss: 0.2026185691356659|Training Accuracy : 0.96875\n",
            "Batch : 1868|Training Loss: 0.01944064535200596|Training Accuracy : 1.0\n",
            "Batch : 1869|Training Loss: 0.30007293820381165|Training Accuracy : 0.875\n",
            "Batch : 1870|Training Loss: 0.24260643124580383|Training Accuracy : 0.9375\n",
            "Batch : 1871|Training Loss: 0.13107338547706604|Training Accuracy : 0.9375\n",
            "Batch : 1872|Training Loss: 0.31576335430145264|Training Accuracy : 0.90625\n",
            "Batch : 1873|Training Loss: 0.14093095064163208|Training Accuracy : 0.9375\n",
            "Batch : 1874|Training Loss: 0.22854609787464142|Training Accuracy : 0.90625\n",
            "Test Loss: 0.27333900332450867|Test Accuracy : 0.90625\n",
            "Test Loss: 1.0264766216278076|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3592176139354706|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5333399772644043|Test Accuracy : 0.875\n",
            "Test Loss: 0.6407773494720459|Test Accuracy : 0.9375\n",
            "Test Loss: 0.1634172648191452|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5933047533035278|Test Accuracy : 0.78125\n",
            "Test Loss: 0.1871901899576187|Test Accuracy : 0.90625\n",
            "Test Loss: 0.18865753710269928|Test Accuracy : 0.875\n",
            "Test Loss: 1.1764616966247559|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4707486629486084|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5011242032051086|Test Accuracy : 0.875\n",
            "Test Loss: 0.44539639353752136|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4002498388290405|Test Accuracy : 0.84375\n",
            "Test Loss: 1.1639134883880615|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5426234602928162|Test Accuracy : 0.8125\n",
            "Test Loss: 0.421173095703125|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4028633236885071|Test Accuracy : 0.90625\n",
            "Test Loss: 0.09873296320438385|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6176690459251404|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3492572605609894|Test Accuracy : 0.8125\n",
            "Test Loss: 1.0096790790557861|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3272988498210907|Test Accuracy : 0.90625\n",
            "Test Loss: 0.39998671412467957|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6447722911834717|Test Accuracy : 0.78125\n",
            "Test Loss: 0.14313893020153046|Test Accuracy : 0.96875\n",
            "Test Loss: 0.7674422860145569|Test Accuracy : 0.78125\n",
            "Test Loss: 1.146101713180542|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2781164348125458|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5795990228652954|Test Accuracy : 0.78125\n",
            "Test Loss: 0.7578484416007996|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3192017376422882|Test Accuracy : 0.875\n",
            "Test Loss: 0.20277732610702515|Test Accuracy : 0.96875\n",
            "Test Loss: 0.23310375213623047|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6887285113334656|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3963339924812317|Test Accuracy : 0.875\n",
            "Test Loss: 0.22584769129753113|Test Accuracy : 0.96875\n",
            "Test Loss: 0.5126463174819946|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4990752935409546|Test Accuracy : 0.90625\n",
            "Test Loss: 1.0568550825119019|Test Accuracy : 0.8125\n",
            "Test Loss: 0.7507237195968628|Test Accuracy : 0.71875\n",
            "Test Loss: 0.7691962718963623|Test Accuracy : 0.78125\n",
            "Test Loss: 0.024150755256414413|Test Accuracy : 1.0\n",
            "Test Loss: 0.46197229623794556|Test Accuracy : 0.84375\n",
            "Test Loss: 0.17901843786239624|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4355488419532776|Test Accuracy : 0.875\n",
            "Test Loss: 0.5324708223342896|Test Accuracy : 0.875\n",
            "Test Loss: 0.6197136640548706|Test Accuracy : 0.84375\n",
            "Test Loss: 0.23235373198986053|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5302906036376953|Test Accuracy : 0.875\n",
            "Test Loss: 0.2924482226371765|Test Accuracy : 0.9375\n",
            "Test Loss: 0.2634223997592926|Test Accuracy : 0.90625\n",
            "Test Loss: 0.23769687116146088|Test Accuracy : 0.9375\n",
            "Test Loss: 0.48356783390045166|Test Accuracy : 0.875\n",
            "Test Loss: 0.483638197183609|Test Accuracy : 0.84375\n",
            "Test Loss: 1.1169716119766235|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2728971242904663|Test Accuracy : 0.90625\n",
            "Test Loss: 0.47890904545783997|Test Accuracy : 0.78125\n",
            "Test Loss: 0.49957942962646484|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3860650360584259|Test Accuracy : 0.90625\n",
            "Test Loss: 0.14627961814403534|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6155088543891907|Test Accuracy : 0.78125\n",
            "Test Loss: 0.9319039583206177|Test Accuracy : 0.71875\n",
            "Test Loss: 0.42801666259765625|Test Accuracy : 0.875\n",
            "Test Loss: 0.24833878874778748|Test Accuracy : 0.875\n",
            "Test Loss: 0.4022763967514038|Test Accuracy : 0.875\n",
            "Test Loss: 0.18722133338451385|Test Accuracy : 0.90625\n",
            "Test Loss: 0.21882662177085876|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5269210934638977|Test Accuracy : 0.8125\n",
            "Test Loss: 0.30681461095809937|Test Accuracy : 0.90625\n",
            "Test Loss: 0.32234010100364685|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5073955655097961|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3289550244808197|Test Accuracy : 0.90625\n",
            "Test Loss: 0.40659794211387634|Test Accuracy : 0.90625\n",
            "Test Loss: 0.9975675344467163|Test Accuracy : 0.875\n",
            "Test Loss: 0.40461257100105286|Test Accuracy : 0.96875\n",
            "Test Loss: 0.4448583722114563|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5349162220954895|Test Accuracy : 0.84375\n",
            "Test Loss: 0.8695493936538696|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6158004403114319|Test Accuracy : 0.84375\n",
            "Test Loss: 0.18032723665237427|Test Accuracy : 0.90625\n",
            "Test Loss: 0.45178306102752686|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6500949263572693|Test Accuracy : 0.90625\n",
            "Test Loss: 0.267949640750885|Test Accuracy : 0.875\n",
            "Test Loss: 0.22541940212249756|Test Accuracy : 0.90625\n",
            "Test Loss: 0.10820744186639786|Test Accuracy : 0.9375\n",
            "Test Loss: 0.40381571650505066|Test Accuracy : 0.875\n",
            "Test Loss: 0.3433016538619995|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5361232757568359|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2138073891401291|Test Accuracy : 0.9375\n",
            "Test Loss: 0.18447571992874146|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5582876205444336|Test Accuracy : 0.8125\n",
            "Test Loss: 1.3657009601593018|Test Accuracy : 0.84375\n",
            "Test Loss: 0.40422070026397705|Test Accuracy : 0.875\n",
            "Test Loss: 0.25282806158065796|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6532741785049438|Test Accuracy : 0.8125\n",
            "Test Loss: 0.7320751547813416|Test Accuracy : 0.875\n",
            "Test Loss: 0.6216152906417847|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6284589171409607|Test Accuracy : 0.84375\n",
            "Test Loss: 0.18984976410865784|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2713991403579712|Test Accuracy : 0.9375\n",
            "Test Loss: 0.12663784623146057|Test Accuracy : 0.9375\n",
            "Test Loss: 0.24742087721824646|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4268667697906494|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5798285603523254|Test Accuracy : 0.78125\n",
            "Test Loss: 0.16755235195159912|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3059213161468506|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3541666567325592|Test Accuracy : 0.84375\n",
            "Test Loss: 0.41743457317352295|Test Accuracy : 0.75\n",
            "Test Loss: 0.10656294971704483|Test Accuracy : 0.96875\n",
            "Test Loss: 0.26498839259147644|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3566237688064575|Test Accuracy : 0.875\n",
            "Test Loss: 0.4165510833263397|Test Accuracy : 0.90625\n",
            "Test Loss: 0.9578648209571838|Test Accuracy : 0.65625\n",
            "Test Loss: 1.225327968597412|Test Accuracy : 0.84375\n",
            "Test Loss: 0.8224955201148987|Test Accuracy : 0.84375\n",
            "Test Loss: 0.17352306842803955|Test Accuracy : 0.9375\n",
            "Test Loss: 0.30253931879997253|Test Accuracy : 0.9375\n",
            "Test Loss: 0.7637401819229126|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3389259874820709|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4328160285949707|Test Accuracy : 0.84375\n",
            "Test Loss: 0.44137507677078247|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5063469409942627|Test Accuracy : 0.875\n",
            "Test Loss: 0.21048909425735474|Test Accuracy : 0.9375\n",
            "Test Loss: 0.25897151231765747|Test Accuracy : 0.875\n",
            "Test Loss: 0.35314592719078064|Test Accuracy : 0.875\n",
            "Test Loss: 0.5869016647338867|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6251112222671509|Test Accuracy : 0.9375\n",
            "Test Loss: 1.1640623807907104|Test Accuracy : 0.84375\n",
            "Test Loss: 0.9865497350692749|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5514832139015198|Test Accuracy : 0.84375\n",
            "Test Loss: 0.07530776411294937|Test Accuracy : 0.96875\n",
            "Test Loss: 1.19218909740448|Test Accuracy : 0.78125\n",
            "Test Loss: 1.9629740715026855|Test Accuracy : 0.71875\n",
            "Test Loss: 0.5966969132423401|Test Accuracy : 0.8125\n",
            "Test Loss: 0.2505679428577423|Test Accuracy : 0.9375\n",
            "Test Loss: 0.708094596862793|Test Accuracy : 0.84375\n",
            "Test Loss: 0.13071241974830627|Test Accuracy : 0.9375\n",
            "Test Loss: 0.43693530559539795|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3430515229701996|Test Accuracy : 0.875\n",
            "Test Loss: 1.017869472503662|Test Accuracy : 0.8125\n",
            "Test Loss: 1.4700547456741333|Test Accuracy : 0.78125\n",
            "Test Loss: 0.6025770902633667|Test Accuracy : 0.875\n",
            "Test Loss: 0.3501279056072235|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3995080888271332|Test Accuracy : 0.875\n",
            "Test Loss: 0.5119284987449646|Test Accuracy : 0.875\n",
            "Test Loss: 0.2571558356285095|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3351929783821106|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5104148387908936|Test Accuracy : 0.8125\n",
            "Test Loss: 0.47829777002334595|Test Accuracy : 0.875\n",
            "Test Loss: 0.40667739510536194|Test Accuracy : 0.9375\n",
            "Test Loss: 0.46694043278694153|Test Accuracy : 0.8125\n",
            "Test Loss: 0.7242666482925415|Test Accuracy : 0.84375\n",
            "Test Loss: 0.31801217794418335|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4648110270500183|Test Accuracy : 0.78125\n",
            "Test Loss: 0.09483111649751663|Test Accuracy : 0.96875\n",
            "Test Loss: 0.8881258964538574|Test Accuracy : 0.78125\n",
            "Test Loss: 0.1392643302679062|Test Accuracy : 0.9375\n",
            "Test Loss: 0.28642594814300537|Test Accuracy : 0.8125\n",
            "Test Loss: 0.9530697464942932|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3817507028579712|Test Accuracy : 0.90625\n",
            "Test Loss: 0.22155946493148804|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4095444679260254|Test Accuracy : 0.875\n",
            "Test Loss: 0.9229636788368225|Test Accuracy : 0.90625\n",
            "Test Loss: 0.36466914415359497|Test Accuracy : 0.90625\n",
            "Test Loss: 0.475563645362854|Test Accuracy : 0.78125\n",
            "Test Loss: 0.327097088098526|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7874224781990051|Test Accuracy : 0.75\n",
            "Test Loss: 0.5612701773643494|Test Accuracy : 0.75\n",
            "Test Loss: 0.5909910798072815|Test Accuracy : 0.875\n",
            "Test Loss: 0.8011730313301086|Test Accuracy : 0.78125\n",
            "Test Loss: 0.44440096616744995|Test Accuracy : 0.875\n",
            "Test Loss: 1.2930006980895996|Test Accuracy : 0.78125\n",
            "Test Loss: 0.27025115489959717|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5150201320648193|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2828322649002075|Test Accuracy : 0.9375\n",
            "Test Loss: 2.7995376586914062|Test Accuracy : 0.78125\n",
            "Test Loss: 0.7804205417633057|Test Accuracy : 0.875\n",
            "Test Loss: 0.33601516485214233|Test Accuracy : 0.875\n",
            "Test Loss: 0.4594314694404602|Test Accuracy : 0.875\n",
            "Test Loss: 0.1907135397195816|Test Accuracy : 0.90625\n",
            "Test Loss: 0.16862675547599792|Test Accuracy : 0.9375\n",
            "Test Loss: 1.2114537954330444|Test Accuracy : 0.78125\n",
            "Test Loss: 0.46791985630989075|Test Accuracy : 0.875\n",
            "Test Loss: 0.17182515561580658|Test Accuracy : 0.96875\n",
            "Test Loss: 0.7496791481971741|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5411224365234375|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3335530161857605|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6404654383659363|Test Accuracy : 0.8125\n",
            "Test Loss: 0.39565202593803406|Test Accuracy : 0.8125\n",
            "Test Loss: 0.31706205010414124|Test Accuracy : 0.875\n",
            "Test Loss: 0.5435044169425964|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4439319372177124|Test Accuracy : 0.875\n",
            "Test Loss: 0.28916195034980774|Test Accuracy : 0.90625\n",
            "Test Loss: 0.684922456741333|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2681047022342682|Test Accuracy : 0.90625\n",
            "Test Loss: 0.16254250705242157|Test Accuracy : 0.9375\n",
            "Test Loss: 0.25623297691345215|Test Accuracy : 0.90625\n",
            "Test Loss: 0.28018802404403687|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5108934044837952|Test Accuracy : 0.9375\n",
            "Test Loss: 0.33588847517967224|Test Accuracy : 0.875\n",
            "Test Loss: 0.4944186508655548|Test Accuracy : 0.84375\n",
            "Test Loss: 0.2716728448867798|Test Accuracy : 0.90625\n",
            "Test Loss: 0.8658466339111328|Test Accuracy : 0.75\n",
            "Test Loss: 0.6479018330574036|Test Accuracy : 0.90625\n",
            "Test Loss: 0.16217711567878723|Test Accuracy : 0.96875\n",
            "Test Loss: 0.5047522187232971|Test Accuracy : 0.875\n",
            "Test Loss: 0.2967034578323364|Test Accuracy : 0.84375\n",
            "Test Loss: 0.17713706195354462|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7640787363052368|Test Accuracy : 0.8125\n",
            "Test Loss: 0.7073536515235901|Test Accuracy : 0.875\n",
            "Test Loss: 0.1136350929737091|Test Accuracy : 0.96875\n",
            "Test Loss: 0.11817032098770142|Test Accuracy : 0.96875\n",
            "Test Loss: 0.6182474493980408|Test Accuracy : 0.71875\n",
            "Test Loss: 0.31586408615112305|Test Accuracy : 0.90625\n",
            "Test Loss: 0.33288323879241943|Test Accuracy : 0.9375\n",
            "Test Loss: 0.8016650080680847|Test Accuracy : 0.875\n",
            "Test Loss: 0.20741276443004608|Test Accuracy : 0.875\n",
            "Test Loss: 0.6471836566925049|Test Accuracy : 0.78125\n",
            "Test Loss: 0.6770537495613098|Test Accuracy : 0.90625\n",
            "Test Loss: 0.517228901386261|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3831314444541931|Test Accuracy : 0.875\n",
            "Test Loss: 0.879613995552063|Test Accuracy : 0.875\n",
            "Test Loss: 0.7738320827484131|Test Accuracy : 0.75\n",
            "Test Loss: 0.5352440476417542|Test Accuracy : 0.90625\n",
            "Test Loss: 0.20055533945560455|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3565810024738312|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3690611720085144|Test Accuracy : 0.84375\n",
            "Test Loss: 0.0617380291223526|Test Accuracy : 1.0\n",
            "Test Loss: 0.44139766693115234|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4844120740890503|Test Accuracy : 0.875\n",
            "Test Loss: 0.7304450273513794|Test Accuracy : 0.78125\n",
            "Test Loss: 0.27053186297416687|Test Accuracy : 0.90625\n",
            "Test Loss: 0.18859590590000153|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5020802021026611|Test Accuracy : 0.875\n",
            "Test Loss: 0.09733085334300995|Test Accuracy : 0.9375\n",
            "Test Loss: 0.20507720112800598|Test Accuracy : 0.9375\n",
            "Test Loss: 0.543831467628479|Test Accuracy : 0.8125\n",
            "Test Loss: 0.15300115942955017|Test Accuracy : 0.96875\n",
            "Test Loss: 0.16396869719028473|Test Accuracy : 0.875\n",
            "Test Loss: 1.1315252780914307|Test Accuracy : 0.78125\n",
            "Test Loss: 0.8165904879570007|Test Accuracy : 0.75\n",
            "Test Loss: 0.16226240992546082|Test Accuracy : 0.90625\n",
            "Test Loss: 0.49272292852401733|Test Accuracy : 0.875\n",
            "Test Loss: 0.739591658115387|Test Accuracy : 0.875\n",
            "Test Loss: 0.34462976455688477|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2235645353794098|Test Accuracy : 0.9375\n",
            "Test Loss: 0.29448890686035156|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6505408883094788|Test Accuracy : 0.875\n",
            "Test Loss: 0.45451709628105164|Test Accuracy : 0.9375\n",
            "Test Loss: 0.56407231092453|Test Accuracy : 0.875\n",
            "Test Loss: 0.45685890316963196|Test Accuracy : 0.875\n",
            "Test Loss: 0.20299609005451202|Test Accuracy : 0.90625\n",
            "Test Loss: 0.8113301396369934|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3810001015663147|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6107349395751953|Test Accuracy : 0.75\n",
            "Test Loss: 0.18863129615783691|Test Accuracy : 0.90625\n",
            "Test Loss: 0.20454859733581543|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6083964109420776|Test Accuracy : 0.78125\n",
            "Test Loss: 0.9223952889442444|Test Accuracy : 0.71875\n",
            "Test Loss: 0.5221718549728394|Test Accuracy : 0.875\n",
            "Test Loss: 1.4070022106170654|Test Accuracy : 0.75\n",
            "Test Loss: 0.10605216771364212|Test Accuracy : 0.96875\n",
            "Test Loss: 0.27668851613998413|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6778188943862915|Test Accuracy : 0.84375\n",
            "Test Loss: 0.543044924736023|Test Accuracy : 0.8125\n",
            "Test Loss: 0.9309483766555786|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3538552224636078|Test Accuracy : 0.84375\n",
            "Test Loss: 0.47874197363853455|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4226519465446472|Test Accuracy : 0.9375\n",
            "Test Loss: 0.32828232645988464|Test Accuracy : 0.875\n",
            "Test Loss: 0.38852089643478394|Test Accuracy : 0.875\n",
            "Test Loss: 0.272817999124527|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3629415035247803|Test Accuracy : 0.84375\n",
            "Test Loss: 0.8537548184394836|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5194734930992126|Test Accuracy : 0.84375\n",
            "Test Loss: 0.16222937405109406|Test Accuracy : 0.96875\n",
            "Test Loss: 0.4287751317024231|Test Accuracy : 0.9375\n",
            "Test Loss: 0.9952830076217651|Test Accuracy : 0.84375\n",
            "Test Loss: 0.32532188296318054|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4395311772823334|Test Accuracy : 0.90625\n",
            "Test Loss: 3.3633947372436523|Test Accuracy : 0.8125\n",
            "Test Loss: 0.13179324567317963|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6618645191192627|Test Accuracy : 0.875\n",
            "Test Loss: 0.39860764145851135|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5779559016227722|Test Accuracy : 0.8125\n",
            "Test Loss: 0.45110899209976196|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3278183341026306|Test Accuracy : 0.84375\n",
            "Test Loss: 0.17791874706745148|Test Accuracy : 0.96875\n",
            "Test Loss: 1.0616382360458374|Test Accuracy : 0.71875\n",
            "Test Loss: 0.19204184412956238|Test Accuracy : 0.9375\n",
            "Test Loss: 0.37533894181251526|Test Accuracy : 0.90625\n",
            "Test Loss: 1.5289807319641113|Test Accuracy : 0.875\n",
            "Test Loss: 0.34783533215522766|Test Accuracy : 0.9375\n",
            "Test Loss: 0.745181679725647|Test Accuracy : 0.78125\n",
            "Test Loss: 0.36643165349960327|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7141656875610352|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3676183521747589|Test Accuracy : 0.84375\n",
            "Test Loss: 1.0466265678405762|Test Accuracy : 0.71875\n",
            "Test Loss: 0.39367473125457764|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3695322573184967|Test Accuracy : 0.90625\n",
            "Test Loss: 0.10719732940196991|Test Accuracy : 0.96875\n",
            "Test Loss: 0.7307455539703369|Test Accuracy : 0.8125\n",
            "Test Loss: 0.15174482762813568|Test Accuracy : 0.90625\n",
            "Test Loss: 0.38876089453697205|Test Accuracy : 0.875\n",
            "Test Loss: 0.14079533517360687|Test Accuracy : 0.90625\n",
            "Test Loss: 0.41796764731407166|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7236760258674622|Test Accuracy : 0.75\n",
            "Test Loss: 0.3957647383213043|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3215293884277344|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6172542572021484|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3829308748245239|Test Accuracy : 0.84375\n",
            "Test Loss: 1.003815770149231|Test Accuracy : 0.8125\n",
            "Epoch :99\n",
            "Batch : 0|Training Loss: 0.23791030049324036|Training Accuracy : 0.875\n",
            "Batch : 1|Training Loss: 0.04566863551735878|Training Accuracy : 1.0\n",
            "Batch : 2|Training Loss: 0.04190273955464363|Training Accuracy : 1.0\n",
            "Batch : 3|Training Loss: 0.13442093133926392|Training Accuracy : 0.96875\n",
            "Batch : 4|Training Loss: 0.10983313620090485|Training Accuracy : 0.9375\n",
            "Batch : 5|Training Loss: 0.13041062653064728|Training Accuracy : 0.9375\n",
            "Batch : 6|Training Loss: 0.21094363927841187|Training Accuracy : 0.9375\n",
            "Batch : 7|Training Loss: 0.12827973067760468|Training Accuracy : 0.9375\n",
            "Batch : 8|Training Loss: 0.15402397513389587|Training Accuracy : 0.90625\n",
            "Batch : 9|Training Loss: 0.38173672556877136|Training Accuracy : 0.875\n",
            "Batch : 10|Training Loss: 0.17201343178749084|Training Accuracy : 0.9375\n",
            "Batch : 11|Training Loss: 0.17947079241275787|Training Accuracy : 0.9375\n",
            "Batch : 12|Training Loss: 0.341048926115036|Training Accuracy : 0.90625\n",
            "Batch : 13|Training Loss: 0.1737445890903473|Training Accuracy : 0.90625\n",
            "Batch : 14|Training Loss: 0.23351705074310303|Training Accuracy : 0.9375\n",
            "Batch : 15|Training Loss: 0.10711415857076645|Training Accuracy : 0.96875\n",
            "Batch : 16|Training Loss: 0.28795039653778076|Training Accuracy : 0.90625\n",
            "Batch : 17|Training Loss: 0.2085084617137909|Training Accuracy : 0.9375\n",
            "Batch : 18|Training Loss: 0.2699727714061737|Training Accuracy : 0.9375\n",
            "Batch : 19|Training Loss: 0.13434627652168274|Training Accuracy : 0.96875\n",
            "Batch : 20|Training Loss: 0.16949158906936646|Training Accuracy : 0.96875\n",
            "Batch : 21|Training Loss: 0.05979634076356888|Training Accuracy : 0.96875\n",
            "Batch : 22|Training Loss: 0.3067415654659271|Training Accuracy : 0.90625\n",
            "Batch : 23|Training Loss: 0.15318921208381653|Training Accuracy : 0.9375\n",
            "Batch : 24|Training Loss: 0.03736836463212967|Training Accuracy : 1.0\n",
            "Batch : 25|Training Loss: 0.2912488579750061|Training Accuracy : 0.90625\n",
            "Batch : 26|Training Loss: 0.11210068315267563|Training Accuracy : 0.96875\n",
            "Batch : 27|Training Loss: 0.10260994732379913|Training Accuracy : 0.96875\n",
            "Batch : 28|Training Loss: 0.11805227398872375|Training Accuracy : 0.9375\n",
            "Batch : 29|Training Loss: 0.22175677120685577|Training Accuracy : 0.84375\n",
            "Batch : 30|Training Loss: 0.05966831371188164|Training Accuracy : 0.96875\n",
            "Batch : 31|Training Loss: 0.2070157825946808|Training Accuracy : 0.90625\n",
            "Batch : 32|Training Loss: 0.10249412804841995|Training Accuracy : 0.96875\n",
            "Batch : 33|Training Loss: 0.07614210993051529|Training Accuracy : 0.96875\n",
            "Batch : 34|Training Loss: 0.10578210651874542|Training Accuracy : 0.9375\n",
            "Batch : 35|Training Loss: 0.167684406042099|Training Accuracy : 0.90625\n",
            "Batch : 36|Training Loss: 0.15910665690898895|Training Accuracy : 0.90625\n",
            "Batch : 37|Training Loss: 0.2596129775047302|Training Accuracy : 0.875\n",
            "Batch : 38|Training Loss: 0.09798364341259003|Training Accuracy : 0.96875\n",
            "Batch : 39|Training Loss: 0.1154666393995285|Training Accuracy : 0.9375\n",
            "Batch : 40|Training Loss: 0.11488202214241028|Training Accuracy : 0.96875\n",
            "Batch : 41|Training Loss: 0.07853122055530548|Training Accuracy : 0.96875\n",
            "Batch : 42|Training Loss: 0.10481251031160355|Training Accuracy : 0.96875\n",
            "Batch : 43|Training Loss: 0.03348850458860397|Training Accuracy : 0.96875\n",
            "Batch : 44|Training Loss: 0.12694694101810455|Training Accuracy : 0.96875\n",
            "Batch : 45|Training Loss: 0.1952909678220749|Training Accuracy : 0.90625\n",
            "Batch : 46|Training Loss: 0.15590432286262512|Training Accuracy : 0.9375\n",
            "Batch : 47|Training Loss: 0.06996473670005798|Training Accuracy : 0.96875\n",
            "Batch : 48|Training Loss: 0.07694391161203384|Training Accuracy : 0.96875\n",
            "Batch : 49|Training Loss: 0.3135110139846802|Training Accuracy : 0.9375\n",
            "Batch : 50|Training Loss: 0.07240057736635208|Training Accuracy : 1.0\n",
            "Batch : 51|Training Loss: 0.4332984685897827|Training Accuracy : 0.90625\n",
            "Batch : 52|Training Loss: 0.11588403582572937|Training Accuracy : 0.9375\n",
            "Batch : 53|Training Loss: 0.09328429400920868|Training Accuracy : 0.96875\n",
            "Batch : 54|Training Loss: 0.3216940462589264|Training Accuracy : 0.8125\n",
            "Batch : 55|Training Loss: 0.18063503503799438|Training Accuracy : 0.90625\n",
            "Batch : 56|Training Loss: 0.2281259447336197|Training Accuracy : 0.9375\n",
            "Batch : 57|Training Loss: 0.12358798086643219|Training Accuracy : 0.96875\n",
            "Batch : 58|Training Loss: 0.17981407046318054|Training Accuracy : 0.9375\n",
            "Batch : 59|Training Loss: 0.09704079478979111|Training Accuracy : 0.9375\n",
            "Batch : 60|Training Loss: 0.400054395198822|Training Accuracy : 0.875\n",
            "Batch : 61|Training Loss: 0.12054955214262009|Training Accuracy : 0.96875\n",
            "Batch : 62|Training Loss: 0.07980795204639435|Training Accuracy : 1.0\n",
            "Batch : 63|Training Loss: 0.1396581083536148|Training Accuracy : 0.96875\n",
            "Batch : 64|Training Loss: 0.19893355667591095|Training Accuracy : 0.9375\n",
            "Batch : 65|Training Loss: 0.20701995491981506|Training Accuracy : 0.9375\n",
            "Batch : 66|Training Loss: 0.039778932929039|Training Accuracy : 1.0\n",
            "Batch : 67|Training Loss: 0.16053050756454468|Training Accuracy : 0.90625\n",
            "Batch : 68|Training Loss: 0.20596863329410553|Training Accuracy : 0.875\n",
            "Batch : 69|Training Loss: 0.24844315648078918|Training Accuracy : 0.90625\n",
            "Batch : 70|Training Loss: 0.12741732597351074|Training Accuracy : 0.96875\n",
            "Batch : 71|Training Loss: 0.2756556272506714|Training Accuracy : 0.8125\n",
            "Batch : 72|Training Loss: 0.26735296845436096|Training Accuracy : 0.84375\n",
            "Batch : 73|Training Loss: 0.09066519141197205|Training Accuracy : 0.96875\n",
            "Batch : 74|Training Loss: 0.36758917570114136|Training Accuracy : 0.9375\n",
            "Batch : 75|Training Loss: 0.13307422399520874|Training Accuracy : 0.96875\n",
            "Batch : 76|Training Loss: 0.15464475750923157|Training Accuracy : 0.9375\n",
            "Batch : 77|Training Loss: 0.2633388042449951|Training Accuracy : 0.90625\n",
            "Batch : 78|Training Loss: 0.12541496753692627|Training Accuracy : 0.96875\n",
            "Batch : 79|Training Loss: 0.20378196239471436|Training Accuracy : 0.90625\n",
            "Batch : 80|Training Loss: 0.09996768087148666|Training Accuracy : 0.9375\n",
            "Batch : 81|Training Loss: 0.30063918232917786|Training Accuracy : 0.875\n",
            "Batch : 82|Training Loss: 0.1867903172969818|Training Accuracy : 0.90625\n",
            "Batch : 83|Training Loss: 0.12358275055885315|Training Accuracy : 0.9375\n",
            "Batch : 84|Training Loss: 0.29588982462882996|Training Accuracy : 0.96875\n",
            "Batch : 85|Training Loss: 0.09782752394676208|Training Accuracy : 0.96875\n",
            "Batch : 86|Training Loss: 0.07091981917619705|Training Accuracy : 1.0\n",
            "Batch : 87|Training Loss: 0.043048303574323654|Training Accuracy : 1.0\n",
            "Batch : 88|Training Loss: 0.1133354902267456|Training Accuracy : 0.96875\n",
            "Batch : 89|Training Loss: 0.4891566038131714|Training Accuracy : 0.9375\n",
            "Batch : 90|Training Loss: 0.20962508022785187|Training Accuracy : 0.96875\n",
            "Batch : 91|Training Loss: 0.07703140377998352|Training Accuracy : 0.96875\n",
            "Batch : 92|Training Loss: 0.3514271080493927|Training Accuracy : 0.84375\n",
            "Batch : 93|Training Loss: 0.12999941408634186|Training Accuracy : 0.96875\n",
            "Batch : 94|Training Loss: 0.05622564256191254|Training Accuracy : 1.0\n",
            "Batch : 95|Training Loss: 0.08153318613767624|Training Accuracy : 1.0\n",
            "Batch : 96|Training Loss: 0.2855415940284729|Training Accuracy : 0.875\n",
            "Batch : 97|Training Loss: 0.22644475102424622|Training Accuracy : 0.90625\n",
            "Batch : 98|Training Loss: 0.03875283524394035|Training Accuracy : 1.0\n",
            "Batch : 99|Training Loss: 0.14247113466262817|Training Accuracy : 0.9375\n",
            "Batch : 100|Training Loss: 0.8628615736961365|Training Accuracy : 0.90625\n",
            "Batch : 101|Training Loss: 0.326788067817688|Training Accuracy : 0.90625\n",
            "Batch : 102|Training Loss: 0.3096849024295807|Training Accuracy : 0.90625\n",
            "Batch : 103|Training Loss: 0.0725945457816124|Training Accuracy : 0.96875\n",
            "Batch : 104|Training Loss: 0.43739673495292664|Training Accuracy : 0.90625\n",
            "Batch : 105|Training Loss: 0.03657064959406853|Training Accuracy : 1.0\n",
            "Batch : 106|Training Loss: 0.12109756469726562|Training Accuracy : 0.96875\n",
            "Batch : 107|Training Loss: 0.09989508986473083|Training Accuracy : 0.96875\n",
            "Batch : 108|Training Loss: 0.25924038887023926|Training Accuracy : 0.875\n",
            "Batch : 109|Training Loss: 0.19626817107200623|Training Accuracy : 0.90625\n",
            "Batch : 110|Training Loss: 0.0697554424405098|Training Accuracy : 0.96875\n",
            "Batch : 111|Training Loss: 0.06407268345355988|Training Accuracy : 1.0\n",
            "Batch : 112|Training Loss: 0.13348077237606049|Training Accuracy : 0.96875\n",
            "Batch : 113|Training Loss: 0.4078579545021057|Training Accuracy : 0.84375\n",
            "Batch : 114|Training Loss: 0.4079921841621399|Training Accuracy : 0.90625\n",
            "Batch : 115|Training Loss: 0.2062283605337143|Training Accuracy : 0.9375\n",
            "Batch : 116|Training Loss: 0.31118664145469666|Training Accuracy : 0.875\n",
            "Batch : 117|Training Loss: 0.31695279479026794|Training Accuracy : 0.875\n",
            "Batch : 118|Training Loss: 0.2439761906862259|Training Accuracy : 0.9375\n",
            "Batch : 119|Training Loss: 0.12455133348703384|Training Accuracy : 0.96875\n",
            "Batch : 120|Training Loss: 0.09012366086244583|Training Accuracy : 1.0\n",
            "Batch : 121|Training Loss: 0.10268550366163254|Training Accuracy : 0.96875\n",
            "Batch : 122|Training Loss: 0.10033082216978073|Training Accuracy : 0.96875\n",
            "Batch : 123|Training Loss: 0.21367989480495453|Training Accuracy : 0.875\n",
            "Batch : 124|Training Loss: 0.15580761432647705|Training Accuracy : 0.9375\n",
            "Batch : 125|Training Loss: 0.13025617599487305|Training Accuracy : 0.9375\n",
            "Batch : 126|Training Loss: 0.3059600293636322|Training Accuracy : 0.875\n",
            "Batch : 127|Training Loss: 0.09627536684274673|Training Accuracy : 0.9375\n",
            "Batch : 128|Training Loss: 0.2929758131504059|Training Accuracy : 0.875\n",
            "Batch : 129|Training Loss: 0.10774800181388855|Training Accuracy : 0.96875\n",
            "Batch : 130|Training Loss: 0.5996154546737671|Training Accuracy : 0.84375\n",
            "Batch : 131|Training Loss: 0.08612864464521408|Training Accuracy : 0.96875\n",
            "Batch : 132|Training Loss: 0.19151197373867035|Training Accuracy : 0.9375\n",
            "Batch : 133|Training Loss: 0.17158609628677368|Training Accuracy : 0.90625\n",
            "Batch : 134|Training Loss: 0.27193936705589294|Training Accuracy : 0.90625\n",
            "Batch : 135|Training Loss: 0.05990363657474518|Training Accuracy : 0.96875\n",
            "Batch : 136|Training Loss: 0.3321112394332886|Training Accuracy : 0.84375\n",
            "Batch : 137|Training Loss: 0.14108745753765106|Training Accuracy : 0.90625\n",
            "Batch : 138|Training Loss: 0.18068262934684753|Training Accuracy : 0.90625\n",
            "Batch : 139|Training Loss: 0.07891155034303665|Training Accuracy : 0.9375\n",
            "Batch : 140|Training Loss: 0.23785428702831268|Training Accuracy : 0.875\n",
            "Batch : 141|Training Loss: 0.2580214738845825|Training Accuracy : 0.9375\n",
            "Batch : 142|Training Loss: 0.31731337308883667|Training Accuracy : 0.84375\n",
            "Batch : 143|Training Loss: 0.239190474152565|Training Accuracy : 0.9375\n",
            "Batch : 144|Training Loss: 0.10162211209535599|Training Accuracy : 0.96875\n",
            "Batch : 145|Training Loss: 0.039700496941804886|Training Accuracy : 0.96875\n",
            "Batch : 146|Training Loss: 0.15929152071475983|Training Accuracy : 0.9375\n",
            "Batch : 147|Training Loss: 0.0991014838218689|Training Accuracy : 0.96875\n",
            "Batch : 148|Training Loss: 0.15184271335601807|Training Accuracy : 0.96875\n",
            "Batch : 149|Training Loss: 0.05043249577283859|Training Accuracy : 0.96875\n",
            "Batch : 150|Training Loss: 0.3477649986743927|Training Accuracy : 0.9375\n",
            "Batch : 151|Training Loss: 0.16601930558681488|Training Accuracy : 0.9375\n",
            "Batch : 152|Training Loss: 0.06400428712368011|Training Accuracy : 0.96875\n",
            "Batch : 153|Training Loss: 0.38740965723991394|Training Accuracy : 0.875\n",
            "Batch : 154|Training Loss: 0.10949355363845825|Training Accuracy : 0.9375\n",
            "Batch : 155|Training Loss: 0.12450049817562103|Training Accuracy : 0.96875\n",
            "Batch : 156|Training Loss: 0.1291947066783905|Training Accuracy : 0.96875\n",
            "Batch : 157|Training Loss: 0.18701037764549255|Training Accuracy : 0.90625\n",
            "Batch : 158|Training Loss: 0.2714073061943054|Training Accuracy : 0.875\n",
            "Batch : 159|Training Loss: 0.2960132360458374|Training Accuracy : 0.9375\n",
            "Batch : 160|Training Loss: 0.09563365578651428|Training Accuracy : 0.96875\n",
            "Batch : 161|Training Loss: 0.27431172132492065|Training Accuracy : 0.875\n",
            "Batch : 162|Training Loss: 0.0937948152422905|Training Accuracy : 0.9375\n",
            "Batch : 163|Training Loss: 0.09259463101625443|Training Accuracy : 0.96875\n",
            "Batch : 164|Training Loss: 0.4174881875514984|Training Accuracy : 0.84375\n",
            "Batch : 165|Training Loss: 0.10470805317163467|Training Accuracy : 0.9375\n",
            "Batch : 166|Training Loss: 0.19844108819961548|Training Accuracy : 0.875\n",
            "Batch : 167|Training Loss: 0.2025461196899414|Training Accuracy : 0.9375\n",
            "Batch : 168|Training Loss: 0.19849461317062378|Training Accuracy : 0.9375\n",
            "Batch : 169|Training Loss: 0.2776693105697632|Training Accuracy : 0.875\n",
            "Batch : 170|Training Loss: 0.127190962433815|Training Accuracy : 0.9375\n",
            "Batch : 171|Training Loss: 0.3763827383518219|Training Accuracy : 0.8125\n",
            "Batch : 172|Training Loss: 0.31050604581832886|Training Accuracy : 0.84375\n",
            "Batch : 173|Training Loss: 0.34845423698425293|Training Accuracy : 0.84375\n",
            "Batch : 174|Training Loss: 0.12044322490692139|Training Accuracy : 0.9375\n",
            "Batch : 175|Training Loss: 0.06580453366041183|Training Accuracy : 0.96875\n",
            "Batch : 176|Training Loss: 0.36112117767333984|Training Accuracy : 0.90625\n",
            "Batch : 177|Training Loss: 0.1856023669242859|Training Accuracy : 0.90625\n",
            "Batch : 178|Training Loss: 0.13127318024635315|Training Accuracy : 0.9375\n",
            "Batch : 179|Training Loss: 0.13479895889759064|Training Accuracy : 0.96875\n",
            "Batch : 180|Training Loss: 0.15761679410934448|Training Accuracy : 0.9375\n",
            "Batch : 181|Training Loss: 0.14704859256744385|Training Accuracy : 0.90625\n",
            "Batch : 182|Training Loss: 0.03611943498253822|Training Accuracy : 1.0\n",
            "Batch : 183|Training Loss: 0.19272717833518982|Training Accuracy : 0.90625\n",
            "Batch : 184|Training Loss: 0.14387241005897522|Training Accuracy : 0.90625\n",
            "Batch : 185|Training Loss: 0.13647447526454926|Training Accuracy : 0.9375\n",
            "Batch : 186|Training Loss: 0.1499561220407486|Training Accuracy : 0.9375\n",
            "Batch : 187|Training Loss: 0.13699635863304138|Training Accuracy : 0.96875\n",
            "Batch : 188|Training Loss: 0.26618409156799316|Training Accuracy : 0.90625\n",
            "Batch : 189|Training Loss: 0.09798594564199448|Training Accuracy : 0.96875\n",
            "Batch : 190|Training Loss: 0.17095822095870972|Training Accuracy : 0.90625\n",
            "Batch : 191|Training Loss: 0.03740176558494568|Training Accuracy : 1.0\n",
            "Batch : 192|Training Loss: 0.19445042312145233|Training Accuracy : 0.9375\n",
            "Batch : 193|Training Loss: 0.22286313772201538|Training Accuracy : 0.90625\n",
            "Batch : 194|Training Loss: 0.3557107150554657|Training Accuracy : 0.84375\n",
            "Batch : 195|Training Loss: 0.09733620285987854|Training Accuracy : 0.96875\n",
            "Batch : 196|Training Loss: 0.22555731236934662|Training Accuracy : 0.9375\n",
            "Batch : 197|Training Loss: 0.1011226549744606|Training Accuracy : 0.96875\n",
            "Batch : 198|Training Loss: 0.03938094154000282|Training Accuracy : 1.0\n",
            "Batch : 199|Training Loss: 0.1262596994638443|Training Accuracy : 0.90625\n",
            "Batch : 200|Training Loss: 0.04300788417458534|Training Accuracy : 1.0\n",
            "Batch : 201|Training Loss: 0.2767443060874939|Training Accuracy : 0.875\n",
            "Batch : 202|Training Loss: 0.29124781489372253|Training Accuracy : 0.90625\n",
            "Batch : 203|Training Loss: 0.1015891581773758|Training Accuracy : 0.96875\n",
            "Batch : 204|Training Loss: 0.20018045604228973|Training Accuracy : 0.9375\n",
            "Batch : 205|Training Loss: 0.15163512527942657|Training Accuracy : 0.9375\n",
            "Batch : 206|Training Loss: 0.14075778424739838|Training Accuracy : 0.9375\n",
            "Batch : 207|Training Loss: 0.07452698051929474|Training Accuracy : 0.96875\n",
            "Batch : 208|Training Loss: 0.16080646216869354|Training Accuracy : 0.9375\n",
            "Batch : 209|Training Loss: 0.3495369553565979|Training Accuracy : 0.875\n",
            "Batch : 210|Training Loss: 0.13643598556518555|Training Accuracy : 0.96875\n",
            "Batch : 211|Training Loss: 0.2695810794830322|Training Accuracy : 0.90625\n",
            "Batch : 212|Training Loss: 0.16172096133232117|Training Accuracy : 0.90625\n",
            "Batch : 213|Training Loss: 0.06507313251495361|Training Accuracy : 0.96875\n",
            "Batch : 214|Training Loss: 0.23725219070911407|Training Accuracy : 0.84375\n",
            "Batch : 215|Training Loss: 0.15597744286060333|Training Accuracy : 0.9375\n",
            "Batch : 216|Training Loss: 0.09863780438899994|Training Accuracy : 0.96875\n",
            "Batch : 217|Training Loss: 0.27033478021621704|Training Accuracy : 0.84375\n",
            "Batch : 218|Training Loss: 0.17860540747642517|Training Accuracy : 0.875\n",
            "Batch : 219|Training Loss: 0.2713351547718048|Training Accuracy : 0.90625\n",
            "Batch : 220|Training Loss: 0.11361295729875565|Training Accuracy : 0.9375\n",
            "Batch : 221|Training Loss: 0.2828291654586792|Training Accuracy : 0.875\n",
            "Batch : 222|Training Loss: 0.3204325735569|Training Accuracy : 0.84375\n",
            "Batch : 223|Training Loss: 0.12081661075353622|Training Accuracy : 0.9375\n",
            "Batch : 224|Training Loss: 0.09426265954971313|Training Accuracy : 0.96875\n",
            "Batch : 225|Training Loss: 0.33298417925834656|Training Accuracy : 0.9375\n",
            "Batch : 226|Training Loss: 0.23300522565841675|Training Accuracy : 0.90625\n",
            "Batch : 227|Training Loss: 0.16839002072811127|Training Accuracy : 0.90625\n",
            "Batch : 228|Training Loss: 0.270404189825058|Training Accuracy : 0.90625\n",
            "Batch : 229|Training Loss: 0.026581309735774994|Training Accuracy : 1.0\n",
            "Batch : 230|Training Loss: 0.03233979269862175|Training Accuracy : 1.0\n",
            "Batch : 231|Training Loss: 0.27258211374282837|Training Accuracy : 0.8125\n",
            "Batch : 232|Training Loss: 0.16188953816890717|Training Accuracy : 0.96875\n",
            "Batch : 233|Training Loss: 0.4623122811317444|Training Accuracy : 0.8125\n",
            "Batch : 234|Training Loss: 0.24905622005462646|Training Accuracy : 0.9375\n",
            "Batch : 235|Training Loss: 0.23570916056632996|Training Accuracy : 0.875\n",
            "Batch : 236|Training Loss: 0.39805808663368225|Training Accuracy : 0.84375\n",
            "Batch : 237|Training Loss: 0.08129844814538956|Training Accuracy : 0.96875\n",
            "Batch : 238|Training Loss: 0.05080167204141617|Training Accuracy : 1.0\n",
            "Batch : 239|Training Loss: 0.04239364340901375|Training Accuracy : 1.0\n",
            "Batch : 240|Training Loss: 0.13275019824504852|Training Accuracy : 0.875\n",
            "Batch : 241|Training Loss: 0.179515540599823|Training Accuracy : 0.90625\n",
            "Batch : 242|Training Loss: 0.07233981788158417|Training Accuracy : 0.96875\n",
            "Batch : 243|Training Loss: 0.10661068558692932|Training Accuracy : 0.96875\n",
            "Batch : 244|Training Loss: 0.14579063653945923|Training Accuracy : 0.9375\n",
            "Batch : 245|Training Loss: 0.23147280514240265|Training Accuracy : 0.90625\n",
            "Batch : 246|Training Loss: 0.06511580944061279|Training Accuracy : 0.96875\n",
            "Batch : 247|Training Loss: 0.14493151009082794|Training Accuracy : 0.9375\n",
            "Batch : 248|Training Loss: 0.14904147386550903|Training Accuracy : 0.90625\n",
            "Batch : 249|Training Loss: 0.33054977655410767|Training Accuracy : 0.875\n",
            "Batch : 250|Training Loss: 0.1152028888463974|Training Accuracy : 0.9375\n",
            "Batch : 251|Training Loss: 0.2557046115398407|Training Accuracy : 0.90625\n",
            "Batch : 252|Training Loss: 0.1889525055885315|Training Accuracy : 0.9375\n",
            "Batch : 253|Training Loss: 0.08708362281322479|Training Accuracy : 0.96875\n",
            "Batch : 254|Training Loss: 0.4441932737827301|Training Accuracy : 0.78125\n",
            "Batch : 255|Training Loss: 0.23343878984451294|Training Accuracy : 0.90625\n",
            "Batch : 256|Training Loss: 0.25141528248786926|Training Accuracy : 0.90625\n",
            "Batch : 257|Training Loss: 0.12700700759887695|Training Accuracy : 0.96875\n",
            "Batch : 258|Training Loss: 0.12649546563625336|Training Accuracy : 0.96875\n",
            "Batch : 259|Training Loss: 0.3629578649997711|Training Accuracy : 0.875\n",
            "Batch : 260|Training Loss: 0.13175353407859802|Training Accuracy : 0.96875\n",
            "Batch : 261|Training Loss: 0.09267638623714447|Training Accuracy : 1.0\n",
            "Batch : 262|Training Loss: 0.07833894342184067|Training Accuracy : 0.96875\n",
            "Batch : 263|Training Loss: 0.2577154338359833|Training Accuracy : 0.875\n",
            "Batch : 264|Training Loss: 0.025798769667744637|Training Accuracy : 1.0\n",
            "Batch : 265|Training Loss: 0.2416975200176239|Training Accuracy : 0.875\n",
            "Batch : 266|Training Loss: 0.2681756615638733|Training Accuracy : 0.84375\n",
            "Batch : 267|Training Loss: 0.05625656619668007|Training Accuracy : 1.0\n",
            "Batch : 268|Training Loss: 0.09526308625936508|Training Accuracy : 1.0\n",
            "Batch : 269|Training Loss: 0.09388913214206696|Training Accuracy : 0.9375\n",
            "Batch : 270|Training Loss: 0.15422050654888153|Training Accuracy : 0.90625\n",
            "Batch : 271|Training Loss: 0.2693130373954773|Training Accuracy : 0.875\n",
            "Batch : 272|Training Loss: 0.21615736186504364|Training Accuracy : 0.875\n",
            "Batch : 273|Training Loss: 0.17273734509944916|Training Accuracy : 0.9375\n",
            "Batch : 274|Training Loss: 0.0546463243663311|Training Accuracy : 1.0\n",
            "Batch : 275|Training Loss: 0.27211689949035645|Training Accuracy : 0.875\n",
            "Batch : 276|Training Loss: 0.28214937448501587|Training Accuracy : 0.90625\n",
            "Batch : 277|Training Loss: 0.20379677414894104|Training Accuracy : 0.875\n",
            "Batch : 278|Training Loss: 0.07952594012022018|Training Accuracy : 1.0\n",
            "Batch : 279|Training Loss: 0.07034078985452652|Training Accuracy : 1.0\n",
            "Batch : 280|Training Loss: 0.11400257796049118|Training Accuracy : 0.9375\n",
            "Batch : 281|Training Loss: 0.05707836151123047|Training Accuracy : 1.0\n",
            "Batch : 282|Training Loss: 0.18026892840862274|Training Accuracy : 0.90625\n",
            "Batch : 283|Training Loss: 0.3323727548122406|Training Accuracy : 0.84375\n",
            "Batch : 284|Training Loss: 0.17391660809516907|Training Accuracy : 0.9375\n",
            "Batch : 285|Training Loss: 0.05083586275577545|Training Accuracy : 0.96875\n",
            "Batch : 286|Training Loss: 0.06454269587993622|Training Accuracy : 0.96875\n",
            "Batch : 287|Training Loss: 0.12226227670907974|Training Accuracy : 0.96875\n",
            "Batch : 288|Training Loss: 0.3041197657585144|Training Accuracy : 0.84375\n",
            "Batch : 289|Training Loss: 0.09839022159576416|Training Accuracy : 1.0\n",
            "Batch : 290|Training Loss: 0.21723829209804535|Training Accuracy : 0.90625\n",
            "Batch : 291|Training Loss: 0.04018391668796539|Training Accuracy : 1.0\n",
            "Batch : 292|Training Loss: 0.11502447724342346|Training Accuracy : 0.96875\n",
            "Batch : 293|Training Loss: 0.3296333849430084|Training Accuracy : 0.875\n",
            "Batch : 294|Training Loss: 0.023306332528591156|Training Accuracy : 1.0\n",
            "Batch : 295|Training Loss: 0.31245559453964233|Training Accuracy : 0.90625\n",
            "Batch : 296|Training Loss: 0.12582238018512726|Training Accuracy : 1.0\n",
            "Batch : 297|Training Loss: 0.2168443351984024|Training Accuracy : 0.9375\n",
            "Batch : 298|Training Loss: 0.2214108556509018|Training Accuracy : 0.875\n",
            "Batch : 299|Training Loss: 0.04552317410707474|Training Accuracy : 1.0\n",
            "Batch : 300|Training Loss: 0.04663164168596268|Training Accuracy : 0.96875\n",
            "Batch : 301|Training Loss: 0.1749190092086792|Training Accuracy : 0.9375\n",
            "Batch : 302|Training Loss: 0.06293775141239166|Training Accuracy : 0.96875\n",
            "Batch : 303|Training Loss: 0.20326635241508484|Training Accuracy : 0.9375\n",
            "Batch : 304|Training Loss: 0.17891882359981537|Training Accuracy : 0.9375\n",
            "Batch : 305|Training Loss: 0.20131716132164001|Training Accuracy : 0.875\n",
            "Batch : 306|Training Loss: 0.25325143337249756|Training Accuracy : 0.875\n",
            "Batch : 307|Training Loss: 0.12465424835681915|Training Accuracy : 0.96875\n",
            "Batch : 308|Training Loss: 0.37330588698387146|Training Accuracy : 0.8125\n",
            "Batch : 309|Training Loss: 0.24426035583019257|Training Accuracy : 0.90625\n",
            "Batch : 310|Training Loss: 0.07336027920246124|Training Accuracy : 0.96875\n",
            "Batch : 311|Training Loss: 0.19033491611480713|Training Accuracy : 0.90625\n",
            "Batch : 312|Training Loss: 0.1600293219089508|Training Accuracy : 0.90625\n",
            "Batch : 313|Training Loss: 0.04448843002319336|Training Accuracy : 1.0\n",
            "Batch : 314|Training Loss: 0.16087934374809265|Training Accuracy : 0.9375\n",
            "Batch : 315|Training Loss: 0.10109265893697739|Training Accuracy : 0.96875\n",
            "Batch : 316|Training Loss: 0.09943293780088425|Training Accuracy : 0.9375\n",
            "Batch : 317|Training Loss: 0.03967949002981186|Training Accuracy : 1.0\n",
            "Batch : 318|Training Loss: 0.22798502445220947|Training Accuracy : 0.90625\n",
            "Batch : 319|Training Loss: 0.08578579127788544|Training Accuracy : 0.96875\n",
            "Batch : 320|Training Loss: 0.10482675582170486|Training Accuracy : 0.96875\n",
            "Batch : 321|Training Loss: 0.06005265191197395|Training Accuracy : 1.0\n",
            "Batch : 322|Training Loss: 0.07002092152833939|Training Accuracy : 0.96875\n",
            "Batch : 323|Training Loss: 0.03826006501913071|Training Accuracy : 1.0\n",
            "Batch : 324|Training Loss: 0.10820717364549637|Training Accuracy : 0.9375\n",
            "Batch : 325|Training Loss: 0.19963869452476501|Training Accuracy : 0.9375\n",
            "Batch : 326|Training Loss: 0.22102169692516327|Training Accuracy : 0.90625\n",
            "Batch : 327|Training Loss: 0.13709832727909088|Training Accuracy : 0.875\n",
            "Batch : 328|Training Loss: 0.07475423812866211|Training Accuracy : 1.0\n",
            "Batch : 329|Training Loss: 0.16432243585586548|Training Accuracy : 0.9375\n",
            "Batch : 330|Training Loss: 0.10310744494199753|Training Accuracy : 1.0\n",
            "Batch : 331|Training Loss: 0.3141750693321228|Training Accuracy : 0.96875\n",
            "Batch : 332|Training Loss: 0.03011392429471016|Training Accuracy : 1.0\n",
            "Batch : 333|Training Loss: 0.23206160962581635|Training Accuracy : 0.875\n",
            "Batch : 334|Training Loss: 0.13199958205223083|Training Accuracy : 0.9375\n",
            "Batch : 335|Training Loss: 0.2779575288295746|Training Accuracy : 0.875\n",
            "Batch : 336|Training Loss: 0.38894644379615784|Training Accuracy : 0.875\n",
            "Batch : 337|Training Loss: 0.09178874641656876|Training Accuracy : 1.0\n",
            "Batch : 338|Training Loss: 0.12378054112195969|Training Accuracy : 0.9375\n",
            "Batch : 339|Training Loss: 0.09244075417518616|Training Accuracy : 0.96875\n",
            "Batch : 340|Training Loss: 0.23109516501426697|Training Accuracy : 0.875\n",
            "Batch : 341|Training Loss: 0.29253366589546204|Training Accuracy : 0.9375\n",
            "Batch : 342|Training Loss: 0.09935387223958969|Training Accuracy : 0.9375\n",
            "Batch : 343|Training Loss: 0.1634027510881424|Training Accuracy : 0.9375\n",
            "Batch : 344|Training Loss: 0.870884895324707|Training Accuracy : 0.71875\n",
            "Batch : 345|Training Loss: 0.29188936948776245|Training Accuracy : 0.90625\n",
            "Batch : 346|Training Loss: 0.23523180186748505|Training Accuracy : 0.875\n",
            "Batch : 347|Training Loss: 0.2346022129058838|Training Accuracy : 0.875\n",
            "Batch : 348|Training Loss: 0.09573785960674286|Training Accuracy : 0.96875\n",
            "Batch : 349|Training Loss: 0.32738879323005676|Training Accuracy : 0.9375\n",
            "Batch : 350|Training Loss: 0.09769278764724731|Training Accuracy : 0.9375\n",
            "Batch : 351|Training Loss: 0.07214495539665222|Training Accuracy : 0.96875\n",
            "Batch : 352|Training Loss: 0.07181473076343536|Training Accuracy : 1.0\n",
            "Batch : 353|Training Loss: 0.48352140188217163|Training Accuracy : 0.8125\n",
            "Batch : 354|Training Loss: 0.16221719980239868|Training Accuracy : 0.96875\n",
            "Batch : 355|Training Loss: 0.08906462043523788|Training Accuracy : 0.96875\n",
            "Batch : 356|Training Loss: 0.11957332491874695|Training Accuracy : 0.96875\n",
            "Batch : 357|Training Loss: 0.3198494017124176|Training Accuracy : 0.9375\n",
            "Batch : 358|Training Loss: 0.22223082184791565|Training Accuracy : 0.96875\n",
            "Batch : 359|Training Loss: 0.3337346911430359|Training Accuracy : 0.875\n",
            "Batch : 360|Training Loss: 0.0415845662355423|Training Accuracy : 0.96875\n",
            "Batch : 361|Training Loss: 0.38549965620040894|Training Accuracy : 0.90625\n",
            "Batch : 362|Training Loss: 0.20864710211753845|Training Accuracy : 0.90625\n",
            "Batch : 363|Training Loss: 0.5754867792129517|Training Accuracy : 0.8125\n",
            "Batch : 364|Training Loss: 0.32012397050857544|Training Accuracy : 0.90625\n",
            "Batch : 365|Training Loss: 0.06601513922214508|Training Accuracy : 0.96875\n",
            "Batch : 366|Training Loss: 0.10577910393476486|Training Accuracy : 0.96875\n",
            "Batch : 367|Training Loss: 0.3203870356082916|Training Accuracy : 0.90625\n",
            "Batch : 368|Training Loss: 0.21495766937732697|Training Accuracy : 0.90625\n",
            "Batch : 369|Training Loss: 0.1927274912595749|Training Accuracy : 0.9375\n",
            "Batch : 370|Training Loss: 0.3750789165496826|Training Accuracy : 0.84375\n",
            "Batch : 371|Training Loss: 0.36979052424430847|Training Accuracy : 0.9375\n",
            "Batch : 372|Training Loss: 0.149988055229187|Training Accuracy : 0.9375\n",
            "Batch : 373|Training Loss: 0.23452770709991455|Training Accuracy : 0.90625\n",
            "Batch : 374|Training Loss: 0.03070475533604622|Training Accuracy : 1.0\n",
            "Batch : 375|Training Loss: 0.07293953001499176|Training Accuracy : 0.96875\n",
            "Batch : 376|Training Loss: 0.12770967185497284|Training Accuracy : 0.96875\n",
            "Batch : 377|Training Loss: 0.17017816007137299|Training Accuracy : 0.9375\n",
            "Batch : 378|Training Loss: 0.15520735085010529|Training Accuracy : 0.96875\n",
            "Batch : 379|Training Loss: 0.03073815628886223|Training Accuracy : 1.0\n",
            "Batch : 380|Training Loss: 0.2193714827299118|Training Accuracy : 0.9375\n",
            "Batch : 381|Training Loss: 0.1634608507156372|Training Accuracy : 0.875\n",
            "Batch : 382|Training Loss: 0.154100239276886|Training Accuracy : 0.9375\n",
            "Batch : 383|Training Loss: 0.2291727215051651|Training Accuracy : 0.9375\n",
            "Batch : 384|Training Loss: 0.07790907472372055|Training Accuracy : 1.0\n",
            "Batch : 385|Training Loss: 0.3200470209121704|Training Accuracy : 0.875\n",
            "Batch : 386|Training Loss: 0.15123116970062256|Training Accuracy : 0.9375\n",
            "Batch : 387|Training Loss: 0.15710923075675964|Training Accuracy : 0.90625\n",
            "Batch : 388|Training Loss: 0.03834771737456322|Training Accuracy : 1.0\n",
            "Batch : 389|Training Loss: 0.1494148075580597|Training Accuracy : 0.9375\n",
            "Batch : 390|Training Loss: 0.11900049448013306|Training Accuracy : 0.90625\n",
            "Batch : 391|Training Loss: 0.35330691933631897|Training Accuracy : 0.9375\n",
            "Batch : 392|Training Loss: 0.1593470573425293|Training Accuracy : 0.9375\n",
            "Batch : 393|Training Loss: 0.27109846472740173|Training Accuracy : 0.90625\n",
            "Batch : 394|Training Loss: 0.22587744891643524|Training Accuracy : 0.875\n",
            "Batch : 395|Training Loss: 0.1975206583738327|Training Accuracy : 0.875\n",
            "Batch : 396|Training Loss: 0.24529674649238586|Training Accuracy : 0.84375\n",
            "Batch : 397|Training Loss: 0.5354042649269104|Training Accuracy : 0.875\n",
            "Batch : 398|Training Loss: 0.1383288949728012|Training Accuracy : 0.9375\n",
            "Batch : 399|Training Loss: 0.13535216450691223|Training Accuracy : 0.96875\n",
            "Batch : 400|Training Loss: 0.3784893751144409|Training Accuracy : 0.8125\n",
            "Batch : 401|Training Loss: 0.04572626203298569|Training Accuracy : 1.0\n",
            "Batch : 402|Training Loss: 0.2183440774679184|Training Accuracy : 0.90625\n",
            "Batch : 403|Training Loss: 0.10719636082649231|Training Accuracy : 0.96875\n",
            "Batch : 404|Training Loss: 0.15718358755111694|Training Accuracy : 0.9375\n",
            "Batch : 405|Training Loss: 0.09236553311347961|Training Accuracy : 0.96875\n",
            "Batch : 406|Training Loss: 0.2596004605293274|Training Accuracy : 0.9375\n",
            "Batch : 407|Training Loss: 0.21016040444374084|Training Accuracy : 0.9375\n",
            "Batch : 408|Training Loss: 0.030223891139030457|Training Accuracy : 1.0\n",
            "Batch : 409|Training Loss: 0.06328295171260834|Training Accuracy : 1.0\n",
            "Batch : 410|Training Loss: 0.02702169492840767|Training Accuracy : 1.0\n",
            "Batch : 411|Training Loss: 0.18938638269901276|Training Accuracy : 0.9375\n",
            "Batch : 412|Training Loss: 0.07056862115859985|Training Accuracy : 0.96875\n",
            "Batch : 413|Training Loss: 0.14175452291965485|Training Accuracy : 0.96875\n",
            "Batch : 414|Training Loss: 0.2872430086135864|Training Accuracy : 0.875\n",
            "Batch : 415|Training Loss: 0.15839838981628418|Training Accuracy : 0.96875\n",
            "Batch : 416|Training Loss: 0.5752172470092773|Training Accuracy : 0.78125\n",
            "Batch : 417|Training Loss: 0.10939612984657288|Training Accuracy : 0.96875\n",
            "Batch : 418|Training Loss: 0.27230706810951233|Training Accuracy : 0.875\n",
            "Batch : 419|Training Loss: 0.05216516554355621|Training Accuracy : 0.96875\n",
            "Batch : 420|Training Loss: 0.22097539901733398|Training Accuracy : 0.90625\n",
            "Batch : 421|Training Loss: 0.10893026739358902|Training Accuracy : 0.9375\n",
            "Batch : 422|Training Loss: 0.15897861123085022|Training Accuracy : 0.90625\n",
            "Batch : 423|Training Loss: 0.24193714559078217|Training Accuracy : 0.90625\n",
            "Batch : 424|Training Loss: 0.38609346747398376|Training Accuracy : 0.90625\n",
            "Batch : 425|Training Loss: 0.12140459567308426|Training Accuracy : 0.9375\n",
            "Batch : 426|Training Loss: 0.0861125960946083|Training Accuracy : 1.0\n",
            "Batch : 427|Training Loss: 0.07693157345056534|Training Accuracy : 0.96875\n",
            "Batch : 428|Training Loss: 0.18646739423274994|Training Accuracy : 0.9375\n",
            "Batch : 429|Training Loss: 0.15165868401527405|Training Accuracy : 0.90625\n",
            "Batch : 430|Training Loss: 0.18388238549232483|Training Accuracy : 0.9375\n",
            "Batch : 431|Training Loss: 0.04259162396192551|Training Accuracy : 1.0\n",
            "Batch : 432|Training Loss: 0.22248104214668274|Training Accuracy : 0.90625\n",
            "Batch : 433|Training Loss: 0.3402285873889923|Training Accuracy : 0.875\n",
            "Batch : 434|Training Loss: 0.24492287635803223|Training Accuracy : 0.875\n",
            "Batch : 435|Training Loss: 0.1490376740694046|Training Accuracy : 0.9375\n",
            "Batch : 436|Training Loss: 0.0769159272313118|Training Accuracy : 1.0\n",
            "Batch : 437|Training Loss: 0.11282601207494736|Training Accuracy : 0.9375\n",
            "Batch : 438|Training Loss: 0.29490530490875244|Training Accuracy : 0.875\n",
            "Batch : 439|Training Loss: 0.17534923553466797|Training Accuracy : 0.90625\n",
            "Batch : 440|Training Loss: 0.06810612976551056|Training Accuracy : 1.0\n",
            "Batch : 441|Training Loss: 0.5040809512138367|Training Accuracy : 0.875\n",
            "Batch : 442|Training Loss: 0.2932871878147125|Training Accuracy : 0.84375\n",
            "Batch : 443|Training Loss: 0.1546989232301712|Training Accuracy : 0.96875\n",
            "Batch : 444|Training Loss: 0.22712956368923187|Training Accuracy : 0.90625\n",
            "Batch : 445|Training Loss: 0.1691242903470993|Training Accuracy : 0.90625\n",
            "Batch : 446|Training Loss: 0.18187923729419708|Training Accuracy : 0.9375\n",
            "Batch : 447|Training Loss: 0.1806592494249344|Training Accuracy : 0.875\n",
            "Batch : 448|Training Loss: 0.21516983211040497|Training Accuracy : 0.875\n",
            "Batch : 449|Training Loss: 0.06081503629684448|Training Accuracy : 0.96875\n",
            "Batch : 450|Training Loss: 0.08797433972358704|Training Accuracy : 0.9375\n",
            "Batch : 451|Training Loss: 0.09250669926404953|Training Accuracy : 0.96875\n",
            "Batch : 452|Training Loss: 0.08163721859455109|Training Accuracy : 0.9375\n",
            "Batch : 453|Training Loss: 0.42696651816368103|Training Accuracy : 0.875\n",
            "Batch : 454|Training Loss: 0.09071223437786102|Training Accuracy : 0.96875\n",
            "Batch : 455|Training Loss: 0.21180859208106995|Training Accuracy : 0.9375\n",
            "Batch : 456|Training Loss: 0.053669922053813934|Training Accuracy : 0.96875\n",
            "Batch : 457|Training Loss: 0.2802789807319641|Training Accuracy : 0.8125\n",
            "Batch : 458|Training Loss: 0.20080405473709106|Training Accuracy : 0.90625\n",
            "Batch : 459|Training Loss: 0.10564978420734406|Training Accuracy : 1.0\n",
            "Batch : 460|Training Loss: 0.1964261382818222|Training Accuracy : 0.9375\n",
            "Batch : 461|Training Loss: 0.14649035036563873|Training Accuracy : 0.9375\n",
            "Batch : 462|Training Loss: 0.23154599964618683|Training Accuracy : 0.9375\n",
            "Batch : 463|Training Loss: 0.05715043470263481|Training Accuracy : 1.0\n",
            "Batch : 464|Training Loss: 0.1451929807662964|Training Accuracy : 0.96875\n",
            "Batch : 465|Training Loss: 0.10579971224069595|Training Accuracy : 0.9375\n",
            "Batch : 466|Training Loss: 0.4092343747615814|Training Accuracy : 0.84375\n",
            "Batch : 467|Training Loss: 0.07526139914989471|Training Accuracy : 1.0\n",
            "Batch : 468|Training Loss: 0.056410085409879684|Training Accuracy : 0.96875\n",
            "Batch : 469|Training Loss: 0.17729106545448303|Training Accuracy : 0.9375\n",
            "Batch : 470|Training Loss: 0.0812748372554779|Training Accuracy : 0.96875\n",
            "Batch : 471|Training Loss: 0.20362688601016998|Training Accuracy : 0.9375\n",
            "Batch : 472|Training Loss: 0.3306834399700165|Training Accuracy : 0.90625\n",
            "Batch : 473|Training Loss: 0.241826593875885|Training Accuracy : 0.90625\n",
            "Batch : 474|Training Loss: 0.17000095546245575|Training Accuracy : 0.9375\n",
            "Batch : 475|Training Loss: 0.28606948256492615|Training Accuracy : 0.90625\n",
            "Batch : 476|Training Loss: 0.1802784949541092|Training Accuracy : 0.9375\n",
            "Batch : 477|Training Loss: 0.16874931752681732|Training Accuracy : 0.90625\n",
            "Batch : 478|Training Loss: 0.09222816675901413|Training Accuracy : 0.96875\n",
            "Batch : 479|Training Loss: 0.32934775948524475|Training Accuracy : 0.84375\n",
            "Batch : 480|Training Loss: 0.24770748615264893|Training Accuracy : 0.90625\n",
            "Batch : 481|Training Loss: 0.3695988357067108|Training Accuracy : 0.90625\n",
            "Batch : 482|Training Loss: 0.24119603633880615|Training Accuracy : 0.90625\n",
            "Batch : 483|Training Loss: 0.3166838586330414|Training Accuracy : 0.90625\n",
            "Batch : 484|Training Loss: 0.22051680088043213|Training Accuracy : 0.90625\n",
            "Batch : 485|Training Loss: 0.13156811892986298|Training Accuracy : 0.9375\n",
            "Batch : 486|Training Loss: 0.22281530499458313|Training Accuracy : 0.9375\n",
            "Batch : 487|Training Loss: 0.10054580867290497|Training Accuracy : 0.9375\n",
            "Batch : 488|Training Loss: 0.05771949887275696|Training Accuracy : 1.0\n",
            "Batch : 489|Training Loss: 0.245393306016922|Training Accuracy : 0.875\n",
            "Batch : 490|Training Loss: 0.17312990128993988|Training Accuracy : 0.90625\n",
            "Batch : 491|Training Loss: 0.322075754404068|Training Accuracy : 0.90625\n",
            "Batch : 492|Training Loss: 0.19597001373767853|Training Accuracy : 0.90625\n",
            "Batch : 493|Training Loss: 0.12378144264221191|Training Accuracy : 0.9375\n",
            "Batch : 494|Training Loss: 0.03526511415839195|Training Accuracy : 1.0\n",
            "Batch : 495|Training Loss: 0.34221547842025757|Training Accuracy : 0.9375\n",
            "Batch : 496|Training Loss: 0.09888719767332077|Training Accuracy : 0.96875\n",
            "Batch : 497|Training Loss: 0.12286902219057083|Training Accuracy : 0.9375\n",
            "Batch : 498|Training Loss: 0.0855102613568306|Training Accuracy : 0.96875\n",
            "Batch : 499|Training Loss: 0.19625402987003326|Training Accuracy : 0.9375\n",
            "Batch : 500|Training Loss: 0.20283526182174683|Training Accuracy : 0.9375\n",
            "Batch : 501|Training Loss: 0.26501721143722534|Training Accuracy : 0.90625\n",
            "Batch : 502|Training Loss: 0.31348901987075806|Training Accuracy : 0.90625\n",
            "Batch : 503|Training Loss: 0.20417998731136322|Training Accuracy : 0.9375\n",
            "Batch : 504|Training Loss: 0.1952761709690094|Training Accuracy : 0.875\n",
            "Batch : 505|Training Loss: 0.33097195625305176|Training Accuracy : 0.8125\n",
            "Batch : 506|Training Loss: 0.1004612073302269|Training Accuracy : 0.9375\n",
            "Batch : 507|Training Loss: 0.2995324432849884|Training Accuracy : 0.90625\n",
            "Batch : 508|Training Loss: 0.45452845096588135|Training Accuracy : 0.84375\n",
            "Batch : 509|Training Loss: 0.22323691844940186|Training Accuracy : 0.84375\n",
            "Batch : 510|Training Loss: 0.1367523968219757|Training Accuracy : 0.9375\n",
            "Batch : 511|Training Loss: 0.14836977422237396|Training Accuracy : 0.9375\n",
            "Batch : 512|Training Loss: 0.15446947515010834|Training Accuracy : 0.96875\n",
            "Batch : 513|Training Loss: 0.11492188274860382|Training Accuracy : 0.96875\n",
            "Batch : 514|Training Loss: 0.21711571514606476|Training Accuracy : 0.9375\n",
            "Batch : 515|Training Loss: 0.13951550424098969|Training Accuracy : 0.96875\n",
            "Batch : 516|Training Loss: 0.09588402509689331|Training Accuracy : 0.96875\n",
            "Batch : 517|Training Loss: 0.29484933614730835|Training Accuracy : 0.875\n",
            "Batch : 518|Training Loss: 0.2590498626232147|Training Accuracy : 0.875\n",
            "Batch : 519|Training Loss: 0.2898150384426117|Training Accuracy : 0.875\n",
            "Batch : 520|Training Loss: 0.6112645864486694|Training Accuracy : 0.8125\n",
            "Batch : 521|Training Loss: 0.1697906106710434|Training Accuracy : 0.90625\n",
            "Batch : 522|Training Loss: 0.060241296887397766|Training Accuracy : 1.0\n",
            "Batch : 523|Training Loss: 0.05641013756394386|Training Accuracy : 1.0\n",
            "Batch : 524|Training Loss: 0.12982235848903656|Training Accuracy : 0.9375\n",
            "Batch : 525|Training Loss: 0.29456818103790283|Training Accuracy : 0.9375\n",
            "Batch : 526|Training Loss: 0.28189247846603394|Training Accuracy : 0.84375\n",
            "Batch : 527|Training Loss: 0.2333952635526657|Training Accuracy : 0.9375\n",
            "Batch : 528|Training Loss: 0.1508018970489502|Training Accuracy : 0.875\n",
            "Batch : 529|Training Loss: 0.13008292019367218|Training Accuracy : 0.9375\n",
            "Batch : 530|Training Loss: 0.343644917011261|Training Accuracy : 0.875\n",
            "Batch : 531|Training Loss: 0.20587334036827087|Training Accuracy : 0.90625\n",
            "Batch : 532|Training Loss: 0.06490376591682434|Training Accuracy : 0.96875\n",
            "Batch : 533|Training Loss: 0.37870121002197266|Training Accuracy : 0.90625\n",
            "Batch : 534|Training Loss: 0.1784399300813675|Training Accuracy : 0.90625\n",
            "Batch : 535|Training Loss: 0.1578088104724884|Training Accuracy : 0.90625\n",
            "Batch : 536|Training Loss: 0.20114952325820923|Training Accuracy : 0.875\n",
            "Batch : 537|Training Loss: 0.0640612244606018|Training Accuracy : 1.0\n",
            "Batch : 538|Training Loss: 0.11542385816574097|Training Accuracy : 0.9375\n",
            "Batch : 539|Training Loss: 0.056277547031641006|Training Accuracy : 1.0\n",
            "Batch : 540|Training Loss: 0.08733026683330536|Training Accuracy : 0.96875\n",
            "Batch : 541|Training Loss: 0.35996511578559875|Training Accuracy : 0.875\n",
            "Batch : 542|Training Loss: 0.2381979376077652|Training Accuracy : 0.90625\n",
            "Batch : 543|Training Loss: 0.37440067529678345|Training Accuracy : 0.90625\n",
            "Batch : 544|Training Loss: 0.16202791035175323|Training Accuracy : 0.875\n",
            "Batch : 545|Training Loss: 0.2536783516407013|Training Accuracy : 0.90625\n",
            "Batch : 546|Training Loss: 0.23313646018505096|Training Accuracy : 0.90625\n",
            "Batch : 547|Training Loss: 0.11026941984891891|Training Accuracy : 0.9375\n",
            "Batch : 548|Training Loss: 0.20122475922107697|Training Accuracy : 0.90625\n",
            "Batch : 549|Training Loss: 0.26116278767585754|Training Accuracy : 0.9375\n",
            "Batch : 550|Training Loss: 0.1931728720664978|Training Accuracy : 0.90625\n",
            "Batch : 551|Training Loss: 0.1678345501422882|Training Accuracy : 0.90625\n",
            "Batch : 552|Training Loss: 0.20859414339065552|Training Accuracy : 0.90625\n",
            "Batch : 553|Training Loss: 0.1227019876241684|Training Accuracy : 0.9375\n",
            "Batch : 554|Training Loss: 0.29740962386131287|Training Accuracy : 0.875\n",
            "Batch : 555|Training Loss: 0.2275145798921585|Training Accuracy : 0.90625\n",
            "Batch : 556|Training Loss: 0.5789940357208252|Training Accuracy : 0.90625\n",
            "Batch : 557|Training Loss: 0.1021064817905426|Training Accuracy : 1.0\n",
            "Batch : 558|Training Loss: 0.20593959093093872|Training Accuracy : 0.9375\n",
            "Batch : 559|Training Loss: 0.0833524763584137|Training Accuracy : 0.9375\n",
            "Batch : 560|Training Loss: 0.20733189582824707|Training Accuracy : 0.9375\n",
            "Batch : 561|Training Loss: 0.22045567631721497|Training Accuracy : 0.90625\n",
            "Batch : 562|Training Loss: 0.14216504991054535|Training Accuracy : 0.96875\n",
            "Batch : 563|Training Loss: 0.193818137049675|Training Accuracy : 0.875\n",
            "Batch : 564|Training Loss: 0.36938610672950745|Training Accuracy : 0.84375\n",
            "Batch : 565|Training Loss: 0.3772069215774536|Training Accuracy : 0.875\n",
            "Batch : 566|Training Loss: 0.2717133164405823|Training Accuracy : 0.90625\n",
            "Batch : 567|Training Loss: 0.06198619306087494|Training Accuracy : 1.0\n",
            "Batch : 568|Training Loss: 0.057081833481788635|Training Accuracy : 0.96875\n",
            "Batch : 569|Training Loss: 0.14588868618011475|Training Accuracy : 0.90625\n",
            "Batch : 570|Training Loss: 0.15604335069656372|Training Accuracy : 0.9375\n",
            "Batch : 571|Training Loss: 0.1672307401895523|Training Accuracy : 0.9375\n",
            "Batch : 572|Training Loss: 0.07941552251577377|Training Accuracy : 1.0\n",
            "Batch : 573|Training Loss: 0.10484130680561066|Training Accuracy : 0.96875\n",
            "Batch : 574|Training Loss: 0.06610620021820068|Training Accuracy : 0.96875\n",
            "Batch : 575|Training Loss: 0.14465171098709106|Training Accuracy : 0.90625\n",
            "Batch : 576|Training Loss: 0.1343477964401245|Training Accuracy : 0.9375\n",
            "Batch : 577|Training Loss: 0.09691281616687775|Training Accuracy : 0.96875\n",
            "Batch : 578|Training Loss: 0.1448478251695633|Training Accuracy : 0.96875\n",
            "Batch : 579|Training Loss: 0.1268800050020218|Training Accuracy : 0.96875\n",
            "Batch : 580|Training Loss: 0.28890731930732727|Training Accuracy : 0.875\n",
            "Batch : 581|Training Loss: 0.22173509001731873|Training Accuracy : 0.9375\n",
            "Batch : 582|Training Loss: 0.2838709056377411|Training Accuracy : 0.90625\n",
            "Batch : 583|Training Loss: 0.2908475697040558|Training Accuracy : 0.9375\n",
            "Batch : 584|Training Loss: 0.10929574072360992|Training Accuracy : 0.96875\n",
            "Batch : 585|Training Loss: 0.3098968267440796|Training Accuracy : 0.875\n",
            "Batch : 586|Training Loss: 0.1299150586128235|Training Accuracy : 0.90625\n",
            "Batch : 587|Training Loss: 0.18761925399303436|Training Accuracy : 0.875\n",
            "Batch : 588|Training Loss: 0.12497952580451965|Training Accuracy : 0.9375\n",
            "Batch : 589|Training Loss: 0.25380778312683105|Training Accuracy : 0.90625\n",
            "Batch : 590|Training Loss: 0.07200393080711365|Training Accuracy : 0.96875\n",
            "Batch : 591|Training Loss: 0.16450870037078857|Training Accuracy : 0.90625\n",
            "Batch : 592|Training Loss: 0.061948370188474655|Training Accuracy : 0.96875\n",
            "Batch : 593|Training Loss: 0.1507388800382614|Training Accuracy : 0.9375\n",
            "Batch : 594|Training Loss: 0.21411949396133423|Training Accuracy : 0.90625\n",
            "Batch : 595|Training Loss: 0.1295141726732254|Training Accuracy : 0.9375\n",
            "Batch : 596|Training Loss: 0.24552255868911743|Training Accuracy : 0.9375\n",
            "Batch : 597|Training Loss: 0.3077486455440521|Training Accuracy : 0.875\n",
            "Batch : 598|Training Loss: 0.05255734175443649|Training Accuracy : 1.0\n",
            "Batch : 599|Training Loss: 0.0972670242190361|Training Accuracy : 0.96875\n",
            "Batch : 600|Training Loss: 0.3685325086116791|Training Accuracy : 0.84375\n",
            "Batch : 601|Training Loss: 0.34553924202919006|Training Accuracy : 0.875\n",
            "Batch : 602|Training Loss: 0.04132682457566261|Training Accuracy : 1.0\n",
            "Batch : 603|Training Loss: 0.11896169930696487|Training Accuracy : 0.90625\n",
            "Batch : 604|Training Loss: 0.25236737728118896|Training Accuracy : 0.9375\n",
            "Batch : 605|Training Loss: 0.17346470057964325|Training Accuracy : 0.9375\n",
            "Batch : 606|Training Loss: 0.2017865628004074|Training Accuracy : 0.9375\n",
            "Batch : 607|Training Loss: 0.10357127338647842|Training Accuracy : 0.9375\n",
            "Batch : 608|Training Loss: 0.301008015871048|Training Accuracy : 0.875\n",
            "Batch : 609|Training Loss: 0.06601297110319138|Training Accuracy : 0.96875\n",
            "Batch : 610|Training Loss: 0.17996613681316376|Training Accuracy : 0.9375\n",
            "Batch : 611|Training Loss: 0.19999195635318756|Training Accuracy : 0.90625\n",
            "Batch : 612|Training Loss: 0.17597144842147827|Training Accuracy : 0.90625\n",
            "Batch : 613|Training Loss: 0.1597142070531845|Training Accuracy : 0.96875\n",
            "Batch : 614|Training Loss: 0.17332778871059418|Training Accuracy : 0.90625\n",
            "Batch : 615|Training Loss: 0.2890525460243225|Training Accuracy : 0.90625\n",
            "Batch : 616|Training Loss: 0.30594632029533386|Training Accuracy : 0.9375\n",
            "Batch : 617|Training Loss: 0.08117158710956573|Training Accuracy : 0.96875\n",
            "Batch : 618|Training Loss: 0.15643909573554993|Training Accuracy : 0.90625\n",
            "Batch : 619|Training Loss: 0.07181872427463531|Training Accuracy : 0.9375\n",
            "Batch : 620|Training Loss: 0.15206830203533173|Training Accuracy : 0.90625\n",
            "Batch : 621|Training Loss: 0.32946518063545227|Training Accuracy : 0.8125\n",
            "Batch : 622|Training Loss: 0.4931655824184418|Training Accuracy : 0.84375\n",
            "Batch : 623|Training Loss: 0.1241142675280571|Training Accuracy : 0.9375\n",
            "Batch : 624|Training Loss: 0.12367421388626099|Training Accuracy : 0.9375\n",
            "Batch : 625|Training Loss: 0.193934828042984|Training Accuracy : 0.90625\n",
            "Batch : 626|Training Loss: 0.1425096094608307|Training Accuracy : 0.90625\n",
            "Batch : 627|Training Loss: 0.16930890083312988|Training Accuracy : 0.875\n",
            "Batch : 628|Training Loss: 0.10182811319828033|Training Accuracy : 0.96875\n",
            "Batch : 629|Training Loss: 0.02911967970430851|Training Accuracy : 1.0\n",
            "Batch : 630|Training Loss: 0.22937390208244324|Training Accuracy : 0.875\n",
            "Batch : 631|Training Loss: 0.06678241491317749|Training Accuracy : 0.96875\n",
            "Batch : 632|Training Loss: 0.30567729473114014|Training Accuracy : 0.84375\n",
            "Batch : 633|Training Loss: 0.2580471634864807|Training Accuracy : 0.84375\n",
            "Batch : 634|Training Loss: 0.21296708285808563|Training Accuracy : 0.9375\n",
            "Batch : 635|Training Loss: 0.1372910439968109|Training Accuracy : 0.90625\n",
            "Batch : 636|Training Loss: 0.02464599907398224|Training Accuracy : 1.0\n",
            "Batch : 637|Training Loss: 0.1684606820344925|Training Accuracy : 0.9375\n",
            "Batch : 638|Training Loss: 0.19117102026939392|Training Accuracy : 0.9375\n",
            "Batch : 639|Training Loss: 0.15217511355876923|Training Accuracy : 0.9375\n",
            "Batch : 640|Training Loss: 0.14429502189159393|Training Accuracy : 0.96875\n",
            "Batch : 641|Training Loss: 0.08471358567476273|Training Accuracy : 0.96875\n",
            "Batch : 642|Training Loss: 0.12744948267936707|Training Accuracy : 0.96875\n",
            "Batch : 643|Training Loss: 0.11287645250558853|Training Accuracy : 0.9375\n",
            "Batch : 644|Training Loss: 0.23662757873535156|Training Accuracy : 0.875\n",
            "Batch : 645|Training Loss: 0.16067823767662048|Training Accuracy : 0.90625\n",
            "Batch : 646|Training Loss: 0.08045855909585953|Training Accuracy : 1.0\n",
            "Batch : 647|Training Loss: 0.12385670095682144|Training Accuracy : 0.96875\n",
            "Batch : 648|Training Loss: 0.20072250068187714|Training Accuracy : 0.90625\n",
            "Batch : 649|Training Loss: 0.24708928167819977|Training Accuracy : 0.90625\n",
            "Batch : 650|Training Loss: 0.4036548137664795|Training Accuracy : 0.84375\n",
            "Batch : 651|Training Loss: 0.1912074089050293|Training Accuracy : 0.9375\n",
            "Batch : 652|Training Loss: 0.16402171552181244|Training Accuracy : 0.9375\n",
            "Batch : 653|Training Loss: 0.26981833577156067|Training Accuracy : 0.90625\n",
            "Batch : 654|Training Loss: 0.09882588684558868|Training Accuracy : 0.96875\n",
            "Batch : 655|Training Loss: 0.41502222418785095|Training Accuracy : 0.875\n",
            "Batch : 656|Training Loss: 0.076962411403656|Training Accuracy : 1.0\n",
            "Batch : 657|Training Loss: 0.09841445088386536|Training Accuracy : 0.96875\n",
            "Batch : 658|Training Loss: 0.056626591831445694|Training Accuracy : 1.0\n",
            "Batch : 659|Training Loss: 0.1289457082748413|Training Accuracy : 0.9375\n",
            "Batch : 660|Training Loss: 0.13332508504390717|Training Accuracy : 0.9375\n",
            "Batch : 661|Training Loss: 0.11293137818574905|Training Accuracy : 0.9375\n",
            "Batch : 662|Training Loss: 0.13037796318531036|Training Accuracy : 0.90625\n",
            "Batch : 663|Training Loss: 0.21972915530204773|Training Accuracy : 0.9375\n",
            "Batch : 664|Training Loss: 0.26842033863067627|Training Accuracy : 0.90625\n",
            "Batch : 665|Training Loss: 0.05206873267889023|Training Accuracy : 1.0\n",
            "Batch : 666|Training Loss: 0.11961426585912704|Training Accuracy : 0.96875\n",
            "Batch : 667|Training Loss: 0.1698146015405655|Training Accuracy : 0.9375\n",
            "Batch : 668|Training Loss: 0.22013452649116516|Training Accuracy : 0.9375\n",
            "Batch : 669|Training Loss: 0.0565636083483696|Training Accuracy : 1.0\n",
            "Batch : 670|Training Loss: 0.17151932418346405|Training Accuracy : 0.9375\n",
            "Batch : 671|Training Loss: 0.1496623456478119|Training Accuracy : 0.9375\n",
            "Batch : 672|Training Loss: 0.2128664255142212|Training Accuracy : 0.875\n",
            "Batch : 673|Training Loss: 0.13651306927204132|Training Accuracy : 0.96875\n",
            "Batch : 674|Training Loss: 0.3149333894252777|Training Accuracy : 0.875\n",
            "Batch : 675|Training Loss: 0.11297209560871124|Training Accuracy : 0.96875\n",
            "Batch : 676|Training Loss: 0.12426070123910904|Training Accuracy : 0.9375\n",
            "Batch : 677|Training Loss: 0.03440454974770546|Training Accuracy : 1.0\n",
            "Batch : 678|Training Loss: 0.3124132454395294|Training Accuracy : 0.9375\n",
            "Batch : 679|Training Loss: 0.3943212628364563|Training Accuracy : 0.84375\n",
            "Batch : 680|Training Loss: 0.3171439468860626|Training Accuracy : 0.84375\n",
            "Batch : 681|Training Loss: 0.0664597600698471|Training Accuracy : 0.96875\n",
            "Batch : 682|Training Loss: 0.06222115457057953|Training Accuracy : 0.96875\n",
            "Batch : 683|Training Loss: 0.18600809574127197|Training Accuracy : 0.90625\n",
            "Batch : 684|Training Loss: 0.037813398987054825|Training Accuracy : 1.0\n",
            "Batch : 685|Training Loss: 0.1050705537199974|Training Accuracy : 0.96875\n",
            "Batch : 686|Training Loss: 0.0931185632944107|Training Accuracy : 1.0\n",
            "Batch : 687|Training Loss: 0.14649958908557892|Training Accuracy : 0.96875\n",
            "Batch : 688|Training Loss: 0.0937575027346611|Training Accuracy : 0.9375\n",
            "Batch : 689|Training Loss: 0.19271109998226166|Training Accuracy : 0.90625\n",
            "Batch : 690|Training Loss: 0.058240074664354324|Training Accuracy : 1.0\n",
            "Batch : 691|Training Loss: 0.16445764899253845|Training Accuracy : 0.9375\n",
            "Batch : 692|Training Loss: 0.12926673889160156|Training Accuracy : 0.9375\n",
            "Batch : 693|Training Loss: 0.14147327840328217|Training Accuracy : 0.96875\n",
            "Batch : 694|Training Loss: 0.17608289420604706|Training Accuracy : 0.9375\n",
            "Batch : 695|Training Loss: 0.08598718792200089|Training Accuracy : 0.96875\n",
            "Batch : 696|Training Loss: 0.1584627479314804|Training Accuracy : 0.9375\n",
            "Batch : 697|Training Loss: 0.3204790949821472|Training Accuracy : 0.90625\n",
            "Batch : 698|Training Loss: 0.3499763309955597|Training Accuracy : 0.875\n",
            "Batch : 699|Training Loss: 0.3186483383178711|Training Accuracy : 0.875\n",
            "Batch : 700|Training Loss: 0.29360368847846985|Training Accuracy : 0.875\n",
            "Batch : 701|Training Loss: 0.10881619155406952|Training Accuracy : 0.9375\n",
            "Batch : 702|Training Loss: 0.16840212047100067|Training Accuracy : 0.90625\n",
            "Batch : 703|Training Loss: 0.11095701903104782|Training Accuracy : 0.9375\n",
            "Batch : 704|Training Loss: 0.1324954330921173|Training Accuracy : 0.9375\n",
            "Batch : 705|Training Loss: 0.20415589213371277|Training Accuracy : 0.90625\n",
            "Batch : 706|Training Loss: 0.4479578137397766|Training Accuracy : 0.875\n",
            "Batch : 707|Training Loss: 0.06398724019527435|Training Accuracy : 1.0\n",
            "Batch : 708|Training Loss: 0.15859995782375336|Training Accuracy : 0.9375\n",
            "Batch : 709|Training Loss: 0.1807834804058075|Training Accuracy : 0.90625\n",
            "Batch : 710|Training Loss: 0.2847525477409363|Training Accuracy : 0.875\n",
            "Batch : 711|Training Loss: 0.08018440008163452|Training Accuracy : 0.96875\n",
            "Batch : 712|Training Loss: 0.3577762246131897|Training Accuracy : 0.875\n",
            "Batch : 713|Training Loss: 0.18968036770820618|Training Accuracy : 0.9375\n",
            "Batch : 714|Training Loss: 0.20589487254619598|Training Accuracy : 0.90625\n",
            "Batch : 715|Training Loss: 0.1965000033378601|Training Accuracy : 0.90625\n",
            "Batch : 716|Training Loss: 0.09124084562063217|Training Accuracy : 0.96875\n",
            "Batch : 717|Training Loss: 0.20263592898845673|Training Accuracy : 0.84375\n",
            "Batch : 718|Training Loss: 0.18246343731880188|Training Accuracy : 0.96875\n",
            "Batch : 719|Training Loss: 0.273011714220047|Training Accuracy : 0.9375\n",
            "Batch : 720|Training Loss: 0.24160607159137726|Training Accuracy : 0.90625\n",
            "Batch : 721|Training Loss: 0.1307412087917328|Training Accuracy : 0.9375\n",
            "Batch : 722|Training Loss: 0.09940613061189651|Training Accuracy : 1.0\n",
            "Batch : 723|Training Loss: 0.33489498496055603|Training Accuracy : 0.90625\n",
            "Batch : 724|Training Loss: 0.11244866251945496|Training Accuracy : 0.9375\n",
            "Batch : 725|Training Loss: 0.1735411286354065|Training Accuracy : 0.90625\n",
            "Batch : 726|Training Loss: 0.09018279612064362|Training Accuracy : 0.96875\n",
            "Batch : 727|Training Loss: 0.09440329670906067|Training Accuracy : 0.96875\n",
            "Batch : 728|Training Loss: 0.23623526096343994|Training Accuracy : 0.90625\n",
            "Batch : 729|Training Loss: 0.26047924160957336|Training Accuracy : 0.90625\n",
            "Batch : 730|Training Loss: 0.15619871020317078|Training Accuracy : 0.9375\n",
            "Batch : 731|Training Loss: 0.08915720880031586|Training Accuracy : 0.96875\n",
            "Batch : 732|Training Loss: 0.1851990520954132|Training Accuracy : 0.9375\n",
            "Batch : 733|Training Loss: 0.28151246905326843|Training Accuracy : 0.875\n",
            "Batch : 734|Training Loss: 0.16264645755290985|Training Accuracy : 0.9375\n",
            "Batch : 735|Training Loss: 0.25143736600875854|Training Accuracy : 0.875\n",
            "Batch : 736|Training Loss: 0.33125030994415283|Training Accuracy : 0.875\n",
            "Batch : 737|Training Loss: 0.052025191485881805|Training Accuracy : 1.0\n",
            "Batch : 738|Training Loss: 0.2797936201095581|Training Accuracy : 0.875\n",
            "Batch : 739|Training Loss: 0.08233927935361862|Training Accuracy : 0.96875\n",
            "Batch : 740|Training Loss: 0.33588355779647827|Training Accuracy : 0.875\n",
            "Batch : 741|Training Loss: 0.2111123502254486|Training Accuracy : 0.90625\n",
            "Batch : 742|Training Loss: 0.09091023355722427|Training Accuracy : 0.9375\n",
            "Batch : 743|Training Loss: 0.08395358175039291|Training Accuracy : 0.9375\n",
            "Batch : 744|Training Loss: 0.17587196826934814|Training Accuracy : 0.90625\n",
            "Batch : 745|Training Loss: 0.06072208285331726|Training Accuracy : 0.96875\n",
            "Batch : 746|Training Loss: 0.18677853047847748|Training Accuracy : 0.9375\n",
            "Batch : 747|Training Loss: 0.16433879733085632|Training Accuracy : 0.90625\n",
            "Batch : 748|Training Loss: 0.030620237812399864|Training Accuracy : 1.0\n",
            "Batch : 749|Training Loss: 0.1268448531627655|Training Accuracy : 0.96875\n",
            "Batch : 750|Training Loss: 0.0905236005783081|Training Accuracy : 0.96875\n",
            "Batch : 751|Training Loss: 0.10310323536396027|Training Accuracy : 0.96875\n",
            "Batch : 752|Training Loss: 0.0914812684059143|Training Accuracy : 0.96875\n",
            "Batch : 753|Training Loss: 0.08297361433506012|Training Accuracy : 0.96875\n",
            "Batch : 754|Training Loss: 0.24172848463058472|Training Accuracy : 0.90625\n",
            "Batch : 755|Training Loss: 0.16996049880981445|Training Accuracy : 0.96875\n",
            "Batch : 756|Training Loss: 0.09875951707363129|Training Accuracy : 0.96875\n",
            "Batch : 757|Training Loss: 0.07644836604595184|Training Accuracy : 1.0\n",
            "Batch : 758|Training Loss: 0.05053836852312088|Training Accuracy : 1.0\n",
            "Batch : 759|Training Loss: 0.027367178350687027|Training Accuracy : 1.0\n",
            "Batch : 760|Training Loss: 0.13377435505390167|Training Accuracy : 0.96875\n",
            "Batch : 761|Training Loss: 0.2716761529445648|Training Accuracy : 0.875\n",
            "Batch : 762|Training Loss: 0.08412763476371765|Training Accuracy : 0.96875\n",
            "Batch : 763|Training Loss: 0.15970517694950104|Training Accuracy : 0.90625\n",
            "Batch : 764|Training Loss: 0.18803371489048004|Training Accuracy : 0.90625\n",
            "Batch : 765|Training Loss: 0.1426176130771637|Training Accuracy : 0.96875\n",
            "Batch : 766|Training Loss: 0.12755949795246124|Training Accuracy : 0.96875\n",
            "Batch : 767|Training Loss: 0.21674633026123047|Training Accuracy : 0.9375\n",
            "Batch : 768|Training Loss: 0.19524505734443665|Training Accuracy : 0.9375\n",
            "Batch : 769|Training Loss: 0.12241505831480026|Training Accuracy : 0.9375\n",
            "Batch : 770|Training Loss: 0.2163913995027542|Training Accuracy : 0.875\n",
            "Batch : 771|Training Loss: 0.09338996559381485|Training Accuracy : 0.9375\n",
            "Batch : 772|Training Loss: 0.20872193574905396|Training Accuracy : 0.9375\n",
            "Batch : 773|Training Loss: 0.23765553534030914|Training Accuracy : 0.875\n",
            "Batch : 774|Training Loss: 0.2638965845108032|Training Accuracy : 0.90625\n",
            "Batch : 775|Training Loss: 0.061221085488796234|Training Accuracy : 0.96875\n",
            "Batch : 776|Training Loss: 0.18485407531261444|Training Accuracy : 0.90625\n",
            "Batch : 777|Training Loss: 0.21948683261871338|Training Accuracy : 0.875\n",
            "Batch : 778|Training Loss: 0.09038209915161133|Training Accuracy : 0.96875\n",
            "Batch : 779|Training Loss: 0.07083860784769058|Training Accuracy : 1.0\n",
            "Batch : 780|Training Loss: 0.07806800305843353|Training Accuracy : 1.0\n",
            "Batch : 781|Training Loss: 0.20510408282279968|Training Accuracy : 0.875\n",
            "Batch : 782|Training Loss: 0.370269238948822|Training Accuracy : 0.8125\n",
            "Batch : 783|Training Loss: 0.056975819170475006|Training Accuracy : 1.0\n",
            "Batch : 784|Training Loss: 0.21436437964439392|Training Accuracy : 0.90625\n",
            "Batch : 785|Training Loss: 0.2743055522441864|Training Accuracy : 0.8125\n",
            "Batch : 786|Training Loss: 0.08123309165239334|Training Accuracy : 0.96875\n",
            "Batch : 787|Training Loss: 0.023641815409064293|Training Accuracy : 1.0\n",
            "Batch : 788|Training Loss: 0.13655491173267365|Training Accuracy : 0.96875\n",
            "Batch : 789|Training Loss: 0.2099575698375702|Training Accuracy : 0.96875\n",
            "Batch : 790|Training Loss: 0.018941596150398254|Training Accuracy : 1.0\n",
            "Batch : 791|Training Loss: 0.06387894600629807|Training Accuracy : 0.96875\n",
            "Batch : 792|Training Loss: 0.21104662120342255|Training Accuracy : 0.875\n",
            "Batch : 793|Training Loss: 0.1461699903011322|Training Accuracy : 0.9375\n",
            "Batch : 794|Training Loss: 0.33669787645339966|Training Accuracy : 0.90625\n",
            "Batch : 795|Training Loss: 0.2856607437133789|Training Accuracy : 0.875\n",
            "Batch : 796|Training Loss: 0.19931742548942566|Training Accuracy : 0.90625\n",
            "Batch : 797|Training Loss: 0.20016203820705414|Training Accuracy : 0.90625\n",
            "Batch : 798|Training Loss: 0.20648953318595886|Training Accuracy : 0.96875\n",
            "Batch : 799|Training Loss: 0.10000130534172058|Training Accuracy : 0.9375\n",
            "Batch : 800|Training Loss: 0.23446319997310638|Training Accuracy : 0.9375\n",
            "Batch : 801|Training Loss: 0.18820327520370483|Training Accuracy : 0.875\n",
            "Batch : 802|Training Loss: 0.18969057500362396|Training Accuracy : 0.9375\n",
            "Batch : 803|Training Loss: 0.23485499620437622|Training Accuracy : 0.875\n",
            "Batch : 804|Training Loss: 0.11453600972890854|Training Accuracy : 0.9375\n",
            "Batch : 805|Training Loss: 0.06358769536018372|Training Accuracy : 1.0\n",
            "Batch : 806|Training Loss: 0.30800095200538635|Training Accuracy : 0.875\n",
            "Batch : 807|Training Loss: 0.06923974305391312|Training Accuracy : 0.96875\n",
            "Batch : 808|Training Loss: 0.23195792734622955|Training Accuracy : 0.875\n",
            "Batch : 809|Training Loss: 0.24733667075634003|Training Accuracy : 0.90625\n",
            "Batch : 810|Training Loss: 0.12122251838445663|Training Accuracy : 0.96875\n",
            "Batch : 811|Training Loss: 0.18513986468315125|Training Accuracy : 0.90625\n",
            "Batch : 812|Training Loss: 0.15283174812793732|Training Accuracy : 0.96875\n",
            "Batch : 813|Training Loss: 0.18035563826560974|Training Accuracy : 0.9375\n",
            "Batch : 814|Training Loss: 0.04748149588704109|Training Accuracy : 1.0\n",
            "Batch : 815|Training Loss: 0.09939583390951157|Training Accuracy : 0.96875\n",
            "Batch : 816|Training Loss: 0.04747522994875908|Training Accuracy : 0.96875\n",
            "Batch : 817|Training Loss: 0.1592591106891632|Training Accuracy : 0.9375\n",
            "Batch : 818|Training Loss: 0.12359511852264404|Training Accuracy : 0.9375\n",
            "Batch : 819|Training Loss: 0.29784926772117615|Training Accuracy : 0.96875\n",
            "Batch : 820|Training Loss: 0.15435095131397247|Training Accuracy : 0.90625\n",
            "Batch : 821|Training Loss: 0.06788208335638046|Training Accuracy : 0.96875\n",
            "Batch : 822|Training Loss: 0.08162470906972885|Training Accuracy : 0.96875\n",
            "Batch : 823|Training Loss: 0.47179192304611206|Training Accuracy : 0.9375\n",
            "Batch : 824|Training Loss: 0.20822186768054962|Training Accuracy : 0.9375\n",
            "Batch : 825|Training Loss: 0.13125237822532654|Training Accuracy : 0.90625\n",
            "Batch : 826|Training Loss: 0.3081464171409607|Training Accuracy : 0.84375\n",
            "Batch : 827|Training Loss: 0.05325355380773544|Training Accuracy : 1.0\n",
            "Batch : 828|Training Loss: 0.2675715386867523|Training Accuracy : 0.90625\n",
            "Batch : 829|Training Loss: 0.21306583285331726|Training Accuracy : 0.9375\n",
            "Batch : 830|Training Loss: 0.12156209349632263|Training Accuracy : 0.96875\n",
            "Batch : 831|Training Loss: 0.025288119912147522|Training Accuracy : 1.0\n",
            "Batch : 832|Training Loss: 0.17347778379917145|Training Accuracy : 0.90625\n",
            "Batch : 833|Training Loss: 0.13547676801681519|Training Accuracy : 0.96875\n",
            "Batch : 834|Training Loss: 0.09574583172798157|Training Accuracy : 0.96875\n",
            "Batch : 835|Training Loss: 0.13136212527751923|Training Accuracy : 0.96875\n",
            "Batch : 836|Training Loss: 0.36983469128608704|Training Accuracy : 0.90625\n",
            "Batch : 837|Training Loss: 0.3983632028102875|Training Accuracy : 0.90625\n",
            "Batch : 838|Training Loss: 0.10970104485750198|Training Accuracy : 0.96875\n",
            "Batch : 839|Training Loss: 0.2309739589691162|Training Accuracy : 0.9375\n",
            "Batch : 840|Training Loss: 0.20055878162384033|Training Accuracy : 0.90625\n",
            "Batch : 841|Training Loss: 0.2150869071483612|Training Accuracy : 0.875\n",
            "Batch : 842|Training Loss: 0.3344516158103943|Training Accuracy : 0.90625\n",
            "Batch : 843|Training Loss: 0.10972314327955246|Training Accuracy : 0.96875\n",
            "Batch : 844|Training Loss: 0.2534586191177368|Training Accuracy : 0.9375\n",
            "Batch : 845|Training Loss: 0.31456446647644043|Training Accuracy : 0.90625\n",
            "Batch : 846|Training Loss: 0.384009450674057|Training Accuracy : 0.875\n",
            "Batch : 847|Training Loss: 0.2645733952522278|Training Accuracy : 0.9375\n",
            "Batch : 848|Training Loss: 0.19203320145606995|Training Accuracy : 0.90625\n",
            "Batch : 849|Training Loss: 0.24571798741817474|Training Accuracy : 0.90625\n",
            "Batch : 850|Training Loss: 0.07618018984794617|Training Accuracy : 1.0\n",
            "Batch : 851|Training Loss: 0.11577078700065613|Training Accuracy : 0.96875\n",
            "Batch : 852|Training Loss: 0.13916327059268951|Training Accuracy : 0.90625\n",
            "Batch : 853|Training Loss: 0.38763558864593506|Training Accuracy : 0.875\n",
            "Batch : 854|Training Loss: 0.03105836920440197|Training Accuracy : 1.0\n",
            "Batch : 855|Training Loss: 0.04387529939413071|Training Accuracy : 1.0\n",
            "Batch : 856|Training Loss: 0.24088993668556213|Training Accuracy : 0.875\n",
            "Batch : 857|Training Loss: 0.4426090121269226|Training Accuracy : 0.9375\n",
            "Batch : 858|Training Loss: 0.19152048230171204|Training Accuracy : 0.9375\n",
            "Batch : 859|Training Loss: 0.18070776760578156|Training Accuracy : 0.9375\n",
            "Batch : 860|Training Loss: 0.17485837638378143|Training Accuracy : 0.96875\n",
            "Batch : 861|Training Loss: 0.06499296426773071|Training Accuracy : 1.0\n",
            "Batch : 862|Training Loss: 0.130112886428833|Training Accuracy : 0.9375\n",
            "Batch : 863|Training Loss: 0.17824088037014008|Training Accuracy : 0.90625\n",
            "Batch : 864|Training Loss: 0.10754383355379105|Training Accuracy : 0.96875\n",
            "Batch : 865|Training Loss: 0.22115401923656464|Training Accuracy : 0.875\n",
            "Batch : 866|Training Loss: 0.12189706414937973|Training Accuracy : 0.9375\n",
            "Batch : 867|Training Loss: 0.30245354771614075|Training Accuracy : 0.90625\n",
            "Batch : 868|Training Loss: 0.19998511672019958|Training Accuracy : 0.9375\n",
            "Batch : 869|Training Loss: 0.20444142818450928|Training Accuracy : 0.875\n",
            "Batch : 870|Training Loss: 0.266989529132843|Training Accuracy : 0.9375\n",
            "Batch : 871|Training Loss: 0.44623732566833496|Training Accuracy : 0.875\n",
            "Batch : 872|Training Loss: 0.12904328107833862|Training Accuracy : 0.96875\n",
            "Batch : 873|Training Loss: 0.1050780713558197|Training Accuracy : 0.96875\n",
            "Batch : 874|Training Loss: 0.09778925776481628|Training Accuracy : 0.9375\n",
            "Batch : 875|Training Loss: 0.23513418436050415|Training Accuracy : 0.90625\n",
            "Batch : 876|Training Loss: 0.041220709681510925|Training Accuracy : 0.96875\n",
            "Batch : 877|Training Loss: 0.08023415505886078|Training Accuracy : 0.96875\n",
            "Batch : 878|Training Loss: 0.14896802604198456|Training Accuracy : 0.9375\n",
            "Batch : 879|Training Loss: 0.032732706516981125|Training Accuracy : 1.0\n",
            "Batch : 880|Training Loss: 0.14860644936561584|Training Accuracy : 0.90625\n",
            "Batch : 881|Training Loss: 0.12625792622566223|Training Accuracy : 0.96875\n",
            "Batch : 882|Training Loss: 0.18340516090393066|Training Accuracy : 0.90625\n",
            "Batch : 883|Training Loss: 0.028394905850291252|Training Accuracy : 1.0\n",
            "Batch : 884|Training Loss: 0.10475972294807434|Training Accuracy : 0.96875\n",
            "Batch : 885|Training Loss: 0.18039977550506592|Training Accuracy : 0.9375\n",
            "Batch : 886|Training Loss: 0.1650826781988144|Training Accuracy : 0.96875\n",
            "Batch : 887|Training Loss: 0.06062481552362442|Training Accuracy : 0.96875\n",
            "Batch : 888|Training Loss: 0.18081900477409363|Training Accuracy : 0.90625\n",
            "Batch : 889|Training Loss: 0.05077355355024338|Training Accuracy : 1.0\n",
            "Batch : 890|Training Loss: 0.1353570967912674|Training Accuracy : 0.90625\n",
            "Batch : 891|Training Loss: 0.19896352291107178|Training Accuracy : 0.9375\n",
            "Batch : 892|Training Loss: 0.10690344870090485|Training Accuracy : 0.96875\n",
            "Batch : 893|Training Loss: 0.035026825964450836|Training Accuracy : 1.0\n",
            "Batch : 894|Training Loss: 0.0462241992354393|Training Accuracy : 1.0\n",
            "Batch : 895|Training Loss: 0.12856994569301605|Training Accuracy : 0.96875\n",
            "Batch : 896|Training Loss: 0.10919968783855438|Training Accuracy : 0.96875\n",
            "Batch : 897|Training Loss: 0.14930632710456848|Training Accuracy : 0.9375\n",
            "Batch : 898|Training Loss: 0.2014501541852951|Training Accuracy : 0.9375\n",
            "Batch : 899|Training Loss: 0.24563230574131012|Training Accuracy : 0.875\n",
            "Batch : 900|Training Loss: 0.44708573818206787|Training Accuracy : 0.90625\n",
            "Batch : 901|Training Loss: 0.06287221610546112|Training Accuracy : 1.0\n",
            "Batch : 902|Training Loss: 0.20846986770629883|Training Accuracy : 0.90625\n",
            "Batch : 903|Training Loss: 0.3114621341228485|Training Accuracy : 0.9375\n",
            "Batch : 904|Training Loss: 0.2679210305213928|Training Accuracy : 0.84375\n",
            "Batch : 905|Training Loss: 0.13846038281917572|Training Accuracy : 0.9375\n",
            "Batch : 906|Training Loss: 0.12932418286800385|Training Accuracy : 0.96875\n",
            "Batch : 907|Training Loss: 0.04297721013426781|Training Accuracy : 1.0\n",
            "Batch : 908|Training Loss: 0.1866178661584854|Training Accuracy : 0.96875\n",
            "Batch : 909|Training Loss: 0.05530146509408951|Training Accuracy : 1.0\n",
            "Batch : 910|Training Loss: 0.33559346199035645|Training Accuracy : 0.90625\n",
            "Batch : 911|Training Loss: 0.14045558869838715|Training Accuracy : 0.90625\n",
            "Batch : 912|Training Loss: 0.09737429022789001|Training Accuracy : 0.9375\n",
            "Batch : 913|Training Loss: 0.3749701678752899|Training Accuracy : 0.84375\n",
            "Batch : 914|Training Loss: 0.15172693133354187|Training Accuracy : 0.9375\n",
            "Batch : 915|Training Loss: 0.27689453959465027|Training Accuracy : 0.875\n",
            "Batch : 916|Training Loss: 0.163033127784729|Training Accuracy : 0.9375\n",
            "Batch : 917|Training Loss: 0.09879609942436218|Training Accuracy : 0.96875\n",
            "Batch : 918|Training Loss: 0.061462365090847015|Training Accuracy : 1.0\n",
            "Batch : 919|Training Loss: 0.2803381085395813|Training Accuracy : 0.90625\n",
            "Batch : 920|Training Loss: 0.3313273787498474|Training Accuracy : 0.90625\n",
            "Batch : 921|Training Loss: 0.07814532518386841|Training Accuracy : 0.96875\n",
            "Batch : 922|Training Loss: 0.3082953095436096|Training Accuracy : 0.90625\n",
            "Batch : 923|Training Loss: 0.31308165192604065|Training Accuracy : 0.875\n",
            "Batch : 924|Training Loss: 0.06357159465551376|Training Accuracy : 0.96875\n",
            "Batch : 925|Training Loss: 0.3203955590724945|Training Accuracy : 0.84375\n",
            "Batch : 926|Training Loss: 0.23150688409805298|Training Accuracy : 0.9375\n",
            "Batch : 927|Training Loss: 0.17174556851387024|Training Accuracy : 0.90625\n",
            "Batch : 928|Training Loss: 0.14042040705680847|Training Accuracy : 0.96875\n",
            "Batch : 929|Training Loss: 0.23666910827159882|Training Accuracy : 0.90625\n",
            "Batch : 930|Training Loss: 0.15016236901283264|Training Accuracy : 0.96875\n",
            "Batch : 931|Training Loss: 0.10213851183652878|Training Accuracy : 0.96875\n",
            "Batch : 932|Training Loss: 0.12097116559743881|Training Accuracy : 0.9375\n",
            "Batch : 933|Training Loss: 0.25639608502388|Training Accuracy : 0.90625\n",
            "Batch : 934|Training Loss: 0.23756617307662964|Training Accuracy : 0.875\n",
            "Batch : 935|Training Loss: 0.14805233478546143|Training Accuracy : 0.96875\n",
            "Batch : 936|Training Loss: 0.16735060513019562|Training Accuracy : 0.90625\n",
            "Batch : 937|Training Loss: 0.30262884497642517|Training Accuracy : 0.875\n",
            "Batch : 938|Training Loss: 0.18503610789775848|Training Accuracy : 0.875\n",
            "Batch : 939|Training Loss: 0.06362956762313843|Training Accuracy : 1.0\n",
            "Batch : 940|Training Loss: 0.17303678393363953|Training Accuracy : 0.90625\n",
            "Batch : 941|Training Loss: 0.18762768805027008|Training Accuracy : 0.9375\n",
            "Batch : 942|Training Loss: 0.15094678103923798|Training Accuracy : 0.90625\n",
            "Batch : 943|Training Loss: 0.13846942782402039|Training Accuracy : 0.9375\n",
            "Batch : 944|Training Loss: 0.32721662521362305|Training Accuracy : 0.8125\n",
            "Batch : 945|Training Loss: 0.06810792535543442|Training Accuracy : 0.96875\n",
            "Batch : 946|Training Loss: 0.11093246936798096|Training Accuracy : 0.96875\n",
            "Batch : 947|Training Loss: 0.11062638461589813|Training Accuracy : 0.9375\n",
            "Batch : 948|Training Loss: 0.2728463411331177|Training Accuracy : 0.875\n",
            "Batch : 949|Training Loss: 0.4001547694206238|Training Accuracy : 0.90625\n",
            "Batch : 950|Training Loss: 0.09681015461683273|Training Accuracy : 0.9375\n",
            "Batch : 951|Training Loss: 0.2551557421684265|Training Accuracy : 0.875\n",
            "Batch : 952|Training Loss: 0.08624829351902008|Training Accuracy : 0.9375\n",
            "Batch : 953|Training Loss: 0.11848051846027374|Training Accuracy : 0.9375\n",
            "Batch : 954|Training Loss: 0.23304907977581024|Training Accuracy : 0.84375\n",
            "Batch : 955|Training Loss: 0.15294241905212402|Training Accuracy : 0.9375\n",
            "Batch : 956|Training Loss: 0.33656248450279236|Training Accuracy : 0.8125\n",
            "Batch : 957|Training Loss: 0.15167666971683502|Training Accuracy : 0.9375\n",
            "Batch : 958|Training Loss: 0.21545115113258362|Training Accuracy : 0.96875\n",
            "Batch : 959|Training Loss: 0.06337299942970276|Training Accuracy : 1.0\n",
            "Batch : 960|Training Loss: 0.29863324761390686|Training Accuracy : 0.875\n",
            "Batch : 961|Training Loss: 0.30272358655929565|Training Accuracy : 0.90625\n",
            "Batch : 962|Training Loss: 0.3183955252170563|Training Accuracy : 0.875\n",
            "Batch : 963|Training Loss: 0.19094422459602356|Training Accuracy : 0.90625\n",
            "Batch : 964|Training Loss: 0.1506480574607849|Training Accuracy : 0.9375\n",
            "Batch : 965|Training Loss: 0.16850487887859344|Training Accuracy : 0.90625\n",
            "Batch : 966|Training Loss: 0.36232078075408936|Training Accuracy : 0.84375\n",
            "Batch : 967|Training Loss: 0.15147966146469116|Training Accuracy : 0.9375\n",
            "Batch : 968|Training Loss: 0.021217163652181625|Training Accuracy : 1.0\n",
            "Batch : 969|Training Loss: 0.38931483030319214|Training Accuracy : 0.90625\n",
            "Batch : 970|Training Loss: 0.14824281632900238|Training Accuracy : 0.9375\n",
            "Batch : 971|Training Loss: 0.13420875370502472|Training Accuracy : 0.9375\n",
            "Batch : 972|Training Loss: 0.1270769238471985|Training Accuracy : 0.9375\n",
            "Batch : 973|Training Loss: 0.14769764244556427|Training Accuracy : 0.9375\n",
            "Batch : 974|Training Loss: 0.18844403326511383|Training Accuracy : 0.9375\n",
            "Batch : 975|Training Loss: 0.14533904194831848|Training Accuracy : 0.96875\n",
            "Batch : 976|Training Loss: 0.18578886985778809|Training Accuracy : 0.9375\n",
            "Batch : 977|Training Loss: 0.23174259066581726|Training Accuracy : 0.96875\n",
            "Batch : 978|Training Loss: 0.10541614890098572|Training Accuracy : 1.0\n",
            "Batch : 979|Training Loss: 0.12014304846525192|Training Accuracy : 0.9375\n",
            "Batch : 980|Training Loss: 0.14146281778812408|Training Accuracy : 0.96875\n",
            "Batch : 981|Training Loss: 0.22293217480182648|Training Accuracy : 0.9375\n",
            "Batch : 982|Training Loss: 0.12681686878204346|Training Accuracy : 0.96875\n",
            "Batch : 983|Training Loss: 0.2623435854911804|Training Accuracy : 0.90625\n",
            "Batch : 984|Training Loss: 0.12656372785568237|Training Accuracy : 0.9375\n",
            "Batch : 985|Training Loss: 0.2015802413225174|Training Accuracy : 0.90625\n",
            "Batch : 986|Training Loss: 0.3118777871131897|Training Accuracy : 0.875\n",
            "Batch : 987|Training Loss: 0.19217342138290405|Training Accuracy : 0.90625\n",
            "Batch : 988|Training Loss: 0.14528025686740875|Training Accuracy : 0.9375\n",
            "Batch : 989|Training Loss: 0.2172340750694275|Training Accuracy : 0.90625\n",
            "Batch : 990|Training Loss: 0.13867031037807465|Training Accuracy : 0.9375\n",
            "Batch : 991|Training Loss: 0.1681554913520813|Training Accuracy : 0.96875\n",
            "Batch : 992|Training Loss: 0.28420960903167725|Training Accuracy : 0.90625\n",
            "Batch : 993|Training Loss: 0.17174285650253296|Training Accuracy : 0.96875\n",
            "Batch : 994|Training Loss: 0.2876140773296356|Training Accuracy : 0.8125\n",
            "Batch : 995|Training Loss: 0.2598917484283447|Training Accuracy : 0.875\n",
            "Batch : 996|Training Loss: 0.13913355767726898|Training Accuracy : 0.90625\n",
            "Batch : 997|Training Loss: 0.13878877460956573|Training Accuracy : 0.96875\n",
            "Batch : 998|Training Loss: 0.21136918663978577|Training Accuracy : 0.90625\n",
            "Batch : 999|Training Loss: 0.0354151651263237|Training Accuracy : 1.0\n",
            "Batch : 1000|Training Loss: 0.171410471200943|Training Accuracy : 0.90625\n",
            "Batch : 1001|Training Loss: 0.310528039932251|Training Accuracy : 0.8125\n",
            "Batch : 1002|Training Loss: 0.08975322544574738|Training Accuracy : 0.96875\n",
            "Batch : 1003|Training Loss: 0.20123009383678436|Training Accuracy : 0.90625\n",
            "Batch : 1004|Training Loss: 0.08075164258480072|Training Accuracy : 0.96875\n",
            "Batch : 1005|Training Loss: 0.1277163326740265|Training Accuracy : 0.90625\n",
            "Batch : 1006|Training Loss: 0.09542908519506454|Training Accuracy : 0.96875\n",
            "Batch : 1007|Training Loss: 0.17199109494686127|Training Accuracy : 0.9375\n",
            "Batch : 1008|Training Loss: 0.020702306181192398|Training Accuracy : 1.0\n",
            "Batch : 1009|Training Loss: 0.09998536109924316|Training Accuracy : 0.96875\n",
            "Batch : 1010|Training Loss: 0.37926822900772095|Training Accuracy : 0.875\n",
            "Batch : 1011|Training Loss: 0.3915862739086151|Training Accuracy : 0.90625\n",
            "Batch : 1012|Training Loss: 0.1949724555015564|Training Accuracy : 0.9375\n",
            "Batch : 1013|Training Loss: 0.15071240067481995|Training Accuracy : 0.90625\n",
            "Batch : 1014|Training Loss: 0.2361641675233841|Training Accuracy : 0.9375\n",
            "Batch : 1015|Training Loss: 0.17681695520877838|Training Accuracy : 0.90625\n",
            "Batch : 1016|Training Loss: 0.149861142039299|Training Accuracy : 0.9375\n",
            "Batch : 1017|Training Loss: 0.23570185899734497|Training Accuracy : 0.9375\n",
            "Batch : 1018|Training Loss: 0.026880325749516487|Training Accuracy : 1.0\n",
            "Batch : 1019|Training Loss: 0.17395342886447906|Training Accuracy : 0.90625\n",
            "Batch : 1020|Training Loss: 0.13031283020973206|Training Accuracy : 0.96875\n",
            "Batch : 1021|Training Loss: 0.32533740997314453|Training Accuracy : 0.9375\n",
            "Batch : 1022|Training Loss: 0.029025115072727203|Training Accuracy : 1.0\n",
            "Batch : 1023|Training Loss: 0.2810452878475189|Training Accuracy : 0.84375\n",
            "Batch : 1024|Training Loss: 0.14293928444385529|Training Accuracy : 0.9375\n",
            "Batch : 1025|Training Loss: 0.16521134972572327|Training Accuracy : 0.96875\n",
            "Batch : 1026|Training Loss: 0.27136391401290894|Training Accuracy : 0.84375\n",
            "Batch : 1027|Training Loss: 0.0466274693608284|Training Accuracy : 1.0\n",
            "Batch : 1028|Training Loss: 0.19543755054473877|Training Accuracy : 0.875\n",
            "Batch : 1029|Training Loss: 0.30077797174453735|Training Accuracy : 0.90625\n",
            "Batch : 1030|Training Loss: 0.13443659245967865|Training Accuracy : 0.90625\n",
            "Batch : 1031|Training Loss: 0.17882591485977173|Training Accuracy : 0.9375\n",
            "Batch : 1032|Training Loss: 0.2734009325504303|Training Accuracy : 0.90625\n",
            "Batch : 1033|Training Loss: 0.18791398406028748|Training Accuracy : 0.9375\n",
            "Batch : 1034|Training Loss: 0.1354527622461319|Training Accuracy : 0.9375\n",
            "Batch : 1035|Training Loss: 0.19995012879371643|Training Accuracy : 0.90625\n",
            "Batch : 1036|Training Loss: 0.19456633925437927|Training Accuracy : 0.9375\n",
            "Batch : 1037|Training Loss: 0.24288228154182434|Training Accuracy : 0.90625\n",
            "Batch : 1038|Training Loss: 0.28271952271461487|Training Accuracy : 0.90625\n",
            "Batch : 1039|Training Loss: 0.3839232325553894|Training Accuracy : 0.875\n",
            "Batch : 1040|Training Loss: 0.15473778545856476|Training Accuracy : 0.9375\n",
            "Batch : 1041|Training Loss: 0.04646720364689827|Training Accuracy : 1.0\n",
            "Batch : 1042|Training Loss: 0.16905799508094788|Training Accuracy : 0.90625\n",
            "Batch : 1043|Training Loss: 0.09415090084075928|Training Accuracy : 0.9375\n",
            "Batch : 1044|Training Loss: 0.08304132521152496|Training Accuracy : 0.96875\n",
            "Batch : 1045|Training Loss: 0.1701737493276596|Training Accuracy : 0.90625\n",
            "Batch : 1046|Training Loss: 0.21260444819927216|Training Accuracy : 0.90625\n",
            "Batch : 1047|Training Loss: 0.10146145522594452|Training Accuracy : 0.96875\n",
            "Batch : 1048|Training Loss: 0.19409169256687164|Training Accuracy : 0.90625\n",
            "Batch : 1049|Training Loss: 0.12473099678754807|Training Accuracy : 0.96875\n",
            "Batch : 1050|Training Loss: 0.2051914781332016|Training Accuracy : 0.9375\n",
            "Batch : 1051|Training Loss: 0.09765191376209259|Training Accuracy : 0.9375\n",
            "Batch : 1052|Training Loss: 0.07630487531423569|Training Accuracy : 1.0\n",
            "Batch : 1053|Training Loss: 0.2878849506378174|Training Accuracy : 0.90625\n",
            "Batch : 1054|Training Loss: 0.21680200099945068|Training Accuracy : 0.9375\n",
            "Batch : 1055|Training Loss: 0.09189145267009735|Training Accuracy : 0.9375\n",
            "Batch : 1056|Training Loss: 0.12261319160461426|Training Accuracy : 0.9375\n",
            "Batch : 1057|Training Loss: 0.16101017594337463|Training Accuracy : 0.9375\n",
            "Batch : 1058|Training Loss: 0.33124929666519165|Training Accuracy : 0.875\n",
            "Batch : 1059|Training Loss: 0.0508236400783062|Training Accuracy : 1.0\n",
            "Batch : 1060|Training Loss: 0.35089316964149475|Training Accuracy : 0.8125\n",
            "Batch : 1061|Training Loss: 0.025312431156635284|Training Accuracy : 1.0\n",
            "Batch : 1062|Training Loss: 0.23577992618083954|Training Accuracy : 0.90625\n",
            "Batch : 1063|Training Loss: 0.16607162356376648|Training Accuracy : 0.9375\n",
            "Batch : 1064|Training Loss: 0.16999706625938416|Training Accuracy : 0.90625\n",
            "Batch : 1065|Training Loss: 0.49220967292785645|Training Accuracy : 0.875\n",
            "Batch : 1066|Training Loss: 0.09746626764535904|Training Accuracy : 0.96875\n",
            "Batch : 1067|Training Loss: 0.12354017794132233|Training Accuracy : 0.96875\n",
            "Batch : 1068|Training Loss: 0.4119277000427246|Training Accuracy : 0.875\n",
            "Batch : 1069|Training Loss: 0.204889714717865|Training Accuracy : 0.96875\n",
            "Batch : 1070|Training Loss: 0.15842442214488983|Training Accuracy : 0.90625\n",
            "Batch : 1071|Training Loss: 0.2074601948261261|Training Accuracy : 0.9375\n",
            "Batch : 1072|Training Loss: 0.20263177156448364|Training Accuracy : 0.90625\n",
            "Batch : 1073|Training Loss: 0.3152754008769989|Training Accuracy : 0.84375\n",
            "Batch : 1074|Training Loss: 0.17484267055988312|Training Accuracy : 0.9375\n",
            "Batch : 1075|Training Loss: 0.09860549867153168|Training Accuracy : 0.9375\n",
            "Batch : 1076|Training Loss: 0.14631251990795135|Training Accuracy : 0.9375\n",
            "Batch : 1077|Training Loss: 0.09817583858966827|Training Accuracy : 0.96875\n",
            "Batch : 1078|Training Loss: 0.26704132556915283|Training Accuracy : 0.90625\n",
            "Batch : 1079|Training Loss: 0.1513247936964035|Training Accuracy : 0.9375\n",
            "Batch : 1080|Training Loss: 0.142643004655838|Training Accuracy : 0.96875\n",
            "Batch : 1081|Training Loss: 0.3151434659957886|Training Accuracy : 0.90625\n",
            "Batch : 1082|Training Loss: 0.24920925498008728|Training Accuracy : 0.90625\n",
            "Batch : 1083|Training Loss: 0.16100294888019562|Training Accuracy : 0.9375\n",
            "Batch : 1084|Training Loss: 0.05918440595269203|Training Accuracy : 0.96875\n",
            "Batch : 1085|Training Loss: 0.32251495122909546|Training Accuracy : 0.875\n",
            "Batch : 1086|Training Loss: 0.13260778784751892|Training Accuracy : 0.9375\n",
            "Batch : 1087|Training Loss: 0.17127616703510284|Training Accuracy : 0.9375\n",
            "Batch : 1088|Training Loss: 0.1985516995191574|Training Accuracy : 0.9375\n",
            "Batch : 1089|Training Loss: 0.12236473709344864|Training Accuracy : 0.96875\n",
            "Batch : 1090|Training Loss: 0.04572853073477745|Training Accuracy : 1.0\n",
            "Batch : 1091|Training Loss: 0.07003096491098404|Training Accuracy : 0.96875\n",
            "Batch : 1092|Training Loss: 0.21050909161567688|Training Accuracy : 0.90625\n",
            "Batch : 1093|Training Loss: 0.2065620869398117|Training Accuracy : 0.9375\n",
            "Batch : 1094|Training Loss: 0.10777657479047775|Training Accuracy : 0.90625\n",
            "Batch : 1095|Training Loss: 0.22901684045791626|Training Accuracy : 0.90625\n",
            "Batch : 1096|Training Loss: 0.16566629707813263|Training Accuracy : 0.9375\n",
            "Batch : 1097|Training Loss: 0.22882075607776642|Training Accuracy : 0.90625\n",
            "Batch : 1098|Training Loss: 0.20266610383987427|Training Accuracy : 0.96875\n",
            "Batch : 1099|Training Loss: 0.1292945295572281|Training Accuracy : 0.9375\n",
            "Batch : 1100|Training Loss: 0.4451964497566223|Training Accuracy : 0.78125\n",
            "Batch : 1101|Training Loss: 0.1902879923582077|Training Accuracy : 0.9375\n",
            "Batch : 1102|Training Loss: 0.2908208966255188|Training Accuracy : 0.875\n",
            "Batch : 1103|Training Loss: 0.12721264362335205|Training Accuracy : 0.96875\n",
            "Batch : 1104|Training Loss: 0.28843894600868225|Training Accuracy : 0.875\n",
            "Batch : 1105|Training Loss: 0.29949256777763367|Training Accuracy : 0.875\n",
            "Batch : 1106|Training Loss: 0.2994903326034546|Training Accuracy : 0.9375\n",
            "Batch : 1107|Training Loss: 0.1284278780221939|Training Accuracy : 0.96875\n",
            "Batch : 1108|Training Loss: 0.24133412539958954|Training Accuracy : 0.90625\n",
            "Batch : 1109|Training Loss: 0.0660741776227951|Training Accuracy : 1.0\n",
            "Batch : 1110|Training Loss: 0.15947997570037842|Training Accuracy : 0.90625\n",
            "Batch : 1111|Training Loss: 0.1788521409034729|Training Accuracy : 0.9375\n",
            "Batch : 1112|Training Loss: 0.07275377959012985|Training Accuracy : 1.0\n",
            "Batch : 1113|Training Loss: 0.20207692682743073|Training Accuracy : 0.875\n",
            "Batch : 1114|Training Loss: 0.04393598064780235|Training Accuracy : 1.0\n",
            "Batch : 1115|Training Loss: 0.18941135704517365|Training Accuracy : 0.9375\n",
            "Batch : 1116|Training Loss: 0.3741636276245117|Training Accuracy : 0.90625\n",
            "Batch : 1117|Training Loss: 0.11683139204978943|Training Accuracy : 0.9375\n",
            "Batch : 1118|Training Loss: 0.1037307158112526|Training Accuracy : 0.96875\n",
            "Batch : 1119|Training Loss: 0.18400777876377106|Training Accuracy : 0.96875\n",
            "Batch : 1120|Training Loss: 0.2119428962469101|Training Accuracy : 0.9375\n",
            "Batch : 1121|Training Loss: 0.0454770065844059|Training Accuracy : 1.0\n",
            "Batch : 1122|Training Loss: 0.08149014413356781|Training Accuracy : 0.96875\n",
            "Batch : 1123|Training Loss: 0.3744206428527832|Training Accuracy : 0.875\n",
            "Batch : 1124|Training Loss: 0.28780317306518555|Training Accuracy : 0.90625\n",
            "Batch : 1125|Training Loss: 0.028428805992007256|Training Accuracy : 1.0\n",
            "Batch : 1126|Training Loss: 0.19207613170146942|Training Accuracy : 0.90625\n",
            "Batch : 1127|Training Loss: 0.23839950561523438|Training Accuracy : 0.875\n",
            "Batch : 1128|Training Loss: 0.19221146404743195|Training Accuracy : 0.9375\n",
            "Batch : 1129|Training Loss: 0.28689464926719666|Training Accuracy : 0.9375\n",
            "Batch : 1130|Training Loss: 0.20774444937705994|Training Accuracy : 0.9375\n",
            "Batch : 1131|Training Loss: 0.09168951958417892|Training Accuracy : 0.96875\n",
            "Batch : 1132|Training Loss: 0.12150507420301437|Training Accuracy : 0.9375\n",
            "Batch : 1133|Training Loss: 0.12189584970474243|Training Accuracy : 0.96875\n",
            "Batch : 1134|Training Loss: 0.1415594071149826|Training Accuracy : 0.9375\n",
            "Batch : 1135|Training Loss: 0.09101071208715439|Training Accuracy : 0.96875\n",
            "Batch : 1136|Training Loss: 0.2181800901889801|Training Accuracy : 0.90625\n",
            "Batch : 1137|Training Loss: 0.1996980905532837|Training Accuracy : 0.90625\n",
            "Batch : 1138|Training Loss: 0.08744325488805771|Training Accuracy : 0.96875\n",
            "Batch : 1139|Training Loss: 0.1902598738670349|Training Accuracy : 0.9375\n",
            "Batch : 1140|Training Loss: 0.1400267779827118|Training Accuracy : 0.9375\n",
            "Batch : 1141|Training Loss: 0.10931781679391861|Training Accuracy : 0.96875\n",
            "Batch : 1142|Training Loss: 0.19884948432445526|Training Accuracy : 0.9375\n",
            "Batch : 1143|Training Loss: 0.026067296043038368|Training Accuracy : 1.0\n",
            "Batch : 1144|Training Loss: 0.20047026872634888|Training Accuracy : 0.9375\n",
            "Batch : 1145|Training Loss: 0.1651470959186554|Training Accuracy : 0.9375\n",
            "Batch : 1146|Training Loss: 0.059700533747673035|Training Accuracy : 0.96875\n",
            "Batch : 1147|Training Loss: 0.057436149567365646|Training Accuracy : 1.0\n",
            "Batch : 1148|Training Loss: 0.08274643123149872|Training Accuracy : 0.96875\n",
            "Batch : 1149|Training Loss: 0.2036706954240799|Training Accuracy : 0.90625\n",
            "Batch : 1150|Training Loss: 0.14751112461090088|Training Accuracy : 0.9375\n",
            "Batch : 1151|Training Loss: 0.2041308581829071|Training Accuracy : 0.9375\n",
            "Batch : 1152|Training Loss: 0.21646124124526978|Training Accuracy : 0.875\n",
            "Batch : 1153|Training Loss: 0.14544786512851715|Training Accuracy : 0.9375\n",
            "Batch : 1154|Training Loss: 0.15906570851802826|Training Accuracy : 0.9375\n",
            "Batch : 1155|Training Loss: 0.19973218441009521|Training Accuracy : 0.9375\n",
            "Batch : 1156|Training Loss: 0.17946377396583557|Training Accuracy : 0.96875\n",
            "Batch : 1157|Training Loss: 0.0804690271615982|Training Accuracy : 1.0\n",
            "Batch : 1158|Training Loss: 0.13305166363716125|Training Accuracy : 0.9375\n",
            "Batch : 1159|Training Loss: 0.2427567094564438|Training Accuracy : 0.90625\n",
            "Batch : 1160|Training Loss: 0.22583939135074615|Training Accuracy : 0.875\n",
            "Batch : 1161|Training Loss: 0.1011807918548584|Training Accuracy : 1.0\n",
            "Batch : 1162|Training Loss: 0.06299468129873276|Training Accuracy : 0.96875\n",
            "Batch : 1163|Training Loss: 0.07766430079936981|Training Accuracy : 0.96875\n",
            "Batch : 1164|Training Loss: 0.15629994869232178|Training Accuracy : 0.9375\n",
            "Batch : 1165|Training Loss: 0.39142608642578125|Training Accuracy : 0.875\n",
            "Batch : 1166|Training Loss: 0.06642869114875793|Training Accuracy : 1.0\n",
            "Batch : 1167|Training Loss: 0.18121527135372162|Training Accuracy : 0.9375\n",
            "Batch : 1168|Training Loss: 0.11496011167764664|Training Accuracy : 0.96875\n",
            "Batch : 1169|Training Loss: 0.15028363466262817|Training Accuracy : 0.90625\n",
            "Batch : 1170|Training Loss: 0.05055860057473183|Training Accuracy : 1.0\n",
            "Batch : 1171|Training Loss: 0.07342034578323364|Training Accuracy : 0.96875\n",
            "Batch : 1172|Training Loss: 0.4091184139251709|Training Accuracy : 0.84375\n",
            "Batch : 1173|Training Loss: 0.1944720298051834|Training Accuracy : 0.875\n",
            "Batch : 1174|Training Loss: 0.2111337035894394|Training Accuracy : 0.90625\n",
            "Batch : 1175|Training Loss: 0.07586503028869629|Training Accuracy : 0.96875\n",
            "Batch : 1176|Training Loss: 0.21314755082130432|Training Accuracy : 0.90625\n",
            "Batch : 1177|Training Loss: 0.30792349576950073|Training Accuracy : 0.875\n",
            "Batch : 1178|Training Loss: 0.20578327775001526|Training Accuracy : 0.90625\n",
            "Batch : 1179|Training Loss: 0.09582781791687012|Training Accuracy : 1.0\n",
            "Batch : 1180|Training Loss: 0.16052836179733276|Training Accuracy : 0.9375\n",
            "Batch : 1181|Training Loss: 0.2872517704963684|Training Accuracy : 0.8125\n",
            "Batch : 1182|Training Loss: 0.043987058103084564|Training Accuracy : 0.96875\n",
            "Batch : 1183|Training Loss: 0.09903016686439514|Training Accuracy : 0.96875\n",
            "Batch : 1184|Training Loss: 0.4097329080104828|Training Accuracy : 0.90625\n",
            "Batch : 1185|Training Loss: 0.0585516020655632|Training Accuracy : 1.0\n",
            "Batch : 1186|Training Loss: 0.05717937648296356|Training Accuracy : 1.0\n",
            "Batch : 1187|Training Loss: 0.07723047584295273|Training Accuracy : 0.96875\n",
            "Batch : 1188|Training Loss: 0.05007989704608917|Training Accuracy : 0.96875\n",
            "Batch : 1189|Training Loss: 0.03414159268140793|Training Accuracy : 1.0\n",
            "Batch : 1190|Training Loss: 0.23965799808502197|Training Accuracy : 0.9375\n",
            "Batch : 1191|Training Loss: 0.11841742694377899|Training Accuracy : 0.9375\n",
            "Batch : 1192|Training Loss: 0.22970186173915863|Training Accuracy : 0.90625\n",
            "Batch : 1193|Training Loss: 0.08735735714435577|Training Accuracy : 1.0\n",
            "Batch : 1194|Training Loss: 0.5417358875274658|Training Accuracy : 0.875\n",
            "Batch : 1195|Training Loss: 0.23219825327396393|Training Accuracy : 0.90625\n",
            "Batch : 1196|Training Loss: 0.1423703283071518|Training Accuracy : 0.90625\n",
            "Batch : 1197|Training Loss: 0.24618811905384064|Training Accuracy : 0.9375\n",
            "Batch : 1198|Training Loss: 0.18574118614196777|Training Accuracy : 0.90625\n",
            "Batch : 1199|Training Loss: 0.47750452160835266|Training Accuracy : 0.84375\n",
            "Batch : 1200|Training Loss: 0.36499881744384766|Training Accuracy : 0.90625\n",
            "Batch : 1201|Training Loss: 0.09890224784612656|Training Accuracy : 0.9375\n",
            "Batch : 1202|Training Loss: 0.12401372194290161|Training Accuracy : 1.0\n",
            "Batch : 1203|Training Loss: 0.02203526720404625|Training Accuracy : 1.0\n",
            "Batch : 1204|Training Loss: 0.1744200885295868|Training Accuracy : 0.9375\n",
            "Batch : 1205|Training Loss: 0.44207510352134705|Training Accuracy : 0.9375\n",
            "Batch : 1206|Training Loss: 0.31158533692359924|Training Accuracy : 0.8125\n",
            "Batch : 1207|Training Loss: 0.1568157970905304|Training Accuracy : 0.90625\n",
            "Batch : 1208|Training Loss: 0.10249250382184982|Training Accuracy : 0.96875\n",
            "Batch : 1209|Training Loss: 0.21612605452537537|Training Accuracy : 0.90625\n",
            "Batch : 1210|Training Loss: 0.11380206048488617|Training Accuracy : 0.9375\n",
            "Batch : 1211|Training Loss: 0.27167800068855286|Training Accuracy : 0.90625\n",
            "Batch : 1212|Training Loss: 0.040393974632024765|Training Accuracy : 1.0\n",
            "Batch : 1213|Training Loss: 0.1012982726097107|Training Accuracy : 0.96875\n",
            "Batch : 1214|Training Loss: 0.034174542874097824|Training Accuracy : 1.0\n",
            "Batch : 1215|Training Loss: 0.07747247070074081|Training Accuracy : 1.0\n",
            "Batch : 1216|Training Loss: 0.1913938969373703|Training Accuracy : 0.90625\n",
            "Batch : 1217|Training Loss: 0.33659878373146057|Training Accuracy : 0.90625\n",
            "Batch : 1218|Training Loss: 0.3160645067691803|Training Accuracy : 0.90625\n",
            "Batch : 1219|Training Loss: 0.13578182458877563|Training Accuracy : 0.9375\n",
            "Batch : 1220|Training Loss: 0.16480699181556702|Training Accuracy : 0.90625\n",
            "Batch : 1221|Training Loss: 0.04511178657412529|Training Accuracy : 1.0\n",
            "Batch : 1222|Training Loss: 0.0865146815776825|Training Accuracy : 0.9375\n",
            "Batch : 1223|Training Loss: 0.17195653915405273|Training Accuracy : 0.96875\n",
            "Batch : 1224|Training Loss: 0.4167725145816803|Training Accuracy : 0.8125\n",
            "Batch : 1225|Training Loss: 0.20563696324825287|Training Accuracy : 0.84375\n",
            "Batch : 1226|Training Loss: 0.20045731961727142|Training Accuracy : 0.875\n",
            "Batch : 1227|Training Loss: 0.249773770570755|Training Accuracy : 0.96875\n",
            "Batch : 1228|Training Loss: 0.27214449644088745|Training Accuracy : 0.90625\n",
            "Batch : 1229|Training Loss: 0.12922850251197815|Training Accuracy : 0.96875\n",
            "Batch : 1230|Training Loss: 0.23551702499389648|Training Accuracy : 0.90625\n",
            "Batch : 1231|Training Loss: 0.15793921053409576|Training Accuracy : 0.90625\n",
            "Batch : 1232|Training Loss: 0.15341602265834808|Training Accuracy : 0.90625\n",
            "Batch : 1233|Training Loss: 0.15561696887016296|Training Accuracy : 0.9375\n",
            "Batch : 1234|Training Loss: 0.05492991581559181|Training Accuracy : 1.0\n",
            "Batch : 1235|Training Loss: 0.08017168194055557|Training Accuracy : 0.96875\n",
            "Batch : 1236|Training Loss: 0.31373974680900574|Training Accuracy : 0.84375\n",
            "Batch : 1237|Training Loss: 0.11403611302375793|Training Accuracy : 0.9375\n",
            "Batch : 1238|Training Loss: 0.23395927250385284|Training Accuracy : 0.9375\n",
            "Batch : 1239|Training Loss: 0.024167543277144432|Training Accuracy : 1.0\n",
            "Batch : 1240|Training Loss: 0.07523956894874573|Training Accuracy : 0.96875\n",
            "Batch : 1241|Training Loss: 0.1011737659573555|Training Accuracy : 0.96875\n",
            "Batch : 1242|Training Loss: 0.36324676871299744|Training Accuracy : 0.9375\n",
            "Batch : 1243|Training Loss: 0.08129352331161499|Training Accuracy : 0.96875\n",
            "Batch : 1244|Training Loss: 0.33356034755706787|Training Accuracy : 0.84375\n",
            "Batch : 1245|Training Loss: 0.186672642827034|Training Accuracy : 0.9375\n",
            "Batch : 1246|Training Loss: 0.13754355907440186|Training Accuracy : 0.90625\n",
            "Batch : 1247|Training Loss: 0.22039972245693207|Training Accuracy : 0.9375\n",
            "Batch : 1248|Training Loss: 0.1971239596605301|Training Accuracy : 0.90625\n",
            "Batch : 1249|Training Loss: 0.12584282457828522|Training Accuracy : 0.9375\n",
            "Batch : 1250|Training Loss: 0.3040553629398346|Training Accuracy : 0.84375\n",
            "Batch : 1251|Training Loss: 0.07532159239053726|Training Accuracy : 0.96875\n",
            "Batch : 1252|Training Loss: 0.16705118119716644|Training Accuracy : 0.96875\n",
            "Batch : 1253|Training Loss: 0.11769087612628937|Training Accuracy : 0.96875\n",
            "Batch : 1254|Training Loss: 0.08011462539434433|Training Accuracy : 0.96875\n",
            "Batch : 1255|Training Loss: 0.2638373374938965|Training Accuracy : 0.90625\n",
            "Batch : 1256|Training Loss: 0.16636617481708527|Training Accuracy : 0.875\n",
            "Batch : 1257|Training Loss: 0.20829840004444122|Training Accuracy : 0.9375\n",
            "Batch : 1258|Training Loss: 0.20410938560962677|Training Accuracy : 0.875\n",
            "Batch : 1259|Training Loss: 0.0679820254445076|Training Accuracy : 1.0\n",
            "Batch : 1260|Training Loss: 0.1517711877822876|Training Accuracy : 0.9375\n",
            "Batch : 1261|Training Loss: 0.12477703392505646|Training Accuracy : 0.9375\n",
            "Batch : 1262|Training Loss: 0.055392902344465256|Training Accuracy : 0.96875\n",
            "Batch : 1263|Training Loss: 0.21805758774280548|Training Accuracy : 0.90625\n",
            "Batch : 1264|Training Loss: 0.04278052598237991|Training Accuracy : 0.96875\n",
            "Batch : 1265|Training Loss: 0.037630580365657806|Training Accuracy : 1.0\n",
            "Batch : 1266|Training Loss: 0.14321590960025787|Training Accuracy : 0.96875\n",
            "Batch : 1267|Training Loss: 0.2371208816766739|Training Accuracy : 0.90625\n",
            "Batch : 1268|Training Loss: 0.21014496684074402|Training Accuracy : 0.875\n",
            "Batch : 1269|Training Loss: 0.13804680109024048|Training Accuracy : 0.9375\n",
            "Batch : 1270|Training Loss: 0.11302190274000168|Training Accuracy : 0.96875\n",
            "Batch : 1271|Training Loss: 0.1849963515996933|Training Accuracy : 0.875\n",
            "Batch : 1272|Training Loss: 0.23752614855766296|Training Accuracy : 0.9375\n",
            "Batch : 1273|Training Loss: 0.1806599497795105|Training Accuracy : 0.9375\n",
            "Batch : 1274|Training Loss: 0.06951738148927689|Training Accuracy : 1.0\n",
            "Batch : 1275|Training Loss: 0.2503200173377991|Training Accuracy : 0.90625\n",
            "Batch : 1276|Training Loss: 0.24436530470848083|Training Accuracy : 0.90625\n",
            "Batch : 1277|Training Loss: 0.10790067911148071|Training Accuracy : 0.9375\n",
            "Batch : 1278|Training Loss: 0.06809874624013901|Training Accuracy : 1.0\n",
            "Batch : 1279|Training Loss: 0.08451947569847107|Training Accuracy : 1.0\n",
            "Batch : 1280|Training Loss: 0.1919950395822525|Training Accuracy : 0.9375\n",
            "Batch : 1281|Training Loss: 0.22008489072322845|Training Accuracy : 0.96875\n",
            "Batch : 1282|Training Loss: 0.09195125102996826|Training Accuracy : 0.96875\n",
            "Batch : 1283|Training Loss: 0.30683112144470215|Training Accuracy : 0.9375\n",
            "Batch : 1284|Training Loss: 0.203652486205101|Training Accuracy : 0.9375\n",
            "Batch : 1285|Training Loss: 0.19380806386470795|Training Accuracy : 0.875\n",
            "Batch : 1286|Training Loss: 0.2550443708896637|Training Accuracy : 0.875\n",
            "Batch : 1287|Training Loss: 0.12059450149536133|Training Accuracy : 0.9375\n",
            "Batch : 1288|Training Loss: 0.1514548361301422|Training Accuracy : 0.9375\n",
            "Batch : 1289|Training Loss: 0.11151379346847534|Training Accuracy : 0.96875\n",
            "Batch : 1290|Training Loss: 0.19971546530723572|Training Accuracy : 0.875\n",
            "Batch : 1291|Training Loss: 0.16680048406124115|Training Accuracy : 0.90625\n",
            "Batch : 1292|Training Loss: 0.18666750192642212|Training Accuracy : 0.90625\n",
            "Batch : 1293|Training Loss: 0.038464464247226715|Training Accuracy : 1.0\n",
            "Batch : 1294|Training Loss: 0.11352469027042389|Training Accuracy : 0.9375\n",
            "Batch : 1295|Training Loss: 0.08143892884254456|Training Accuracy : 1.0\n",
            "Batch : 1296|Training Loss: 0.1590404212474823|Training Accuracy : 0.90625\n",
            "Batch : 1297|Training Loss: 0.2529183328151703|Training Accuracy : 0.9375\n",
            "Batch : 1298|Training Loss: 0.031623076647520065|Training Accuracy : 1.0\n",
            "Batch : 1299|Training Loss: 0.1802292913198471|Training Accuracy : 0.90625\n",
            "Batch : 1300|Training Loss: 0.10422925651073456|Training Accuracy : 0.9375\n",
            "Batch : 1301|Training Loss: 0.10969425737857819|Training Accuracy : 0.96875\n",
            "Batch : 1302|Training Loss: 0.1814974695444107|Training Accuracy : 0.90625\n",
            "Batch : 1303|Training Loss: 0.13809090852737427|Training Accuracy : 0.96875\n",
            "Batch : 1304|Training Loss: 0.32684290409088135|Training Accuracy : 0.84375\n",
            "Batch : 1305|Training Loss: 0.1954387128353119|Training Accuracy : 0.90625\n",
            "Batch : 1306|Training Loss: 0.21301791071891785|Training Accuracy : 0.90625\n",
            "Batch : 1307|Training Loss: 0.063833087682724|Training Accuracy : 1.0\n",
            "Batch : 1308|Training Loss: 0.44609519839286804|Training Accuracy : 0.90625\n",
            "Batch : 1309|Training Loss: 0.15597586333751678|Training Accuracy : 0.9375\n",
            "Batch : 1310|Training Loss: 0.09836925566196442|Training Accuracy : 0.96875\n",
            "Batch : 1311|Training Loss: 0.08585359901189804|Training Accuracy : 0.9375\n",
            "Batch : 1312|Training Loss: 0.11266828328371048|Training Accuracy : 1.0\n",
            "Batch : 1313|Training Loss: 0.19526340067386627|Training Accuracy : 0.875\n",
            "Batch : 1314|Training Loss: 0.19058042764663696|Training Accuracy : 0.90625\n",
            "Batch : 1315|Training Loss: 0.08621113747358322|Training Accuracy : 0.9375\n",
            "Batch : 1316|Training Loss: 0.2970566749572754|Training Accuracy : 0.84375\n",
            "Batch : 1317|Training Loss: 0.3999549448490143|Training Accuracy : 0.875\n",
            "Batch : 1318|Training Loss: 0.15944649279117584|Training Accuracy : 0.96875\n",
            "Batch : 1319|Training Loss: 0.20413216948509216|Training Accuracy : 0.90625\n",
            "Batch : 1320|Training Loss: 0.20209737122058868|Training Accuracy : 0.90625\n",
            "Batch : 1321|Training Loss: 0.25388240814208984|Training Accuracy : 0.875\n",
            "Batch : 1322|Training Loss: 0.15574564039707184|Training Accuracy : 0.90625\n",
            "Batch : 1323|Training Loss: 0.2724115252494812|Training Accuracy : 0.9375\n",
            "Batch : 1324|Training Loss: 0.11614841967821121|Training Accuracy : 0.9375\n",
            "Batch : 1325|Training Loss: 0.07119215279817581|Training Accuracy : 0.96875\n",
            "Batch : 1326|Training Loss: 0.1319829523563385|Training Accuracy : 0.96875\n",
            "Batch : 1327|Training Loss: 0.054614365100860596|Training Accuracy : 1.0\n",
            "Batch : 1328|Training Loss: 0.2127072513103485|Training Accuracy : 0.9375\n",
            "Batch : 1329|Training Loss: 0.16214866936206818|Training Accuracy : 0.90625\n",
            "Batch : 1330|Training Loss: 0.24158835411071777|Training Accuracy : 0.90625\n",
            "Batch : 1331|Training Loss: 0.32811564207077026|Training Accuracy : 0.9375\n",
            "Batch : 1332|Training Loss: 0.17143651843070984|Training Accuracy : 0.9375\n",
            "Batch : 1333|Training Loss: 0.10168461501598358|Training Accuracy : 0.96875\n",
            "Batch : 1334|Training Loss: 0.26809385418891907|Training Accuracy : 0.875\n",
            "Batch : 1335|Training Loss: 0.08757606148719788|Training Accuracy : 1.0\n",
            "Batch : 1336|Training Loss: 0.12303240597248077|Training Accuracy : 0.9375\n",
            "Batch : 1337|Training Loss: 0.06795425713062286|Training Accuracy : 0.96875\n",
            "Batch : 1338|Training Loss: 0.10959458351135254|Training Accuracy : 0.96875\n",
            "Batch : 1339|Training Loss: 0.09323539584875107|Training Accuracy : 0.9375\n",
            "Batch : 1340|Training Loss: 0.17380838096141815|Training Accuracy : 0.90625\n",
            "Batch : 1341|Training Loss: 0.021244851872324944|Training Accuracy : 1.0\n",
            "Batch : 1342|Training Loss: 0.053387854248285294|Training Accuracy : 1.0\n",
            "Batch : 1343|Training Loss: 0.2221873551607132|Training Accuracy : 0.90625\n",
            "Batch : 1344|Training Loss: 0.07923755049705505|Training Accuracy : 0.96875\n",
            "Batch : 1345|Training Loss: 0.27711057662963867|Training Accuracy : 0.90625\n",
            "Batch : 1346|Training Loss: 0.06482414901256561|Training Accuracy : 0.96875\n",
            "Batch : 1347|Training Loss: 0.149083212018013|Training Accuracy : 0.96875\n",
            "Batch : 1348|Training Loss: 0.10331616550683975|Training Accuracy : 0.96875\n",
            "Batch : 1349|Training Loss: 0.06560417264699936|Training Accuracy : 0.96875\n",
            "Batch : 1350|Training Loss: 0.29504621028900146|Training Accuracy : 0.875\n",
            "Batch : 1351|Training Loss: 0.09779799729585648|Training Accuracy : 1.0\n",
            "Batch : 1352|Training Loss: 0.10978588461875916|Training Accuracy : 0.96875\n",
            "Batch : 1353|Training Loss: 0.2975793778896332|Training Accuracy : 0.9375\n",
            "Batch : 1354|Training Loss: 0.10721568763256073|Training Accuracy : 1.0\n",
            "Batch : 1355|Training Loss: 0.2254818081855774|Training Accuracy : 0.9375\n",
            "Batch : 1356|Training Loss: 0.17051327228546143|Training Accuracy : 0.96875\n",
            "Batch : 1357|Training Loss: 0.4433191418647766|Training Accuracy : 0.875\n",
            "Batch : 1358|Training Loss: 0.08632088452577591|Training Accuracy : 0.96875\n",
            "Batch : 1359|Training Loss: 0.30014586448669434|Training Accuracy : 0.84375\n",
            "Batch : 1360|Training Loss: 0.3638722896575928|Training Accuracy : 0.84375\n",
            "Batch : 1361|Training Loss: 0.2327088713645935|Training Accuracy : 0.9375\n",
            "Batch : 1362|Training Loss: 0.18158407509326935|Training Accuracy : 0.96875\n",
            "Batch : 1363|Training Loss: 0.14044101536273956|Training Accuracy : 0.9375\n",
            "Batch : 1364|Training Loss: 0.08201368898153305|Training Accuracy : 0.96875\n",
            "Batch : 1365|Training Loss: 0.31057628989219666|Training Accuracy : 0.875\n",
            "Batch : 1366|Training Loss: 0.1778426617383957|Training Accuracy : 0.90625\n",
            "Batch : 1367|Training Loss: 0.3412960171699524|Training Accuracy : 0.875\n",
            "Batch : 1368|Training Loss: 0.04886883869767189|Training Accuracy : 0.96875\n",
            "Batch : 1369|Training Loss: 0.1360599547624588|Training Accuracy : 0.9375\n",
            "Batch : 1370|Training Loss: 0.20816825330257416|Training Accuracy : 0.9375\n",
            "Batch : 1371|Training Loss: 0.07955192774534225|Training Accuracy : 0.96875\n",
            "Batch : 1372|Training Loss: 0.05451071262359619|Training Accuracy : 0.96875\n",
            "Batch : 1373|Training Loss: 0.1959584653377533|Training Accuracy : 0.90625\n",
            "Batch : 1374|Training Loss: 0.09138323366641998|Training Accuracy : 0.96875\n",
            "Batch : 1375|Training Loss: 0.015307380817830563|Training Accuracy : 1.0\n",
            "Batch : 1376|Training Loss: 0.02612265758216381|Training Accuracy : 1.0\n",
            "Batch : 1377|Training Loss: 0.15557432174682617|Training Accuracy : 0.96875\n",
            "Batch : 1378|Training Loss: 0.030342401936650276|Training Accuracy : 1.0\n",
            "Batch : 1379|Training Loss: 0.0738106518983841|Training Accuracy : 0.96875\n",
            "Batch : 1380|Training Loss: 0.18713606894016266|Training Accuracy : 0.875\n",
            "Batch : 1381|Training Loss: 0.13494230806827545|Training Accuracy : 0.9375\n",
            "Batch : 1382|Training Loss: 0.09837161004543304|Training Accuracy : 0.9375\n",
            "Batch : 1383|Training Loss: 0.09047649055719376|Training Accuracy : 1.0\n",
            "Batch : 1384|Training Loss: 0.20347239077091217|Training Accuracy : 0.90625\n",
            "Batch : 1385|Training Loss: 0.2459728717803955|Training Accuracy : 0.90625\n",
            "Batch : 1386|Training Loss: 0.21360954642295837|Training Accuracy : 0.90625\n",
            "Batch : 1387|Training Loss: 0.2031250149011612|Training Accuracy : 0.9375\n",
            "Batch : 1388|Training Loss: 0.09891532361507416|Training Accuracy : 0.9375\n",
            "Batch : 1389|Training Loss: 0.10707855224609375|Training Accuracy : 0.9375\n",
            "Batch : 1390|Training Loss: 0.512495219707489|Training Accuracy : 0.875\n",
            "Batch : 1391|Training Loss: 0.2240491360425949|Training Accuracy : 0.90625\n",
            "Batch : 1392|Training Loss: 0.019532324746251106|Training Accuracy : 1.0\n",
            "Batch : 1393|Training Loss: 0.3022859990596771|Training Accuracy : 0.875\n",
            "Batch : 1394|Training Loss: 0.11697131395339966|Training Accuracy : 0.9375\n",
            "Batch : 1395|Training Loss: 0.2685509920120239|Training Accuracy : 0.90625\n",
            "Batch : 1396|Training Loss: 0.2922345995903015|Training Accuracy : 0.90625\n",
            "Batch : 1397|Training Loss: 0.21732927858829498|Training Accuracy : 0.875\n",
            "Batch : 1398|Training Loss: 0.2746385931968689|Training Accuracy : 0.90625\n",
            "Batch : 1399|Training Loss: 0.41125616431236267|Training Accuracy : 0.8125\n",
            "Batch : 1400|Training Loss: 0.13213679194450378|Training Accuracy : 0.96875\n",
            "Batch : 1401|Training Loss: 0.22679650783538818|Training Accuracy : 0.875\n",
            "Batch : 1402|Training Loss: 0.16138148307800293|Training Accuracy : 0.9375\n",
            "Batch : 1403|Training Loss: 0.37299033999443054|Training Accuracy : 0.90625\n",
            "Batch : 1404|Training Loss: 0.16760101914405823|Training Accuracy : 0.9375\n",
            "Batch : 1405|Training Loss: 0.19241565465927124|Training Accuracy : 0.96875\n",
            "Batch : 1406|Training Loss: 0.312723845243454|Training Accuracy : 0.875\n",
            "Batch : 1407|Training Loss: 0.3356914222240448|Training Accuracy : 0.875\n",
            "Batch : 1408|Training Loss: 0.12057190388441086|Training Accuracy : 0.96875\n",
            "Batch : 1409|Training Loss: 0.1647704839706421|Training Accuracy : 0.9375\n",
            "Batch : 1410|Training Loss: 0.14538142085075378|Training Accuracy : 0.96875\n",
            "Batch : 1411|Training Loss: 0.14420728385448456|Training Accuracy : 0.9375\n",
            "Batch : 1412|Training Loss: 0.22444623708724976|Training Accuracy : 0.84375\n",
            "Batch : 1413|Training Loss: 0.3170592188835144|Training Accuracy : 0.875\n",
            "Batch : 1414|Training Loss: 0.24121659994125366|Training Accuracy : 0.875\n",
            "Batch : 1415|Training Loss: 0.28704631328582764|Training Accuracy : 0.875\n",
            "Batch : 1416|Training Loss: 0.27479615807533264|Training Accuracy : 0.84375\n",
            "Batch : 1417|Training Loss: 0.5386465787887573|Training Accuracy : 0.84375\n",
            "Batch : 1418|Training Loss: 0.16150715947151184|Training Accuracy : 0.90625\n",
            "Batch : 1419|Training Loss: 0.5147970914840698|Training Accuracy : 0.90625\n",
            "Batch : 1420|Training Loss: 0.07218093425035477|Training Accuracy : 0.96875\n",
            "Batch : 1421|Training Loss: 0.4191052317619324|Training Accuracy : 0.9375\n",
            "Batch : 1422|Training Loss: 0.08155486732721329|Training Accuracy : 0.96875\n",
            "Batch : 1423|Training Loss: 0.21977663040161133|Training Accuracy : 0.875\n",
            "Batch : 1424|Training Loss: 0.02604323998093605|Training Accuracy : 1.0\n",
            "Batch : 1425|Training Loss: 0.16014394164085388|Training Accuracy : 0.9375\n",
            "Batch : 1426|Training Loss: 0.16961267590522766|Training Accuracy : 0.9375\n",
            "Batch : 1427|Training Loss: 0.1504155546426773|Training Accuracy : 0.9375\n",
            "Batch : 1428|Training Loss: 0.17881974577903748|Training Accuracy : 0.875\n",
            "Batch : 1429|Training Loss: 0.1686410754919052|Training Accuracy : 0.9375\n",
            "Batch : 1430|Training Loss: 0.19655758142471313|Training Accuracy : 0.9375\n",
            "Batch : 1431|Training Loss: 0.16781389713287354|Training Accuracy : 0.90625\n",
            "Batch : 1432|Training Loss: 0.24650709331035614|Training Accuracy : 0.875\n",
            "Batch : 1433|Training Loss: 0.45293521881103516|Training Accuracy : 0.90625\n",
            "Batch : 1434|Training Loss: 0.06493417173624039|Training Accuracy : 1.0\n",
            "Batch : 1435|Training Loss: 0.10484760999679565|Training Accuracy : 0.9375\n",
            "Batch : 1436|Training Loss: 0.5209033489227295|Training Accuracy : 0.875\n",
            "Batch : 1437|Training Loss: 0.14773036539554596|Training Accuracy : 0.9375\n",
            "Batch : 1438|Training Loss: 0.08918649703264236|Training Accuracy : 0.96875\n",
            "Batch : 1439|Training Loss: 0.31355637311935425|Training Accuracy : 0.875\n",
            "Batch : 1440|Training Loss: 0.08667357265949249|Training Accuracy : 1.0\n",
            "Batch : 1441|Training Loss: 0.19457505643367767|Training Accuracy : 0.9375\n",
            "Batch : 1442|Training Loss: 0.14529862999916077|Training Accuracy : 0.90625\n",
            "Batch : 1443|Training Loss: 0.346610963344574|Training Accuracy : 0.875\n",
            "Batch : 1444|Training Loss: 0.4037999212741852|Training Accuracy : 0.875\n",
            "Batch : 1445|Training Loss: 0.31468188762664795|Training Accuracy : 0.84375\n",
            "Batch : 1446|Training Loss: 0.6280073523521423|Training Accuracy : 0.84375\n",
            "Batch : 1447|Training Loss: 0.11761423945426941|Training Accuracy : 0.96875\n",
            "Batch : 1448|Training Loss: 0.10587459802627563|Training Accuracy : 0.9375\n",
            "Batch : 1449|Training Loss: 0.28530237078666687|Training Accuracy : 0.90625\n",
            "Batch : 1450|Training Loss: 0.08064795285463333|Training Accuracy : 1.0\n",
            "Batch : 1451|Training Loss: 0.09536981582641602|Training Accuracy : 0.9375\n",
            "Batch : 1452|Training Loss: 0.2601611614227295|Training Accuracy : 0.875\n",
            "Batch : 1453|Training Loss: 0.177277609705925|Training Accuracy : 0.90625\n",
            "Batch : 1454|Training Loss: 0.1321483850479126|Training Accuracy : 0.96875\n",
            "Batch : 1455|Training Loss: 0.19012756645679474|Training Accuracy : 0.9375\n",
            "Batch : 1456|Training Loss: 0.17622491717338562|Training Accuracy : 0.9375\n",
            "Batch : 1457|Training Loss: 0.26960548758506775|Training Accuracy : 0.875\n",
            "Batch : 1458|Training Loss: 0.23441261053085327|Training Accuracy : 0.9375\n",
            "Batch : 1459|Training Loss: 0.16495275497436523|Training Accuracy : 0.90625\n",
            "Batch : 1460|Training Loss: 0.2071004956960678|Training Accuracy : 0.875\n",
            "Batch : 1461|Training Loss: 0.2738436758518219|Training Accuracy : 0.90625\n",
            "Batch : 1462|Training Loss: 0.18269526958465576|Training Accuracy : 0.9375\n",
            "Batch : 1463|Training Loss: 0.05020713806152344|Training Accuracy : 1.0\n",
            "Batch : 1464|Training Loss: 0.29483720660209656|Training Accuracy : 0.875\n",
            "Batch : 1465|Training Loss: 0.2492751181125641|Training Accuracy : 0.90625\n",
            "Batch : 1466|Training Loss: 0.11858931183815002|Training Accuracy : 0.9375\n",
            "Batch : 1467|Training Loss: 0.1584601253271103|Training Accuracy : 0.9375\n",
            "Batch : 1468|Training Loss: 0.03901464119553566|Training Accuracy : 1.0\n",
            "Batch : 1469|Training Loss: 0.09578537940979004|Training Accuracy : 0.9375\n",
            "Batch : 1470|Training Loss: 0.32495659589767456|Training Accuracy : 0.875\n",
            "Batch : 1471|Training Loss: 0.22349153459072113|Training Accuracy : 0.9375\n",
            "Batch : 1472|Training Loss: 0.2190595120191574|Training Accuracy : 0.875\n",
            "Batch : 1473|Training Loss: 0.42225754261016846|Training Accuracy : 0.9375\n",
            "Batch : 1474|Training Loss: 0.18364666402339935|Training Accuracy : 0.90625\n",
            "Batch : 1475|Training Loss: 0.21202291548252106|Training Accuracy : 0.9375\n",
            "Batch : 1476|Training Loss: 0.08114655315876007|Training Accuracy : 0.96875\n",
            "Batch : 1477|Training Loss: 0.17084328830242157|Training Accuracy : 0.90625\n",
            "Batch : 1478|Training Loss: 0.12934017181396484|Training Accuracy : 0.9375\n",
            "Batch : 1479|Training Loss: 0.07739219069480896|Training Accuracy : 0.96875\n",
            "Batch : 1480|Training Loss: 0.13387852907180786|Training Accuracy : 0.90625\n",
            "Batch : 1481|Training Loss: 0.15658096969127655|Training Accuracy : 0.9375\n",
            "Batch : 1482|Training Loss: 0.20543017983436584|Training Accuracy : 0.9375\n",
            "Batch : 1483|Training Loss: 0.11525281518697739|Training Accuracy : 0.9375\n",
            "Batch : 1484|Training Loss: 0.11625057458877563|Training Accuracy : 1.0\n",
            "Batch : 1485|Training Loss: 0.2074677050113678|Training Accuracy : 0.9375\n",
            "Batch : 1486|Training Loss: 0.12775655090808868|Training Accuracy : 0.9375\n",
            "Batch : 1487|Training Loss: 0.12451332807540894|Training Accuracy : 0.96875\n",
            "Batch : 1488|Training Loss: 0.057598795741796494|Training Accuracy : 0.96875\n",
            "Batch : 1489|Training Loss: 0.08086982369422913|Training Accuracy : 0.96875\n",
            "Batch : 1490|Training Loss: 0.13441430032253265|Training Accuracy : 0.96875\n",
            "Batch : 1491|Training Loss: 0.03644394129514694|Training Accuracy : 1.0\n",
            "Batch : 1492|Training Loss: 0.13210389018058777|Training Accuracy : 0.9375\n",
            "Batch : 1493|Training Loss: 0.15148703753948212|Training Accuracy : 0.90625\n",
            "Batch : 1494|Training Loss: 0.10843124985694885|Training Accuracy : 0.96875\n",
            "Batch : 1495|Training Loss: 0.06523879617452621|Training Accuracy : 1.0\n",
            "Batch : 1496|Training Loss: 0.23864172399044037|Training Accuracy : 0.90625\n",
            "Batch : 1497|Training Loss: 0.12244507670402527|Training Accuracy : 0.9375\n",
            "Batch : 1498|Training Loss: 0.4022553265094757|Training Accuracy : 0.90625\n",
            "Batch : 1499|Training Loss: 0.0629270002245903|Training Accuracy : 0.96875\n",
            "Batch : 1500|Training Loss: 0.2595561742782593|Training Accuracy : 0.90625\n",
            "Batch : 1501|Training Loss: 0.1829942911863327|Training Accuracy : 0.9375\n",
            "Batch : 1502|Training Loss: 0.15225668251514435|Training Accuracy : 0.9375\n",
            "Batch : 1503|Training Loss: 0.08507687598466873|Training Accuracy : 0.96875\n",
            "Batch : 1504|Training Loss: 0.09621161222457886|Training Accuracy : 0.96875\n",
            "Batch : 1505|Training Loss: 0.32073718309402466|Training Accuracy : 0.90625\n",
            "Batch : 1506|Training Loss: 0.22312423586845398|Training Accuracy : 0.90625\n",
            "Batch : 1507|Training Loss: 0.07962670922279358|Training Accuracy : 0.96875\n",
            "Batch : 1508|Training Loss: 0.2053159773349762|Training Accuracy : 0.875\n",
            "Batch : 1509|Training Loss: 0.09651727974414825|Training Accuracy : 0.96875\n",
            "Batch : 1510|Training Loss: 0.2600085139274597|Training Accuracy : 0.90625\n",
            "Batch : 1511|Training Loss: 0.19278192520141602|Training Accuracy : 0.9375\n",
            "Batch : 1512|Training Loss: 0.3611859679222107|Training Accuracy : 0.875\n",
            "Batch : 1513|Training Loss: 0.11437414586544037|Training Accuracy : 0.96875\n",
            "Batch : 1514|Training Loss: 0.10877260565757751|Training Accuracy : 0.96875\n",
            "Batch : 1515|Training Loss: 0.07012837380170822|Training Accuracy : 1.0\n",
            "Batch : 1516|Training Loss: 0.2174026221036911|Training Accuracy : 0.90625\n",
            "Batch : 1517|Training Loss: 0.2415013611316681|Training Accuracy : 0.96875\n",
            "Batch : 1518|Training Loss: 0.10901010036468506|Training Accuracy : 0.96875\n",
            "Batch : 1519|Training Loss: 0.20679767429828644|Training Accuracy : 0.875\n",
            "Batch : 1520|Training Loss: 0.14241749048233032|Training Accuracy : 0.9375\n",
            "Batch : 1521|Training Loss: 0.22676189243793488|Training Accuracy : 0.90625\n",
            "Batch : 1522|Training Loss: 0.1135079637169838|Training Accuracy : 0.96875\n",
            "Batch : 1523|Training Loss: 0.49325743317604065|Training Accuracy : 0.90625\n",
            "Batch : 1524|Training Loss: 0.17872296273708344|Training Accuracy : 0.875\n",
            "Batch : 1525|Training Loss: 0.36969947814941406|Training Accuracy : 0.875\n",
            "Batch : 1526|Training Loss: 0.10148359835147858|Training Accuracy : 1.0\n",
            "Batch : 1527|Training Loss: 0.08585206419229507|Training Accuracy : 0.96875\n",
            "Batch : 1528|Training Loss: 0.154597669839859|Training Accuracy : 0.9375\n",
            "Batch : 1529|Training Loss: 0.09532473236322403|Training Accuracy : 0.96875\n",
            "Batch : 1530|Training Loss: 0.13799399137496948|Training Accuracy : 0.96875\n",
            "Batch : 1531|Training Loss: 0.21156816184520721|Training Accuracy : 0.90625\n",
            "Batch : 1532|Training Loss: 0.19912397861480713|Training Accuracy : 0.9375\n",
            "Batch : 1533|Training Loss: 0.11741446703672409|Training Accuracy : 0.96875\n",
            "Batch : 1534|Training Loss: 0.32711201906204224|Training Accuracy : 0.84375\n",
            "Batch : 1535|Training Loss: 0.3205190896987915|Training Accuracy : 0.90625\n",
            "Batch : 1536|Training Loss: 0.11492152512073517|Training Accuracy : 0.96875\n",
            "Batch : 1537|Training Loss: 0.3285368084907532|Training Accuracy : 0.90625\n",
            "Batch : 1538|Training Loss: 0.11563435196876526|Training Accuracy : 0.9375\n",
            "Batch : 1539|Training Loss: 0.19555120170116425|Training Accuracy : 0.90625\n",
            "Batch : 1540|Training Loss: 0.32574549317359924|Training Accuracy : 0.875\n",
            "Batch : 1541|Training Loss: 0.22330662608146667|Training Accuracy : 0.90625\n",
            "Batch : 1542|Training Loss: 0.1056196391582489|Training Accuracy : 1.0\n",
            "Batch : 1543|Training Loss: 0.06577768176794052|Training Accuracy : 1.0\n",
            "Batch : 1544|Training Loss: 0.6199022531509399|Training Accuracy : 0.8125\n",
            "Batch : 1545|Training Loss: 0.06137045845389366|Training Accuracy : 0.96875\n",
            "Batch : 1546|Training Loss: 0.2788156270980835|Training Accuracy : 0.875\n",
            "Batch : 1547|Training Loss: 0.3424130976200104|Training Accuracy : 0.8125\n",
            "Batch : 1548|Training Loss: 0.28606054186820984|Training Accuracy : 0.90625\n",
            "Batch : 1549|Training Loss: 0.19447392225265503|Training Accuracy : 0.90625\n",
            "Batch : 1550|Training Loss: 0.2583540081977844|Training Accuracy : 0.875\n",
            "Batch : 1551|Training Loss: 0.13086389005184174|Training Accuracy : 0.96875\n",
            "Batch : 1552|Training Loss: 0.17019402980804443|Training Accuracy : 0.9375\n",
            "Batch : 1553|Training Loss: 0.23236389458179474|Training Accuracy : 0.9375\n",
            "Batch : 1554|Training Loss: 0.20403946936130524|Training Accuracy : 0.9375\n",
            "Batch : 1555|Training Loss: 0.2079649120569229|Training Accuracy : 0.96875\n",
            "Batch : 1556|Training Loss: 0.08716936409473419|Training Accuracy : 0.96875\n",
            "Batch : 1557|Training Loss: 0.14570064842700958|Training Accuracy : 0.9375\n",
            "Batch : 1558|Training Loss: 0.031706131994724274|Training Accuracy : 1.0\n",
            "Batch : 1559|Training Loss: 0.07420093566179276|Training Accuracy : 1.0\n",
            "Batch : 1560|Training Loss: 0.16137826442718506|Training Accuracy : 0.9375\n",
            "Batch : 1561|Training Loss: 0.3400344252586365|Training Accuracy : 0.875\n",
            "Batch : 1562|Training Loss: 0.2518984079360962|Training Accuracy : 0.9375\n",
            "Batch : 1563|Training Loss: 0.11160153150558472|Training Accuracy : 0.96875\n",
            "Batch : 1564|Training Loss: 0.14802643656730652|Training Accuracy : 0.9375\n",
            "Batch : 1565|Training Loss: 0.22416529059410095|Training Accuracy : 0.9375\n",
            "Batch : 1566|Training Loss: 0.07394003868103027|Training Accuracy : 0.96875\n",
            "Batch : 1567|Training Loss: 0.1544305831193924|Training Accuracy : 0.9375\n",
            "Batch : 1568|Training Loss: 0.20337606966495514|Training Accuracy : 0.9375\n",
            "Batch : 1569|Training Loss: 0.2802142798900604|Training Accuracy : 0.9375\n",
            "Batch : 1570|Training Loss: 0.06365982443094254|Training Accuracy : 0.96875\n",
            "Batch : 1571|Training Loss: 0.22143031656742096|Training Accuracy : 0.9375\n",
            "Batch : 1572|Training Loss: 0.2095889449119568|Training Accuracy : 0.9375\n",
            "Batch : 1573|Training Loss: 0.07127375155687332|Training Accuracy : 1.0\n",
            "Batch : 1574|Training Loss: 0.09801189601421356|Training Accuracy : 0.96875\n",
            "Batch : 1575|Training Loss: 0.18656599521636963|Training Accuracy : 0.9375\n",
            "Batch : 1576|Training Loss: 0.1555410772562027|Training Accuracy : 0.9375\n",
            "Batch : 1577|Training Loss: 0.12426969408988953|Training Accuracy : 0.96875\n",
            "Batch : 1578|Training Loss: 0.058453939855098724|Training Accuracy : 1.0\n",
            "Batch : 1579|Training Loss: 0.10736369341611862|Training Accuracy : 0.9375\n",
            "Batch : 1580|Training Loss: 0.13527609407901764|Training Accuracy : 0.96875\n",
            "Batch : 1581|Training Loss: 0.3579404950141907|Training Accuracy : 0.875\n",
            "Batch : 1582|Training Loss: 0.261525422334671|Training Accuracy : 0.875\n",
            "Batch : 1583|Training Loss: 0.06271597743034363|Training Accuracy : 0.96875\n",
            "Batch : 1584|Training Loss: 0.11550422012805939|Training Accuracy : 1.0\n",
            "Batch : 1585|Training Loss: 0.20491668581962585|Training Accuracy : 0.9375\n",
            "Batch : 1586|Training Loss: 0.1666545569896698|Training Accuracy : 0.96875\n",
            "Batch : 1587|Training Loss: 0.04775843024253845|Training Accuracy : 1.0\n",
            "Batch : 1588|Training Loss: 0.21184830367565155|Training Accuracy : 0.90625\n",
            "Batch : 1589|Training Loss: 0.2520410120487213|Training Accuracy : 0.90625\n",
            "Batch : 1590|Training Loss: 0.23305830359458923|Training Accuracy : 0.9375\n",
            "Batch : 1591|Training Loss: 0.21600650250911713|Training Accuracy : 0.90625\n",
            "Batch : 1592|Training Loss: 0.5604386329650879|Training Accuracy : 0.75\n",
            "Batch : 1593|Training Loss: 0.054183900356292725|Training Accuracy : 0.96875\n",
            "Batch : 1594|Training Loss: 0.16029268503189087|Training Accuracy : 0.9375\n",
            "Batch : 1595|Training Loss: 0.08252318203449249|Training Accuracy : 1.0\n",
            "Batch : 1596|Training Loss: 0.11155146360397339|Training Accuracy : 0.9375\n",
            "Batch : 1597|Training Loss: 0.22101053595542908|Training Accuracy : 0.9375\n",
            "Batch : 1598|Training Loss: 0.3775464594364166|Training Accuracy : 0.875\n",
            "Batch : 1599|Training Loss: 0.04740769788622856|Training Accuracy : 0.96875\n",
            "Batch : 1600|Training Loss: 0.1161855161190033|Training Accuracy : 0.9375\n",
            "Batch : 1601|Training Loss: 0.12145256251096725|Training Accuracy : 0.96875\n",
            "Batch : 1602|Training Loss: 0.046344317495822906|Training Accuracy : 1.0\n",
            "Batch : 1603|Training Loss: 0.15943875908851624|Training Accuracy : 0.9375\n",
            "Batch : 1604|Training Loss: 0.1109732910990715|Training Accuracy : 0.96875\n",
            "Batch : 1605|Training Loss: 0.16666781902313232|Training Accuracy : 0.96875\n",
            "Batch : 1606|Training Loss: 0.5702904462814331|Training Accuracy : 0.875\n",
            "Batch : 1607|Training Loss: 0.21713414788246155|Training Accuracy : 0.90625\n",
            "Batch : 1608|Training Loss: 0.22417981922626495|Training Accuracy : 0.90625\n",
            "Batch : 1609|Training Loss: 0.2326643019914627|Training Accuracy : 0.9375\n",
            "Batch : 1610|Training Loss: 0.2632448375225067|Training Accuracy : 0.84375\n",
            "Batch : 1611|Training Loss: 0.1727280169725418|Training Accuracy : 0.9375\n",
            "Batch : 1612|Training Loss: 0.11129669100046158|Training Accuracy : 0.96875\n",
            "Batch : 1613|Training Loss: 0.3683713972568512|Training Accuracy : 0.84375\n",
            "Batch : 1614|Training Loss: 0.14724841713905334|Training Accuracy : 0.9375\n",
            "Batch : 1615|Training Loss: 0.1418154388666153|Training Accuracy : 0.875\n",
            "Batch : 1616|Training Loss: 0.1335889995098114|Training Accuracy : 0.96875\n",
            "Batch : 1617|Training Loss: 0.25130248069763184|Training Accuracy : 0.84375\n",
            "Batch : 1618|Training Loss: 0.06461696326732635|Training Accuracy : 1.0\n",
            "Batch : 1619|Training Loss: 0.10883652418851852|Training Accuracy : 0.9375\n",
            "Batch : 1620|Training Loss: 0.2009996771812439|Training Accuracy : 0.90625\n",
            "Batch : 1621|Training Loss: 0.20959706604480743|Training Accuracy : 0.9375\n",
            "Batch : 1622|Training Loss: 0.135700985789299|Training Accuracy : 0.9375\n",
            "Batch : 1623|Training Loss: 0.19138696789741516|Training Accuracy : 0.90625\n",
            "Batch : 1624|Training Loss: 0.3242492079734802|Training Accuracy : 0.9375\n",
            "Batch : 1625|Training Loss: 0.11437245458364487|Training Accuracy : 0.9375\n",
            "Batch : 1626|Training Loss: 0.1953945755958557|Training Accuracy : 0.96875\n",
            "Batch : 1627|Training Loss: 0.29012638330459595|Training Accuracy : 0.90625\n",
            "Batch : 1628|Training Loss: 0.05082818865776062|Training Accuracy : 1.0\n",
            "Batch : 1629|Training Loss: 0.238601952791214|Training Accuracy : 0.90625\n",
            "Batch : 1630|Training Loss: 0.15221035480499268|Training Accuracy : 0.90625\n",
            "Batch : 1631|Training Loss: 0.2987866699695587|Training Accuracy : 0.875\n",
            "Batch : 1632|Training Loss: 0.10812944173812866|Training Accuracy : 0.96875\n",
            "Batch : 1633|Training Loss: 0.09628469496965408|Training Accuracy : 0.96875\n",
            "Batch : 1634|Training Loss: 0.19016127288341522|Training Accuracy : 0.90625\n",
            "Batch : 1635|Training Loss: 0.10210227221250534|Training Accuracy : 0.96875\n",
            "Batch : 1636|Training Loss: 0.21815283596515656|Training Accuracy : 0.9375\n",
            "Batch : 1637|Training Loss: 0.44263413548469543|Training Accuracy : 0.78125\n",
            "Batch : 1638|Training Loss: 0.23869384825229645|Training Accuracy : 0.90625\n",
            "Batch : 1639|Training Loss: 0.28770631551742554|Training Accuracy : 0.875\n",
            "Batch : 1640|Training Loss: 0.142471045255661|Training Accuracy : 0.96875\n",
            "Batch : 1641|Training Loss: 0.16640369594097137|Training Accuracy : 0.9375\n",
            "Batch : 1642|Training Loss: 0.2632519602775574|Training Accuracy : 0.875\n",
            "Batch : 1643|Training Loss: 0.10683295130729675|Training Accuracy : 0.96875\n",
            "Batch : 1644|Training Loss: 0.4141314923763275|Training Accuracy : 0.875\n",
            "Batch : 1645|Training Loss: 0.2621517479419708|Training Accuracy : 0.9375\n",
            "Batch : 1646|Training Loss: 0.10346949845552444|Training Accuracy : 0.96875\n",
            "Batch : 1647|Training Loss: 0.15902312099933624|Training Accuracy : 0.9375\n",
            "Batch : 1648|Training Loss: 0.04858523979783058|Training Accuracy : 1.0\n",
            "Batch : 1649|Training Loss: 0.14176201820373535|Training Accuracy : 0.9375\n",
            "Batch : 1650|Training Loss: 0.1123688593506813|Training Accuracy : 0.96875\n",
            "Batch : 1651|Training Loss: 0.06514303386211395|Training Accuracy : 1.0\n",
            "Batch : 1652|Training Loss: 0.21436311304569244|Training Accuracy : 0.9375\n",
            "Batch : 1653|Training Loss: 0.14177639782428741|Training Accuracy : 0.96875\n",
            "Batch : 1654|Training Loss: 0.26790663599967957|Training Accuracy : 0.90625\n",
            "Batch : 1655|Training Loss: 0.13908040523529053|Training Accuracy : 0.96875\n",
            "Batch : 1656|Training Loss: 0.3598872125148773|Training Accuracy : 0.875\n",
            "Batch : 1657|Training Loss: 0.08323808014392853|Training Accuracy : 0.96875\n",
            "Batch : 1658|Training Loss: 0.15667563676834106|Training Accuracy : 0.90625\n",
            "Batch : 1659|Training Loss: 0.1133604571223259|Training Accuracy : 0.9375\n",
            "Batch : 1660|Training Loss: 0.06851721554994583|Training Accuracy : 0.96875\n",
            "Batch : 1661|Training Loss: 0.20182086527347565|Training Accuracy : 0.90625\n",
            "Batch : 1662|Training Loss: 0.3435191810131073|Training Accuracy : 0.90625\n",
            "Batch : 1663|Training Loss: 0.02643732912838459|Training Accuracy : 1.0\n",
            "Batch : 1664|Training Loss: 0.19704854488372803|Training Accuracy : 0.90625\n",
            "Batch : 1665|Training Loss: 0.10186202079057693|Training Accuracy : 0.9375\n",
            "Batch : 1666|Training Loss: 0.19934113323688507|Training Accuracy : 0.90625\n",
            "Batch : 1667|Training Loss: 0.1566513031721115|Training Accuracy : 0.90625\n",
            "Batch : 1668|Training Loss: 0.06364709138870239|Training Accuracy : 1.0\n",
            "Batch : 1669|Training Loss: 0.43882572650909424|Training Accuracy : 0.875\n",
            "Batch : 1670|Training Loss: 0.10823310166597366|Training Accuracy : 0.96875\n",
            "Batch : 1671|Training Loss: 0.22959186136722565|Training Accuracy : 0.90625\n",
            "Batch : 1672|Training Loss: 0.2699381113052368|Training Accuracy : 0.90625\n",
            "Batch : 1673|Training Loss: 0.23682372272014618|Training Accuracy : 0.90625\n",
            "Batch : 1674|Training Loss: 0.06547986716032028|Training Accuracy : 0.96875\n",
            "Batch : 1675|Training Loss: 0.36128664016723633|Training Accuracy : 0.875\n",
            "Batch : 1676|Training Loss: 0.1334180384874344|Training Accuracy : 0.96875\n",
            "Batch : 1677|Training Loss: 0.14547757804393768|Training Accuracy : 0.9375\n",
            "Batch : 1678|Training Loss: 0.1893850713968277|Training Accuracy : 0.90625\n",
            "Batch : 1679|Training Loss: 0.09430020302534103|Training Accuracy : 1.0\n",
            "Batch : 1680|Training Loss: 0.10812840610742569|Training Accuracy : 0.96875\n",
            "Batch : 1681|Training Loss: 0.21198497712612152|Training Accuracy : 0.90625\n",
            "Batch : 1682|Training Loss: 0.12447512894868851|Training Accuracy : 0.96875\n",
            "Batch : 1683|Training Loss: 0.22175270318984985|Training Accuracy : 0.84375\n",
            "Batch : 1684|Training Loss: 0.09195882081985474|Training Accuracy : 0.96875\n",
            "Batch : 1685|Training Loss: 0.09621265530586243|Training Accuracy : 0.96875\n",
            "Batch : 1686|Training Loss: 0.34532561898231506|Training Accuracy : 0.875\n",
            "Batch : 1687|Training Loss: 0.19448864459991455|Training Accuracy : 0.90625\n",
            "Batch : 1688|Training Loss: 0.04964348301291466|Training Accuracy : 1.0\n",
            "Batch : 1689|Training Loss: 0.06599412113428116|Training Accuracy : 0.96875\n",
            "Batch : 1690|Training Loss: 0.11226539313793182|Training Accuracy : 0.9375\n",
            "Batch : 1691|Training Loss: 0.13610748946666718|Training Accuracy : 0.96875\n",
            "Batch : 1692|Training Loss: 0.31703031063079834|Training Accuracy : 0.90625\n",
            "Batch : 1693|Training Loss: 0.07537955790758133|Training Accuracy : 0.9375\n",
            "Batch : 1694|Training Loss: 0.2218712419271469|Training Accuracy : 0.875\n",
            "Batch : 1695|Training Loss: 0.11261860281229019|Training Accuracy : 0.96875\n",
            "Batch : 1696|Training Loss: 0.3354582190513611|Training Accuracy : 0.875\n",
            "Batch : 1697|Training Loss: 0.020212393254041672|Training Accuracy : 1.0\n",
            "Batch : 1698|Training Loss: 0.22915877401828766|Training Accuracy : 0.875\n",
            "Batch : 1699|Training Loss: 0.15799152851104736|Training Accuracy : 0.9375\n",
            "Batch : 1700|Training Loss: 0.1318647861480713|Training Accuracy : 0.9375\n",
            "Batch : 1701|Training Loss: 0.2067943513393402|Training Accuracy : 0.875\n",
            "Batch : 1702|Training Loss: 0.16285651922225952|Training Accuracy : 0.96875\n",
            "Batch : 1703|Training Loss: 0.21300821006298065|Training Accuracy : 0.90625\n",
            "Batch : 1704|Training Loss: 0.08978279680013657|Training Accuracy : 0.96875\n",
            "Batch : 1705|Training Loss: 0.19509562849998474|Training Accuracy : 0.90625\n",
            "Batch : 1706|Training Loss: 0.023822030052542686|Training Accuracy : 1.0\n",
            "Batch : 1707|Training Loss: 0.21801407635211945|Training Accuracy : 0.9375\n",
            "Batch : 1708|Training Loss: 0.17696309089660645|Training Accuracy : 0.9375\n",
            "Batch : 1709|Training Loss: 0.1895727962255478|Training Accuracy : 0.9375\n",
            "Batch : 1710|Training Loss: 0.3586493730545044|Training Accuracy : 0.8125\n",
            "Batch : 1711|Training Loss: 0.18674853444099426|Training Accuracy : 0.96875\n",
            "Batch : 1712|Training Loss: 0.12027516216039658|Training Accuracy : 0.9375\n",
            "Batch : 1713|Training Loss: 0.10271741449832916|Training Accuracy : 0.96875\n",
            "Batch : 1714|Training Loss: 0.21381625533103943|Training Accuracy : 0.96875\n",
            "Batch : 1715|Training Loss: 0.2879091799259186|Training Accuracy : 0.8125\n",
            "Batch : 1716|Training Loss: 0.07065117359161377|Training Accuracy : 0.96875\n",
            "Batch : 1717|Training Loss: 0.18207211792469025|Training Accuracy : 0.9375\n",
            "Batch : 1718|Training Loss: 0.2707330882549286|Training Accuracy : 0.9375\n",
            "Batch : 1719|Training Loss: 0.18513615429401398|Training Accuracy : 0.9375\n",
            "Batch : 1720|Training Loss: 0.3048211336135864|Training Accuracy : 0.90625\n",
            "Batch : 1721|Training Loss: 0.05761588364839554|Training Accuracy : 1.0\n",
            "Batch : 1722|Training Loss: 0.17042258381843567|Training Accuracy : 0.9375\n",
            "Batch : 1723|Training Loss: 0.16160914301872253|Training Accuracy : 0.9375\n",
            "Batch : 1724|Training Loss: 0.2750031352043152|Training Accuracy : 0.90625\n",
            "Batch : 1725|Training Loss: 0.2026805877685547|Training Accuracy : 0.90625\n",
            "Batch : 1726|Training Loss: 0.3174208402633667|Training Accuracy : 0.84375\n",
            "Batch : 1727|Training Loss: 0.047592371702194214|Training Accuracy : 0.96875\n",
            "Batch : 1728|Training Loss: 0.12752249836921692|Training Accuracy : 0.9375\n",
            "Batch : 1729|Training Loss: 0.06456662714481354|Training Accuracy : 1.0\n",
            "Batch : 1730|Training Loss: 0.10231348127126694|Training Accuracy : 0.96875\n",
            "Batch : 1731|Training Loss: 0.22596772015094757|Training Accuracy : 0.9375\n",
            "Batch : 1732|Training Loss: 0.19228050112724304|Training Accuracy : 0.9375\n",
            "Batch : 1733|Training Loss: 0.1578737497329712|Training Accuracy : 0.90625\n",
            "Batch : 1734|Training Loss: 0.2076335996389389|Training Accuracy : 0.9375\n",
            "Batch : 1735|Training Loss: 0.18946610391139984|Training Accuracy : 0.90625\n",
            "Batch : 1736|Training Loss: 0.33880603313446045|Training Accuracy : 0.90625\n",
            "Batch : 1737|Training Loss: 0.20393642783164978|Training Accuracy : 0.90625\n",
            "Batch : 1738|Training Loss: 0.2515941858291626|Training Accuracy : 0.90625\n",
            "Batch : 1739|Training Loss: 0.15399520099163055|Training Accuracy : 0.96875\n",
            "Batch : 1740|Training Loss: 0.2724955976009369|Training Accuracy : 0.90625\n",
            "Batch : 1741|Training Loss: 0.15376505255699158|Training Accuracy : 0.90625\n",
            "Batch : 1742|Training Loss: 0.2945646643638611|Training Accuracy : 0.875\n",
            "Batch : 1743|Training Loss: 0.14268431067466736|Training Accuracy : 0.96875\n",
            "Batch : 1744|Training Loss: 0.1784515231847763|Training Accuracy : 0.90625\n",
            "Batch : 1745|Training Loss: 0.13985128700733185|Training Accuracy : 0.90625\n",
            "Batch : 1746|Training Loss: 0.4232277572154999|Training Accuracy : 0.84375\n",
            "Batch : 1747|Training Loss: 0.12002543359994888|Training Accuracy : 0.96875\n",
            "Batch : 1748|Training Loss: 0.2578509449958801|Training Accuracy : 0.90625\n",
            "Batch : 1749|Training Loss: 0.1565311700105667|Training Accuracy : 0.90625\n",
            "Batch : 1750|Training Loss: 0.2115737348794937|Training Accuracy : 0.9375\n",
            "Batch : 1751|Training Loss: 0.11920677125453949|Training Accuracy : 0.9375\n",
            "Batch : 1752|Training Loss: 0.10321067273616791|Training Accuracy : 0.9375\n",
            "Batch : 1753|Training Loss: 0.20592717826366425|Training Accuracy : 0.9375\n",
            "Batch : 1754|Training Loss: 0.16519232094287872|Training Accuracy : 0.9375\n",
            "Batch : 1755|Training Loss: 0.1805650293827057|Training Accuracy : 0.9375\n",
            "Batch : 1756|Training Loss: 0.02208724245429039|Training Accuracy : 1.0\n",
            "Batch : 1757|Training Loss: 0.12890420854091644|Training Accuracy : 0.90625\n",
            "Batch : 1758|Training Loss: 0.32905641198158264|Training Accuracy : 0.875\n",
            "Batch : 1759|Training Loss: 0.12259342521429062|Training Accuracy : 0.9375\n",
            "Batch : 1760|Training Loss: 0.23570038378238678|Training Accuracy : 0.96875\n",
            "Batch : 1761|Training Loss: 0.08131366223096848|Training Accuracy : 0.96875\n",
            "Batch : 1762|Training Loss: 0.33766666054725647|Training Accuracy : 0.875\n",
            "Batch : 1763|Training Loss: 0.18076439201831818|Training Accuracy : 0.90625\n",
            "Batch : 1764|Training Loss: 0.14535191655158997|Training Accuracy : 0.96875\n",
            "Batch : 1765|Training Loss: 0.3081846833229065|Training Accuracy : 0.90625\n",
            "Batch : 1766|Training Loss: 0.22102975845336914|Training Accuracy : 0.875\n",
            "Batch : 1767|Training Loss: 0.3086034953594208|Training Accuracy : 0.875\n",
            "Batch : 1768|Training Loss: 0.19997762143611908|Training Accuracy : 0.9375\n",
            "Batch : 1769|Training Loss: 0.31448403000831604|Training Accuracy : 0.875\n",
            "Batch : 1770|Training Loss: 0.42964962124824524|Training Accuracy : 0.9375\n",
            "Batch : 1771|Training Loss: 0.2601280212402344|Training Accuracy : 0.90625\n",
            "Batch : 1772|Training Loss: 0.17575927078723907|Training Accuracy : 0.96875\n",
            "Batch : 1773|Training Loss: 0.1385711133480072|Training Accuracy : 0.90625\n",
            "Batch : 1774|Training Loss: 0.07993263006210327|Training Accuracy : 0.9375\n",
            "Batch : 1775|Training Loss: 0.2694540023803711|Training Accuracy : 0.9375\n",
            "Batch : 1776|Training Loss: 0.054267413914203644|Training Accuracy : 1.0\n",
            "Batch : 1777|Training Loss: 0.2259468138217926|Training Accuracy : 0.84375\n",
            "Batch : 1778|Training Loss: 0.19069626927375793|Training Accuracy : 0.9375\n",
            "Batch : 1779|Training Loss: 0.635751485824585|Training Accuracy : 0.875\n",
            "Batch : 1780|Training Loss: 0.17607295513153076|Training Accuracy : 0.9375\n",
            "Batch : 1781|Training Loss: 0.09642187505960464|Training Accuracy : 0.96875\n",
            "Batch : 1782|Training Loss: 0.18190449476242065|Training Accuracy : 0.90625\n",
            "Batch : 1783|Training Loss: 0.21466340124607086|Training Accuracy : 0.9375\n",
            "Batch : 1784|Training Loss: 0.2157120406627655|Training Accuracy : 0.875\n",
            "Batch : 1785|Training Loss: 0.1688280999660492|Training Accuracy : 0.9375\n",
            "Batch : 1786|Training Loss: 0.25862881541252136|Training Accuracy : 0.875\n",
            "Batch : 1787|Training Loss: 0.07689942419528961|Training Accuracy : 0.96875\n",
            "Batch : 1788|Training Loss: 0.07151287794113159|Training Accuracy : 0.96875\n",
            "Batch : 1789|Training Loss: 0.13860003650188446|Training Accuracy : 0.9375\n",
            "Batch : 1790|Training Loss: 0.16403648257255554|Training Accuracy : 0.96875\n",
            "Batch : 1791|Training Loss: 0.19976238906383514|Training Accuracy : 0.90625\n",
            "Batch : 1792|Training Loss: 0.10046327114105225|Training Accuracy : 0.96875\n",
            "Batch : 1793|Training Loss: 0.22924286127090454|Training Accuracy : 0.875\n",
            "Batch : 1794|Training Loss: 0.17489050328731537|Training Accuracy : 0.90625\n",
            "Batch : 1795|Training Loss: 0.19719050824642181|Training Accuracy : 0.90625\n",
            "Batch : 1796|Training Loss: 0.20822477340698242|Training Accuracy : 0.90625\n",
            "Batch : 1797|Training Loss: 0.12153875827789307|Training Accuracy : 0.9375\n",
            "Batch : 1798|Training Loss: 0.10497459769248962|Training Accuracy : 0.9375\n",
            "Batch : 1799|Training Loss: 0.04740249365568161|Training Accuracy : 1.0\n",
            "Batch : 1800|Training Loss: 0.12014362215995789|Training Accuracy : 0.9375\n",
            "Batch : 1801|Training Loss: 0.0845399722456932|Training Accuracy : 0.96875\n",
            "Batch : 1802|Training Loss: 0.07589899748563766|Training Accuracy : 0.96875\n",
            "Batch : 1803|Training Loss: 0.21995466947555542|Training Accuracy : 0.90625\n",
            "Batch : 1804|Training Loss: 0.15747010707855225|Training Accuracy : 0.9375\n",
            "Batch : 1805|Training Loss: 0.08958961069583893|Training Accuracy : 0.96875\n",
            "Batch : 1806|Training Loss: 0.14941591024398804|Training Accuracy : 0.96875\n",
            "Batch : 1807|Training Loss: 0.21389448642730713|Training Accuracy : 0.90625\n",
            "Batch : 1808|Training Loss: 0.23159310221672058|Training Accuracy : 0.875\n",
            "Batch : 1809|Training Loss: 0.07861625403165817|Training Accuracy : 1.0\n",
            "Batch : 1810|Training Loss: 0.1885528862476349|Training Accuracy : 0.9375\n",
            "Batch : 1811|Training Loss: 0.2507997155189514|Training Accuracy : 0.90625\n",
            "Batch : 1812|Training Loss: 0.3041858971118927|Training Accuracy : 0.90625\n",
            "Batch : 1813|Training Loss: 0.2854454815387726|Training Accuracy : 0.875\n",
            "Batch : 1814|Training Loss: 0.17094269394874573|Training Accuracy : 0.90625\n",
            "Batch : 1815|Training Loss: 0.3558328151702881|Training Accuracy : 0.875\n",
            "Batch : 1816|Training Loss: 0.29383713006973267|Training Accuracy : 0.90625\n",
            "Batch : 1817|Training Loss: 0.10422990471124649|Training Accuracy : 0.96875\n",
            "Batch : 1818|Training Loss: 0.17471222579479218|Training Accuracy : 0.875\n",
            "Batch : 1819|Training Loss: 0.22546711564064026|Training Accuracy : 0.875\n",
            "Batch : 1820|Training Loss: 0.3119589388370514|Training Accuracy : 0.90625\n",
            "Batch : 1821|Training Loss: 0.17570169270038605|Training Accuracy : 0.9375\n",
            "Batch : 1822|Training Loss: 0.17211680114269257|Training Accuracy : 0.9375\n",
            "Batch : 1823|Training Loss: 0.16329488158226013|Training Accuracy : 0.90625\n",
            "Batch : 1824|Training Loss: 0.4004286527633667|Training Accuracy : 0.8125\n",
            "Batch : 1825|Training Loss: 0.126836359500885|Training Accuracy : 0.96875\n",
            "Batch : 1826|Training Loss: 0.23028947412967682|Training Accuracy : 0.9375\n",
            "Batch : 1827|Training Loss: 0.30568230152130127|Training Accuracy : 0.78125\n",
            "Batch : 1828|Training Loss: 0.3522709906101227|Training Accuracy : 0.90625\n",
            "Batch : 1829|Training Loss: 0.21972250938415527|Training Accuracy : 0.875\n",
            "Batch : 1830|Training Loss: 0.07793456315994263|Training Accuracy : 0.96875\n",
            "Batch : 1831|Training Loss: 0.2896154224872589|Training Accuracy : 0.90625\n",
            "Batch : 1832|Training Loss: 0.13700608909130096|Training Accuracy : 0.9375\n",
            "Batch : 1833|Training Loss: 0.08019712567329407|Training Accuracy : 0.96875\n",
            "Batch : 1834|Training Loss: 0.13240841031074524|Training Accuracy : 0.96875\n",
            "Batch : 1835|Training Loss: 0.5328959226608276|Training Accuracy : 0.84375\n",
            "Batch : 1836|Training Loss: 0.28377866744995117|Training Accuracy : 0.90625\n",
            "Batch : 1837|Training Loss: 0.020792970433831215|Training Accuracy : 1.0\n",
            "Batch : 1838|Training Loss: 0.11357025057077408|Training Accuracy : 0.96875\n",
            "Batch : 1839|Training Loss: 0.2214171588420868|Training Accuracy : 0.9375\n",
            "Batch : 1840|Training Loss: 0.06779678910970688|Training Accuracy : 0.96875\n",
            "Batch : 1841|Training Loss: 0.09288905560970306|Training Accuracy : 0.9375\n",
            "Batch : 1842|Training Loss: 0.35483595728874207|Training Accuracy : 0.84375\n",
            "Batch : 1843|Training Loss: 0.11661699414253235|Training Accuracy : 0.96875\n",
            "Batch : 1844|Training Loss: 0.1804812252521515|Training Accuracy : 0.9375\n",
            "Batch : 1845|Training Loss: 0.028434256091713905|Training Accuracy : 1.0\n",
            "Batch : 1846|Training Loss: 0.09047648310661316|Training Accuracy : 0.9375\n",
            "Batch : 1847|Training Loss: 0.15350262820720673|Training Accuracy : 0.90625\n",
            "Batch : 1848|Training Loss: 0.23085425794124603|Training Accuracy : 0.90625\n",
            "Batch : 1849|Training Loss: 0.2482905387878418|Training Accuracy : 0.9375\n",
            "Batch : 1850|Training Loss: 0.26165473461151123|Training Accuracy : 0.90625\n",
            "Batch : 1851|Training Loss: 0.1913917064666748|Training Accuracy : 0.90625\n",
            "Batch : 1852|Training Loss: 0.2569122314453125|Training Accuracy : 0.90625\n",
            "Batch : 1853|Training Loss: 0.20321975648403168|Training Accuracy : 0.96875\n",
            "Batch : 1854|Training Loss: 0.23120176792144775|Training Accuracy : 0.9375\n",
            "Batch : 1855|Training Loss: 0.223192498087883|Training Accuracy : 0.90625\n",
            "Batch : 1856|Training Loss: 0.11539587378501892|Training Accuracy : 0.90625\n",
            "Batch : 1857|Training Loss: 0.03790009021759033|Training Accuracy : 1.0\n",
            "Batch : 1858|Training Loss: 0.18257306516170502|Training Accuracy : 0.9375\n",
            "Batch : 1859|Training Loss: 0.3405197560787201|Training Accuracy : 0.90625\n",
            "Batch : 1860|Training Loss: 0.212865948677063|Training Accuracy : 0.875\n",
            "Batch : 1861|Training Loss: 0.21104881167411804|Training Accuracy : 0.875\n",
            "Batch : 1862|Training Loss: 0.30855774879455566|Training Accuracy : 0.875\n",
            "Batch : 1863|Training Loss: 0.40905702114105225|Training Accuracy : 0.84375\n",
            "Batch : 1864|Training Loss: 0.2132435142993927|Training Accuracy : 0.90625\n",
            "Batch : 1865|Training Loss: 0.21173112094402313|Training Accuracy : 0.90625\n",
            "Batch : 1866|Training Loss: 0.05370168760418892|Training Accuracy : 1.0\n",
            "Batch : 1867|Training Loss: 0.14318114519119263|Training Accuracy : 0.9375\n",
            "Batch : 1868|Training Loss: 0.1337510347366333|Training Accuracy : 0.9375\n",
            "Batch : 1869|Training Loss: 0.051608484238386154|Training Accuracy : 1.0\n",
            "Batch : 1870|Training Loss: 0.1476164162158966|Training Accuracy : 0.9375\n",
            "Batch : 1871|Training Loss: 0.0600757822394371|Training Accuracy : 1.0\n",
            "Batch : 1872|Training Loss: 0.22475199401378632|Training Accuracy : 0.90625\n",
            "Batch : 1873|Training Loss: 0.07973821461200714|Training Accuracy : 0.96875\n",
            "Batch : 1874|Training Loss: 0.0963110625743866|Training Accuracy : 0.9375\n",
            "Test Loss: 0.28059086203575134|Test Accuracy : 0.90625\n",
            "Test Loss: 0.1619623601436615|Test Accuracy : 0.9375\n",
            "Test Loss: 0.41494858264923096|Test Accuracy : 0.9375\n",
            "Test Loss: 0.44307708740234375|Test Accuracy : 0.9375\n",
            "Test Loss: 0.09009609371423721|Test Accuracy : 0.96875\n",
            "Test Loss: 0.6580526232719421|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3277164399623871|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6457623243331909|Test Accuracy : 0.8125\n",
            "Test Loss: 0.02327587641775608|Test Accuracy : 1.0\n",
            "Test Loss: 0.4074195623397827|Test Accuracy : 0.90625\n",
            "Test Loss: 0.15809595584869385|Test Accuracy : 0.96875\n",
            "Test Loss: 0.6052959561347961|Test Accuracy : 0.875\n",
            "Test Loss: 0.7208781838417053|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3022327423095703|Test Accuracy : 0.9375\n",
            "Test Loss: 0.10881923139095306|Test Accuracy : 0.96875\n",
            "Test Loss: 0.8261159062385559|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4732375741004944|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5155969262123108|Test Accuracy : 0.90625\n",
            "Test Loss: 0.35790491104125977|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4303096532821655|Test Accuracy : 0.90625\n",
            "Test Loss: 0.20261280238628387|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4640495479106903|Test Accuracy : 0.8125\n",
            "Test Loss: 0.236893892288208|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2395637184381485|Test Accuracy : 0.84375\n",
            "Test Loss: 0.36577215790748596|Test Accuracy : 0.8125\n",
            "Test Loss: 0.12223612517118454|Test Accuracy : 0.96875\n",
            "Test Loss: 0.15365587174892426|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4501308500766754|Test Accuracy : 0.875\n",
            "Test Loss: 0.28903958201408386|Test Accuracy : 0.9375\n",
            "Test Loss: 0.7126443386077881|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6085044741630554|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5743301510810852|Test Accuracy : 0.875\n",
            "Test Loss: 0.4633936285972595|Test Accuracy : 0.8125\n",
            "Test Loss: 0.41561418771743774|Test Accuracy : 0.84375\n",
            "Test Loss: 0.45754849910736084|Test Accuracy : 0.875\n",
            "Test Loss: 0.700628936290741|Test Accuracy : 0.8125\n",
            "Test Loss: 0.8381722569465637|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5669122338294983|Test Accuracy : 0.84375\n",
            "Test Loss: 0.13915151357650757|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5705456137657166|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3294500708580017|Test Accuracy : 0.90625\n",
            "Test Loss: 0.19528745114803314|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5687052607536316|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3448939025402069|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3337792158126831|Test Accuracy : 0.875\n",
            "Test Loss: 0.2580827474594116|Test Accuracy : 0.875\n",
            "Test Loss: 0.5096863508224487|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5285593867301941|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5577686429023743|Test Accuracy : 0.8125\n",
            "Test Loss: 0.661375105381012|Test Accuracy : 0.90625\n",
            "Test Loss: 0.17432495951652527|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2866297960281372|Test Accuracy : 0.875\n",
            "Test Loss: 0.22274361550807953|Test Accuracy : 0.90625\n",
            "Test Loss: 0.2699279189109802|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6304224729537964|Test Accuracy : 0.875\n",
            "Test Loss: 0.21930746734142303|Test Accuracy : 0.84375\n",
            "Test Loss: 0.22527757287025452|Test Accuracy : 0.9375\n",
            "Test Loss: 0.06537409126758575|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3402264714241028|Test Accuracy : 0.8125\n",
            "Test Loss: 0.669611930847168|Test Accuracy : 0.875\n",
            "Test Loss: 0.6670678853988647|Test Accuracy : 0.84375\n",
            "Test Loss: 0.26249900460243225|Test Accuracy : 0.875\n",
            "Test Loss: 0.29826903343200684|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4927883446216583|Test Accuracy : 0.8125\n",
            "Test Loss: 0.30938100814819336|Test Accuracy : 0.90625\n",
            "Test Loss: 0.477864533662796|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4814988672733307|Test Accuracy : 0.875\n",
            "Test Loss: 0.6798815131187439|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4121107757091522|Test Accuracy : 0.875\n",
            "Test Loss: 0.4778679609298706|Test Accuracy : 0.8125\n",
            "Test Loss: 0.08822004497051239|Test Accuracy : 0.96875\n",
            "Test Loss: 0.2460966855287552|Test Accuracy : 0.9375\n",
            "Test Loss: 0.7749706506729126|Test Accuracy : 0.78125\n",
            "Test Loss: 0.15996184945106506|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4349561929702759|Test Accuracy : 0.90625\n",
            "Test Loss: 0.28745388984680176|Test Accuracy : 0.875\n",
            "Test Loss: 1.049039602279663|Test Accuracy : 0.71875\n",
            "Test Loss: 0.5851268768310547|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7558503150939941|Test Accuracy : 0.84375\n",
            "Test Loss: 0.24607783555984497|Test Accuracy : 0.9375\n",
            "Test Loss: 0.43254566192626953|Test Accuracy : 0.84375\n",
            "Test Loss: 0.1724923849105835|Test Accuracy : 0.90625\n",
            "Test Loss: 0.38146695494651794|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7432743310928345|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3201412856578827|Test Accuracy : 0.84375\n",
            "Test Loss: 0.7635143995285034|Test Accuracy : 0.71875\n",
            "Test Loss: 0.4211220145225525|Test Accuracy : 0.8125\n",
            "Test Loss: 1.4947407245635986|Test Accuracy : 0.90625\n",
            "Test Loss: 0.41841548681259155|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7850467562675476|Test Accuracy : 0.875\n",
            "Test Loss: 1.052425503730774|Test Accuracy : 0.6875\n",
            "Test Loss: 1.5834527015686035|Test Accuracy : 0.875\n",
            "Test Loss: 0.23374181985855103|Test Accuracy : 0.90625\n",
            "Test Loss: 0.672270655632019|Test Accuracy : 0.875\n",
            "Test Loss: 0.21218441426753998|Test Accuracy : 0.875\n",
            "Test Loss: 0.41467440128326416|Test Accuracy : 0.875\n",
            "Test Loss: 0.9021448493003845|Test Accuracy : 0.78125\n",
            "Test Loss: 0.6336100697517395|Test Accuracy : 0.78125\n",
            "Test Loss: 0.42934054136276245|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6844789385795593|Test Accuracy : 0.84375\n",
            "Test Loss: 0.7012268900871277|Test Accuracy : 0.84375\n",
            "Test Loss: 0.21888090670108795|Test Accuracy : 0.90625\n",
            "Test Loss: 0.47230684757232666|Test Accuracy : 0.84375\n",
            "Test Loss: 0.7727221250534058|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3802347183227539|Test Accuracy : 0.875\n",
            "Test Loss: 0.7195417881011963|Test Accuracy : 0.71875\n",
            "Test Loss: 0.45529621839523315|Test Accuracy : 0.875\n",
            "Test Loss: 0.3130078911781311|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3416762351989746|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5488523244857788|Test Accuracy : 0.875\n",
            "Test Loss: 0.3965564966201782|Test Accuracy : 0.875\n",
            "Test Loss: 0.12480270862579346|Test Accuracy : 0.96875\n",
            "Test Loss: 0.6122260689735413|Test Accuracy : 0.875\n",
            "Test Loss: 0.7434253096580505|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5542735457420349|Test Accuracy : 0.8125\n",
            "Test Loss: 0.37696027755737305|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5030019283294678|Test Accuracy : 0.9375\n",
            "Test Loss: 0.42047956585884094|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6207976937294006|Test Accuracy : 0.90625\n",
            "Test Loss: 0.42880338430404663|Test Accuracy : 0.90625\n",
            "Test Loss: 0.18966792523860931|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7263795137405396|Test Accuracy : 0.875\n",
            "Test Loss: 0.32006728649139404|Test Accuracy : 0.90625\n",
            "Test Loss: 0.606037974357605|Test Accuracy : 0.8125\n",
            "Test Loss: 0.31505465507507324|Test Accuracy : 0.875\n",
            "Test Loss: 0.19927959144115448|Test Accuracy : 0.90625\n",
            "Test Loss: 0.11676143854856491|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3938809633255005|Test Accuracy : 0.84375\n",
            "Test Loss: 0.17530131340026855|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5870808362960815|Test Accuracy : 0.875\n",
            "Test Loss: 0.5466696619987488|Test Accuracy : 0.8125\n",
            "Test Loss: 0.1434670388698578|Test Accuracy : 0.9375\n",
            "Test Loss: 0.2850777804851532|Test Accuracy : 0.875\n",
            "Test Loss: 0.4456619620323181|Test Accuracy : 0.84375\n",
            "Test Loss: 0.1914067268371582|Test Accuracy : 0.90625\n",
            "Test Loss: 0.20295260846614838|Test Accuracy : 0.9375\n",
            "Test Loss: 0.8187987208366394|Test Accuracy : 0.71875\n",
            "Test Loss: 0.12552820146083832|Test Accuracy : 0.96875\n",
            "Test Loss: 0.18513083457946777|Test Accuracy : 0.96875\n",
            "Test Loss: 0.5181177854537964|Test Accuracy : 0.875\n",
            "Test Loss: 0.41885000467300415|Test Accuracy : 0.78125\n",
            "Test Loss: 0.6995066404342651|Test Accuracy : 0.78125\n",
            "Test Loss: 0.24440471827983856|Test Accuracy : 0.9375\n",
            "Test Loss: 0.7823504209518433|Test Accuracy : 0.8125\n",
            "Test Loss: 0.5579363107681274|Test Accuracy : 0.8125\n",
            "Test Loss: 0.2975020408630371|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6340002417564392|Test Accuracy : 0.78125\n",
            "Test Loss: 0.3648161292076111|Test Accuracy : 0.875\n",
            "Test Loss: 0.2736906111240387|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7833757996559143|Test Accuracy : 0.8125\n",
            "Test Loss: 0.15626004338264465|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5863669514656067|Test Accuracy : 0.8125\n",
            "Test Loss: 1.2106298208236694|Test Accuracy : 0.78125\n",
            "Test Loss: 0.22393545508384705|Test Accuracy : 0.90625\n",
            "Test Loss: 0.8910326957702637|Test Accuracy : 0.84375\n",
            "Test Loss: 0.44140613079071045|Test Accuracy : 0.875\n",
            "Test Loss: 0.4867899715900421|Test Accuracy : 0.90625\n",
            "Test Loss: 0.636411190032959|Test Accuracy : 0.8125\n",
            "Test Loss: 0.2620282471179962|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4271572530269623|Test Accuracy : 0.90625\n",
            "Test Loss: 0.9777268767356873|Test Accuracy : 0.75\n",
            "Test Loss: 0.34345266222953796|Test Accuracy : 0.875\n",
            "Test Loss: 0.375331312417984|Test Accuracy : 0.90625\n",
            "Test Loss: 1.0635943412780762|Test Accuracy : 0.78125\n",
            "Test Loss: 0.8277499675750732|Test Accuracy : 0.90625\n",
            "Test Loss: 0.35754889249801636|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4011710584163666|Test Accuracy : 0.875\n",
            "Test Loss: 1.0951565504074097|Test Accuracy : 0.84375\n",
            "Test Loss: 1.1451753377914429|Test Accuracy : 0.71875\n",
            "Test Loss: 0.3921402394771576|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4357345700263977|Test Accuracy : 0.8125\n",
            "Test Loss: 0.33243903517723083|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5177384614944458|Test Accuracy : 0.875\n",
            "Test Loss: 0.26957184076309204|Test Accuracy : 0.9375\n",
            "Test Loss: 0.24410608410835266|Test Accuracy : 0.9375\n",
            "Test Loss: 0.7746028304100037|Test Accuracy : 0.75\n",
            "Test Loss: 0.6870553493499756|Test Accuracy : 0.78125\n",
            "Test Loss: 0.28775110840797424|Test Accuracy : 0.875\n",
            "Test Loss: 0.8454426527023315|Test Accuracy : 0.875\n",
            "Test Loss: 1.1721632480621338|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4116939604282379|Test Accuracy : 0.84375\n",
            "Test Loss: 0.7002117037773132|Test Accuracy : 0.84375\n",
            "Test Loss: 2.8011229038238525|Test Accuracy : 0.75\n",
            "Test Loss: 0.48382583260536194|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4021495580673218|Test Accuracy : 0.9375\n",
            "Test Loss: 0.488425612449646|Test Accuracy : 0.84375\n",
            "Test Loss: 0.3848841190338135|Test Accuracy : 0.84375\n",
            "Test Loss: 0.263703316450119|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3795620799064636|Test Accuracy : 0.875\n",
            "Test Loss: 0.15981647372245789|Test Accuracy : 0.9375\n",
            "Test Loss: 0.44758346676826477|Test Accuracy : 0.875\n",
            "Test Loss: 0.5544566512107849|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6958293914794922|Test Accuracy : 0.84375\n",
            "Test Loss: 1.5880532264709473|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5211310982704163|Test Accuracy : 0.84375\n",
            "Test Loss: 0.43676912784576416|Test Accuracy : 0.8125\n",
            "Test Loss: 0.48776474595069885|Test Accuracy : 0.78125\n",
            "Test Loss: 0.5890024900436401|Test Accuracy : 0.8125\n",
            "Test Loss: 0.33177685737609863|Test Accuracy : 0.875\n",
            "Test Loss: 0.504655659198761|Test Accuracy : 0.875\n",
            "Test Loss: 0.27182531356811523|Test Accuracy : 0.90625\n",
            "Test Loss: 0.19026167690753937|Test Accuracy : 0.9375\n",
            "Test Loss: 0.2951001226902008|Test Accuracy : 0.90625\n",
            "Test Loss: 1.438298225402832|Test Accuracy : 0.84375\n",
            "Test Loss: 0.4889938235282898|Test Accuracy : 0.875\n",
            "Test Loss: 0.778435468673706|Test Accuracy : 0.71875\n",
            "Test Loss: 0.2984890937805176|Test Accuracy : 0.875\n",
            "Test Loss: 0.05291059613227844|Test Accuracy : 1.0\n",
            "Test Loss: 0.12977510690689087|Test Accuracy : 0.9375\n",
            "Test Loss: 0.34646329283714294|Test Accuracy : 0.84375\n",
            "Test Loss: 0.6147127747535706|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4024585485458374|Test Accuracy : 0.875\n",
            "Test Loss: 0.3943600356578827|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5659066438674927|Test Accuracy : 0.90625\n",
            "Test Loss: 0.41755032539367676|Test Accuracy : 0.875\n",
            "Test Loss: 1.5639350414276123|Test Accuracy : 0.71875\n",
            "Test Loss: 0.8287718892097473|Test Accuracy : 0.875\n",
            "Test Loss: 0.7718136310577393|Test Accuracy : 0.78125\n",
            "Test Loss: 0.4173312187194824|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7596368193626404|Test Accuracy : 0.78125\n",
            "Test Loss: 0.6161011457443237|Test Accuracy : 0.90625\n",
            "Test Loss: 1.1715893745422363|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6634518504142761|Test Accuracy : 0.8125\n",
            "Test Loss: 0.8702134490013123|Test Accuracy : 0.84375\n",
            "Test Loss: 0.44867193698883057|Test Accuracy : 0.90625\n",
            "Test Loss: 0.475236177444458|Test Accuracy : 0.90625\n",
            "Test Loss: 0.11663120985031128|Test Accuracy : 0.9375\n",
            "Test Loss: 0.4982852339744568|Test Accuracy : 0.90625\n",
            "Test Loss: 0.41034552454948425|Test Accuracy : 0.90625\n",
            "Test Loss: 0.527591347694397|Test Accuracy : 0.84375\n",
            "Test Loss: 0.10061126947402954|Test Accuracy : 0.9375\n",
            "Test Loss: 0.3565821647644043|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4416881203651428|Test Accuracy : 0.90625\n",
            "Test Loss: 0.43517130613327026|Test Accuracy : 0.75\n",
            "Test Loss: 0.7553691864013672|Test Accuracy : 0.90625\n",
            "Test Loss: 0.25612950325012207|Test Accuracy : 0.875\n",
            "Test Loss: 0.8982352614402771|Test Accuracy : 0.8125\n",
            "Test Loss: 0.48587021231651306|Test Accuracy : 0.875\n",
            "Test Loss: 0.5225394368171692|Test Accuracy : 0.75\n",
            "Test Loss: 0.2047232836484909|Test Accuracy : 0.90625\n",
            "Test Loss: 0.183262899518013|Test Accuracy : 0.9375\n",
            "Test Loss: 0.32667040824890137|Test Accuracy : 0.875\n",
            "Test Loss: 0.4194567799568176|Test Accuracy : 0.90625\n",
            "Test Loss: 0.7319474220275879|Test Accuracy : 0.78125\n",
            "Test Loss: 0.24157707393169403|Test Accuracy : 0.90625\n",
            "Test Loss: 0.38676586747169495|Test Accuracy : 0.875\n",
            "Test Loss: 0.4742097556591034|Test Accuracy : 0.78125\n",
            "Test Loss: 0.8433586359024048|Test Accuracy : 0.875\n",
            "Test Loss: 0.3809088170528412|Test Accuracy : 0.875\n",
            "Test Loss: 0.2974854111671448|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6322680711746216|Test Accuracy : 0.84375\n",
            "Test Loss: 0.5540180206298828|Test Accuracy : 0.84375\n",
            "Test Loss: 0.888553261756897|Test Accuracy : 0.8125\n",
            "Test Loss: 0.3647859990596771|Test Accuracy : 0.84375\n",
            "Test Loss: 0.22489623725414276|Test Accuracy : 0.96875\n",
            "Test Loss: 0.8819609880447388|Test Accuracy : 0.8125\n",
            "Test Loss: 1.1466116905212402|Test Accuracy : 0.8125\n",
            "Test Loss: 0.08559288084506989|Test Accuracy : 0.96875\n",
            "Test Loss: 0.30337074398994446|Test Accuracy : 0.84375\n",
            "Test Loss: 0.42733627557754517|Test Accuracy : 0.875\n",
            "Test Loss: 0.5411161184310913|Test Accuracy : 0.8125\n",
            "Test Loss: 0.22223825752735138|Test Accuracy : 0.96875\n",
            "Test Loss: 0.9147179126739502|Test Accuracy : 0.84375\n",
            "Test Loss: 0.40473121404647827|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5204100012779236|Test Accuracy : 0.8125\n",
            "Test Loss: 0.37326282262802124|Test Accuracy : 0.875\n",
            "Test Loss: 0.43632638454437256|Test Accuracy : 0.875\n",
            "Test Loss: 0.16225923597812653|Test Accuracy : 0.90625\n",
            "Test Loss: 0.25805890560150146|Test Accuracy : 0.90625\n",
            "Test Loss: 0.6770674586296082|Test Accuracy : 0.8125\n",
            "Test Loss: 0.6611244082450867|Test Accuracy : 0.8125\n",
            "Test Loss: 0.4966793656349182|Test Accuracy : 0.875\n",
            "Test Loss: 0.2316768616437912|Test Accuracy : 0.9375\n",
            "Test Loss: 0.1801396608352661|Test Accuracy : 0.90625\n",
            "Test Loss: 0.5003630518913269|Test Accuracy : 0.8125\n",
            "Test Loss: 0.9925503134727478|Test Accuracy : 0.71875\n",
            "Test Loss: 0.6260164380073547|Test Accuracy : 0.875\n",
            "Test Loss: 1.0191981792449951|Test Accuracy : 0.84375\n",
            "Test Loss: 0.46720296144485474|Test Accuracy : 0.8125\n",
            "Test Loss: 0.27162057161331177|Test Accuracy : 0.875\n",
            "Test Loss: 0.1577748954296112|Test Accuracy : 0.9375\n",
            "Test Loss: 0.06966277211904526|Test Accuracy : 0.96875\n",
            "Test Loss: 0.36740612983703613|Test Accuracy : 0.875\n",
            "Test Loss: 0.1854233741760254|Test Accuracy : 0.96875\n",
            "Test Loss: 0.252986341714859|Test Accuracy : 0.9375\n",
            "Test Loss: 0.6226780414581299|Test Accuracy : 0.90625\n",
            "Test Loss: 0.46314406394958496|Test Accuracy : 0.875\n",
            "Test Loss: 0.5535210371017456|Test Accuracy : 0.8125\n",
            "Test Loss: 0.7616007924079895|Test Accuracy : 0.90625\n",
            "Test Loss: 0.14743801951408386|Test Accuracy : 0.96875\n",
            "Test Loss: 0.33201977610588074|Test Accuracy : 0.875\n",
            "Test Loss: 0.3092164099216461|Test Accuracy : 0.90625\n",
            "Test Loss: 0.643225371837616|Test Accuracy : 0.875\n",
            "Test Loss: 0.47475504875183105|Test Accuracy : 0.875\n",
            "Test Loss: 0.2916843891143799|Test Accuracy : 0.90625\n",
            "Test Loss: 1.2258081436157227|Test Accuracy : 0.875\n",
            "Test Loss: 0.5768205523490906|Test Accuracy : 0.8125\n",
            "Test Loss: 0.05539622902870178|Test Accuracy : 0.96875\n",
            "Test Loss: 0.5392119884490967|Test Accuracy : 0.71875\n",
            "Test Loss: 0.4054458737373352|Test Accuracy : 0.875\n",
            "Test Loss: 0.6331673264503479|Test Accuracy : 0.875\n",
            "Test Loss: 0.09446035325527191|Test Accuracy : 0.96875\n",
            "Test Loss: 0.3002816438674927|Test Accuracy : 0.90625\n",
            "Test Loss: 0.3967198133468628|Test Accuracy : 0.875\n",
            "Test Loss: 0.610839307308197|Test Accuracy : 0.875\n",
            "Test Loss: 0.45945510268211365|Test Accuracy : 0.875\n",
            "Test Loss: 0.4699121415615082|Test Accuracy : 0.84375\n",
            "Test Loss: 0.17609086632728577|Test Accuracy : 0.9375\n",
            "Test Loss: 0.5676941275596619|Test Accuracy : 0.875\n",
            "Test Loss: 0.0337224006652832|Test Accuracy : 1.0\n",
            "Test Loss: 0.29633092880249023|Test Accuracy : 0.84375\n",
            "Test Loss: 0.45668309926986694|Test Accuracy : 0.90625\n",
            "Test Loss: 0.4231092929840088|Test Accuracy : 0.8125\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(100):\n",
        "  print(f\"Epoch :{epoch}\")\n",
        "  trainer(train_dataloader,model1,acc_fn,loss_fn,optimizer)\n",
        "  tester(test_dataloader,model1,acc_fn,loss_fn)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}